{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2208254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:53:32.836184: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-09 10:53:32.836245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-09 10:53:32.837606: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 10:53:32.844891: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.pmi_estimators import train_critic_model, neural_pmi\n",
    "from src.psi_estimators import psi_gaussian_train, psi_gaussian_val_class\n",
    "from src.pvi_estimators import train_pvi_null_model, neural_pvi_class, neural_pvi_ensemble_class\n",
    "import src.utils as utils\n",
    "import src.metrics as metrics\n",
    "import src.methods as methods\n",
    "import src.temp_scaling as temp_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa22eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:54:13.337227: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'resnet50'\n",
    "dataset_name = 'cifar10'\n",
    "\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    'cifar10',\n",
    "    split=['train[:85%]', 'train[85%:]', 'test'],\n",
    "    data_dir = '../tensorflow_datasets/',\n",
    "    shuffle_files=False,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "IMG_SIZE = 224\n",
    "num_classes = 10\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    label = tf.one_hot(label, depth=num_classes)\n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train.map(preprocess)\n",
    "ds_val = ds_val.map(preprocess)\n",
    "ds_test = ds_test.map(preprocess)\n",
    "\n",
    "# batch_size = 128\n",
    "# ds_train = ds_train.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "# ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "# ds_test = ds_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "true_y_train = np.argmax([y for x,y in ds_train], axis=1)\n",
    "true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "true_y_test = np.argmax([y for x,y in ds_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b9f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    base_model = ResNet50V2(include_top=False, weights='imagenet', input_tensor=Input(shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(10, activation='linear')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87112179",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b7534",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "    \n",
    "    model = create_model()\n",
    "    \n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1)\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=100, callbacks=[lr_scheduler, early_stop])\n",
    "    \n",
    "    if not os.path.exists(exp_name+'/saved_models'):\n",
    "        print(\"Making directory\", exp_name+'/saved_models')\n",
    "        os.makedirs(exp_name+'/saved_models')\n",
    "\n",
    "    model.save_weights(f'{exp_name}/saved_models/trained_weights.h5')\n",
    "    with open(f'{exp_name}/history.pickle', 'wb') as f:\n",
    "        pickle.dump(history, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b24f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "test_acc = []\n",
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=loss_fn, metrics=['accuracy'])\n",
    "    train_acc.append(model.evaluate(ds_train, verbose=1)[1])\n",
    "    val_acc.append(model.evaluate(ds_val, verbose=1)[1])\n",
    "    test_acc.append(model.evaluate(ds_test, verbose=1)[1])\n",
    "print(f'Average train error: {(100-np.mean(train_acc)*100):.2f} ({(np.std(train_acc)*100):.2f})')\n",
    "print(f'Average validation error: {(100-np.mean(val_acc)*100):.2f} ({(np.std(val_acc)*100):.2f})')\n",
    "print(f'Average test error: {(100-np.mean(test_acc)*100):.2f} ({(np.std(test_acc)*100):.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60918959",
   "metadata": {},
   "source": [
    "### PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0eef75",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.pmi_estimators import train_critic_model, neural_pmi\n",
    "from tqdm import tqdm\n",
    "\n",
    "for run in range(7,10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pmi/separable_variational_f_js'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "\n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PMI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "\n",
    "    print(f'Training PMI model...')\n",
    "    ds_activity_trn = ds_train.batch(128).map(lambda x, y: (int_model(x), y)).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    ds_activity_val = ds_val.batch(128).map(lambda x, y: (int_model(x), y)).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    train_critic_model(ds_activity_trn, ds_activity_val, critic='separable', estimator='variational_f_js', epochs=200, save_path=f'{exp_name}/pmi_output_model')\n",
    "\n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PMI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "\n",
    "    pmi_model = tf.keras.models.load_model(f'{exp_name}/pmi_output_model')\n",
    "    n_classes = 10\n",
    "\n",
    "    print(f'Computing PMI for all validation samples and for all classes...')\n",
    "    encoded_x = []\n",
    "    for x, _ in ds_val.batch(128):\n",
    "        encoded_x.append(int_model(x).numpy())\n",
    "    encoded_x = np.concatenate(encoded_x)\n",
    "    num_samples = encoded_x.shape[0]\n",
    "    \n",
    "    pmi_class = []\n",
    "    batch_size = 1024\n",
    "    for k in range(n_classes):\n",
    "        num_samples = encoded_x.shape[0]\n",
    "        y_k = tf.one_hot(tf.fill([num_samples], k), depth=n_classes)\n",
    "        pmi_list = []\n",
    "        for i in tqdm(range(0, len(encoded_x), batch_size), desc=f\"Computing PMI for class {k+1}\"):\n",
    "            x_batch = encoded_x[i:i+batch_size]\n",
    "            y_batch = y_k[i:i+batch_size]\n",
    "            pmi = neural_pmi(x_batch, y_batch, pmi_model, estimator='variational_f_js')\n",
    "            pmi_list += np.array(pmi).tolist()\n",
    "        pmi_class.append(pmi_list)\n",
    "    np.save(f'{exp_name}/pmi_output_class_val.npy', np.array(pmi_class).T)\n",
    "    \n",
    "    print(f'Computing PMI for all test samples and for all classes...')\n",
    "    encoded_x = []\n",
    "    for x, _ in ds_test.batch(128):\n",
    "        encoded_x.append(int_model(x).numpy())\n",
    "    encoded_x = np.concatenate(encoded_x)\n",
    "    num_samples = encoded_x.shape[0]\n",
    "    \n",
    "    pmi_class = []\n",
    "    batch_size = 1024\n",
    "    for k in range(n_classes):\n",
    "        num_samples = encoded_x.shape[0]\n",
    "        y_k = tf.one_hot(tf.fill([num_samples], k), depth=n_classes)\n",
    "        pmi_list = []\n",
    "        for i in tqdm(range(0, len(encoded_x), batch_size), desc=f\"Computing PMI for class {k+1}\"):\n",
    "            x_batch = encoded_x[i:i+batch_size]\n",
    "            y_batch = y_k[i:i+batch_size]\n",
    "            pmi = neural_pmi(x_batch, y_batch, pmi_model, estimator='variational_f_js')\n",
    "            pmi_list += np.array(pmi).tolist()\n",
    "        pmi_class.append(pmi_list)\n",
    "    np.save(f'{exp_name}/pmi_output_class_test.npy', np.array(pmi_class).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f98d1",
   "metadata": {},
   "source": [
    "### PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14360d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/gaussian'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PSI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    x_logits_list = []\n",
    "    y_labels_list = []\n",
    "\n",
    "    for x_batch, y_batch in ds_train.batch(256):\n",
    "        logits = int_model(x_batch)\n",
    "        labels = tf.argmax(y_batch, axis=1)\n",
    "        x_logits_list.append(logits)\n",
    "        y_labels_list.append(labels)\n",
    "\n",
    "    x = tf.concat(x_logits_list, axis=0).numpy()\n",
    "    y = tf.concat(y_labels_list, axis=0).numpy()\n",
    "    \n",
    "    print(f'Training PSI model (gaussian)...')\n",
    "    psi_data = psi_gaussian_train(x, y, n_projs=500)\n",
    "    np.save(f'{exp_name}/gaussian_output_model_500_projs.npy', psi_data)\n",
    "\n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PSI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "\n",
    "    psi_data = np.load(f'{exp_name}/gaussian_output_model_500_projs.npy', allow_pickle=True).item()\n",
    "\n",
    "    print(f'Computing PSI for all validation samples...')\n",
    "    x_logits_list = []\n",
    "\n",
    "    for x_batch, y_batch in ds_val.batch(256):\n",
    "        logits = int_model(x_batch)\n",
    "        x_logits_list.append(logits)\n",
    "    \n",
    "    x = tf.concat(x_logits_list, axis=0).numpy()\n",
    "    psi_class, pmi_arr = psi_gaussian_val_class(x, psi_data)\n",
    "    np.save(f'{exp_name}/psi_output_class_500_projs_val.npy', np.array(psi_class))\n",
    "\n",
    "    print(f'Computing PSI for all test samples...')\n",
    "    x_logits_list = []\n",
    "\n",
    "    for x_batch, y_batch in ds_test.batch(256):\n",
    "        logits = int_model(x_batch)\n",
    "        x_logits_list.append(logits)\n",
    "    \n",
    "    x = tf.concat(x_logits_list, axis=0).numpy()\n",
    "    psi_class, pmi_arr = psi_gaussian_val_class(x, psi_data)\n",
    "    np.save(f'{exp_name}/psi_output_class_500_projs_test.npy', np.array(psi_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d585ec",
   "metadata": {},
   "source": [
    "### PVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c63a00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_runs = list(range(10))\n",
    "while any(random_runs[i] == i for i in range(10)):\n",
    "    np.random.shuffle(random_runs)\n",
    "    \n",
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{random_runs[run]+1}/saved_models/trained_weights.h5')\n",
    "    pvi_model.save_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    \n",
    "    untrained_model = create_model()\n",
    "    train_pvi_null_model(ds_train, untrained_model, epochs=10, save_path=f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    null_model = create_model()\n",
    "    null_model.load_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "    opt_temp_pvi = utils.temp_scaling_nll(pvi_model.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "    opt_temp_null = utils.temp_scaling_nll(null_model.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "\n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_val.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_test.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c908a79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/finetuned'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    pvi_model.compile(optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1)\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "    pvi_model.fit(ds_train.batch(256), validation_data=ds_val.batch(256), epochs=100, callbacks=[lr_scheduler, early_stop])\n",
    "    \n",
    "    pvi_model.save_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    \n",
    "    untrained_model = create_model()\n",
    "    untrained_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_null_model_weights.h5')\n",
    "    untrained_model.save_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    null_model = create_model()\n",
    "    null_model.load_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "    opt_temp_pvi = temp_scaling.temp_scaling_nll(pvi_model.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "    opt_temp_null = temp_scaling.temp_scaling_nll(null_model.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "\n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_val.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_test.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cf4c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pvi_runs = [1 if i == 9 else 9 for i in range(10)]\n",
    "    \n",
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{pvi_runs[run]+1}/saved_models/trained_weights.h5')\n",
    "    pvi_model.save_weights(f'{exp_name}/pvi_model_best_weights.h5')\n",
    "    \n",
    "#     untrained_model = create_model()\n",
    "#     train_pvi_null_model(ds_train, untrained_model, epochs=10, save_path=f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'{exp_name}/pvi_model_best_weights.h5')\n",
    "    null_model = create_model()\n",
    "    null_model.load_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "    opt_temp_pvi = temp_scaling.temp_scaling_nll(pvi_model.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "    opt_temp_null = temp_scaling.temp_scaling_nll(null_model.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "\n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_val.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_best_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_test.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_best_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5983df5",
   "metadata": {},
   "source": [
    "### Ensemble PVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25edc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/ensemble_no_training_training_from_scratch'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model_1 = create_model()\n",
    "    pvi_model_1.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    null_model_1 = create_model()\n",
    "    null_model_1.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_null_model_weights.h5')\n",
    "    pvi_model_2 = create_model()\n",
    "    pvi_model_2.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_model_weights.h5')\n",
    "    null_model_2 = create_model()\n",
    "    null_model_2.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_null_model_weights.h5')\n",
    "    \n",
    "#     true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "#     opt_temp_pvi_1 = utils.temp_scaling_nll(pvi_model_1.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "#     opt_temp_pvi_2 = utils.temp_scaling_nll(pvi_model_2.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "#     ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "#     opt_temp_null = utils.temp_scaling_nll(null_model_1.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = []\n",
    "    for (x_batch, y_batch) in ds_val.batch(256):\n",
    "        pvi = neural_pvi_ensemble_class([x_batch, x_batch], [pvi_model_1, pvi_model_2], [null_model_1, null_model_2])\n",
    "        pvi_class += np.array(pvi).tolist()\n",
    "    np.save(f'{exp_name}/pvi_class_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = []\n",
    "    for (x_batch, y_batch) in ds_test.batch(256):\n",
    "        pvi = neural_pvi_ensemble_class([x_batch, x_batch], [pvi_model_1, pvi_model_2], [null_model_1, null_model_2])\n",
    "        pvi_class += np.array(pvi).tolist()\n",
    "    np.save(f'{exp_name}/pvi_class_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0a0ec",
   "metadata": {},
   "source": [
    "### Temp Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd788e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "#     if not os.path.exists(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'):\n",
    "#         print(\"Making directory\", f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')\n",
    "#         os.makedirs(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')                                  \n",
    "  \n",
    "    \n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = model.predict(ds_val.batch(512), verbose=0)\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)\n",
    "#     np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)\n",
    "#     np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_nll.npy', opt_temp)\n",
    "\n",
    "    opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4c5cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 09:51:42.500488: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 15s 500ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 09:51:59.367230: I external/local_xla/xla/service/service.cc:168] XLA service 0x55d19b629180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-09 09:51:59.367264: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2025-04-09 09:51:59.372200: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744192319.480472 1464697 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 2\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 3\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 4\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 5\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 6\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 7\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 8\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 9\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 10\n",
      "15/15 [==============================] - 3s 128ms/step\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "#     if not os.path.exists(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'):\n",
    "#         print(\"Making directory\", f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')\n",
    "#         os.makedirs(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')                                  \n",
    "  \n",
    "    \n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = model.predict(ds_val.batch(512), verbose=0)\n",
    "    \n",
    "    opt_temp, opt_weights = temp_scaling.ensemble_temp_scaling_nll(scores, true_y_val, num_classes)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_ets_nll.npy', opt_temp)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_weights_ets_nll.npy', opt_weights)\n",
    "\n",
    "#     opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)\n",
    "#     np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f011ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:41:10.426332: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 15s 501ms/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:41:28.560117: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f89ab2f0db0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-09 10:41:28.560162: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2025-04-09 10:41:28.565547: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744195288.684580 1588526 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 2s 2ms/step - loss: 0.0059\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_1/calibration/calibration_model/pts_model.h5\n",
      "Run: 2\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0143\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_2/calibration/calibration_model/pts_model.h5\n",
      "Run: 3\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0061\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_3/calibration/calibration_model/pts_model.h5\n",
      "Run: 4\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0051\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 8.3690e-04\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 9.1889e-04\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 9.6707e-04\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 8.2663e-04\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 9.0832e-04\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 9.4737e-04\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 7.6685e-04\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 8.5253e-04\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 7.5179e-04\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 8.6028e-04\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 8.1046e-04\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 6.5549e-04\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 6.2614e-04\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 6.4765e-04\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 5.6193e-04\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_4/calibration/calibration_model/pts_model.h5\n",
      "Run: 5\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0069\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_5/calibration/calibration_model/pts_model.h5\n",
      "Run: 6\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0054\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_6/calibration/calibration_model/pts_model.h5\n",
      "Run: 7\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0120\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_7/calibration/calibration_model/pts_model.h5\n",
      "Run: 8\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0091\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_8/calibration/calibration_model/pts_model.h5\n",
      "Run: 9\n",
      "15/15 [==============================] - 3s 126ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0056\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_9/calibration/calibration_model/pts_model.h5\n",
      "Run: 10\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0113\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_10/calibration/calibration_model/pts_model.h5\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    \n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = model.predict(ds_val.batch(512), verbose=0)\n",
    "    \n",
    "    pts = temp_scaling.PTSCalibrator(\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=64,\n",
    "    nlayers=2,\n",
    "    n_nodes=32,\n",
    "    length_logits=10,\n",
    "    top_k_logits=5\n",
    ")\n",
    "\n",
    "    pts.tune(logits=scores, labels=pred_y_val)\n",
    "    pts.save(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9812fc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pmi/separable_variational_f_js'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pmi_output_class_val.npy')\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)                            \n",
    "#     np.save(f'{exp_name}/pmi_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)                                \n",
    "#     np.save(f'{exp_name}/pmi_opt_temp_nll.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)                                \n",
    "    np.save(f'{exp_name}/pmi_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b6bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/gaussian'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/psi_output_class_500_projs_val.npy')\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)                                 \n",
    "#     np.save(f'{exp_name}/psi_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)                            \n",
    "#     np.save(f'{exp_name}/psi_opt_temp_nll.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)                            \n",
    "    np.save(f'{exp_name}/psi_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a4599",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pvi_class_best_val.npy')\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)                                 \n",
    "    np.save(f'{exp_name}/pvi_best_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)                                          \n",
    "#     np.save(f'{exp_name}/pvi_best_opt_temp_nll.npy', opt_temp)\n",
    "\n",
    "#     opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)                                          \n",
    "#     np.save(f'{exp_name}/pvi_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e9f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "15/15 [==============================] - 3s 128ms/step\n",
      "Run: 2\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 3\n",
      "15/15 [==============================] - 3s 132ms/step\n",
      "Run: 4\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 5\n",
      "15/15 [==============================] - 3s 128ms/step\n",
      "Run: 6\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 7\n",
      "15/15 [==============================] - 3s 128ms/step\n",
      "Run: 8\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Run: 9\n",
      "15/15 [==============================] - 3s 128ms/step\n",
      "Run: 10\n",
      "15/15 [==============================] - 3s 128ms/step\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pvi_class_val.npy')\n",
    "    \n",
    "    opt_temp, opt_weights = temp_scaling.ensemble_temp_scaling_nll(scores, true_y_val, num_classes)\n",
    "    np.save(f'{exp_name}/pvi_opt_temp_ets_nll.npy', opt_temp)\n",
    "    np.save(f'{exp_name}/pvi_opt_weights_ets_nll.npy', opt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfbbc00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:54:24.759447: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 16s 506ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:54:40.190099: I external/local_xla/xla/service/service.cc:168] XLA service 0x55b46aecc200 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-09 10:54:40.190136: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2025-04-09 10:54:40.195223: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744196080.303161 1656477 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "118/118 [==============================] - 2s 2ms/step - loss: 0.0149\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_1/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 2\n",
      "15/15 [==============================] - 3s 178ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0158\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_2/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 3\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0151\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_3/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 4\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0144\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_4/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 5\n",
      "15/15 [==============================] - 3s 126ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0153\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_5/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 6\n",
      "15/15 [==============================] - 3s 126ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0163\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_6/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 7\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0154\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_7/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 8\n",
      "15/15 [==============================] - 3s 128ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0150\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_8/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 9\n",
      "15/15 [==============================] - 3s 127ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0152\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_9/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 10\n",
      "15/15 [==============================] - 3s 126ms/step\n",
      "Epoch 1/30\n",
      "118/118 [==============================] - 1s 2ms/step - loss: 0.0152\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Saved PTS model weights to: ../results/PI_Explainability/resnet50_cifar10/run_10/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pvi_class_val.npy')\n",
    "    \n",
    "    opt_temp, opt_weights = temp_scaling.ensemble_temp_scaling_nll(scores, true_y_val, num_classes)\n",
    "    np.save(f'{exp_name}/pvi_opt_temp_ets_nll.npy', opt_temp)\n",
    "    np.save(f'{exp_name}/pvi_opt_weights_ets_nll.npy', opt_weights)\n",
    "    \n",
    "    pts = temp_scaling.PTSCalibrator(\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=64,\n",
    "    nlayers=2,\n",
    "    n_nodes=128,\n",
    "    length_logits=10,\n",
    "    top_k_logits=5\n",
    ")\n",
    "\n",
    "    pts.tune(logits=scores, labels=pred_y_val)\n",
    "    pts.save(path=f'{exp_name}/calibration_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e74734",
   "metadata": {},
   "source": [
    "### Failure Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d6368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_scores(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name):\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'\n",
    "    metric = conf_method.split('_')[-1] if 'temp_scaling' in conf_method else None\n",
    "    method_key = conf_method.replace(f'_temp_scaling_{metric}', '') if metric else conf_method\n",
    "\n",
    "    if method_key == 'softmax':\n",
    "        if metric:\n",
    "            opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "            return methods.max_softmax_prob(model, ds_test, opt_temp)\n",
    "        else:\n",
    "            return methods.max_softmax_prob(model, ds_test)\n",
    "\n",
    "    elif method_key in ['pmi', 'psi', 'pvi', 'pvi_best']:\n",
    "        if method_key == 'pmi':\n",
    "            exp_path = f'{base_path}/pmi/separable_variational_f_js'\n",
    "            class_file = 'pmi_output_class_test.npy'\n",
    "        elif method_key == 'psi':\n",
    "            exp_path = f'{base_path}/psi/gaussian'\n",
    "            class_file = 'psi_output_class_500_projs_test.npy'\n",
    "        elif method_key == 'pvi':\n",
    "            exp_path = f'{base_path}/pvi/training_from_scratch'\n",
    "            class_file = 'pvi_class_test.npy'\n",
    "        elif method_key == 'pvi_best':\n",
    "            exp_path = f'{base_path}/pvi/training_from_scratch'\n",
    "            class_file = 'pvi_class_best_test.npy'\n",
    "\n",
    "        opt_temp = np.load(f'{exp_path}/{method_key}_opt_temp_{metric}.npy')\n",
    "        scores_class = np.load(f'{exp_path}/{class_file}')\n",
    "        scores_class = np.array([utils.softmax(x / opt_temp) for x in scores_class])\n",
    "        return np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "\n",
    "    elif method_key == 'softmax_margin':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.softmax_margin(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'max_logits':\n",
    "        return methods.max_logits(model, ds_test)\n",
    "\n",
    "    elif method_key == 'logits_margin':\n",
    "        return methods.logits_margin(model, ds_test)\n",
    "\n",
    "    elif method_key == 'negative_entropy':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.negative_entropy(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'negative_gini':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.negative_gini(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'isotonic_regression':\n",
    "        return methods.isotonic_reg(model, ds_val, ds_test, true_y_val)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown confidence method: {conf_method}\")\n",
    "\n",
    "\n",
    "def evaluate_failure_pred(ds_test, true_y_test, conf_method, n_runs=10):\n",
    "    results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": [],\n",
    "        \"naurc\": []\n",
    "    }\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        scores_test = get_confidence_scores(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name)\n",
    "\n",
    "#         results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "#         results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "#         results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "#         results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "#         results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "#         results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"naurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_list = ['softmax_temp_scaling_aurc','pmi_temp_scaling_aurc','psi_temp_scaling_aurc','pvi_temp_scaling_aurc',\n",
    "                'softmax_margin_temp_scaling_aurc', 'max_logits', 'logits_margin', 'negative_entropy_temp_scaling_aurc',\n",
    "                'negative_gini_temp_scaling_aurc']\n",
    "for method in methods_list:\n",
    "    print(f'Method: {method}')\n",
    "    results = evaluate_failure_pred(ds_test, true_y_test, conf_method=f'{method}', n_runs=10)\n",
    "#     print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "#     print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "#     print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "#     print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "#     print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "#     print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")\n",
    "    print(f\"NAURC           : {utils.format_ci(results['naurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c64d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax ETS\n",
      "AUROC           : 93.59 (0.24)\n",
      "AUPRC (success) : 99.63 (0.02)\n",
      "AUPRC (error)   : 43.41 (1.20)\n",
      "FPR at 95% TPR  : 36.33 (1.23)\n",
      "AURC            : 4.68 (0.26)\n",
      "EAURC           : 3.52 (0.22)\n"
     ]
    }
   ],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'softmax ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_ets_nll.npy')\n",
    "        opt_weights = np.load(f'{base_path}/softmax_opt_weights_ets_nll.npy')\n",
    "        \n",
    "        scores_class = apply_ets(logits,opt_temp,opt_weights,num_classes)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41d9b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: PVI ETS\n",
      "AUROC           : 95.14 (0.30)\n",
      "AUPRC (success) : 99.72 (0.02)\n",
      "AUPRC (error)   : 60.99 (1.60)\n",
      "FPR at 95% TPR  : 25.77 (1.56)\n",
      "AURC            : 3.88 (0.15)\n",
      "EAURC           : 2.72 (0.17)\n"
     ]
    }
   ],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'PVI ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "        opt_temp = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_temp_ets_nll.npy')\n",
    "        opt_weights = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_weights_ets_nll.npy')\n",
    "        \n",
    "        scores_class = apply_ets(pvi,opt_temp,opt_weights,num_classes)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f829e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax PTS\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_1/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 834us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_2/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 847us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_3/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 875us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_4/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 858us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_5/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 856us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_6/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 838us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_7/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 872us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_8/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 851us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_9/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 857us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_10/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 862us/step\n",
      "AUROC           : 65.28 (8.97)\n",
      "AUPRC (success) : 96.66 (0.92)\n",
      "AUPRC (error)   : 19.36 (7.37)\n",
      "FPR at 95% TPR  : N/A\n",
      "AURC            : 33.53 (9.08)\n",
      "EAURC           : 32.37 (9.05)\n"
     ]
    }
   ],
   "source": [
    "method = 'softmax PTS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_ets_nll.npy')\n",
    "        opt_weights = np.load(f'{base_path}/softmax_opt_weights_ets_nll.npy')\n",
    "        \n",
    "        pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "        pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "        scores_class = pts_loaded.calibrate(logits)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e52c46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: PVI PTS\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_1/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 814us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_2/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 806us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_3/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 816us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_4/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 808us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_5/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 828us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_6/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 856us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_7/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 811us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_8/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 857us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_9/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 848us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_10/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 876us/step\n",
      "AUROC           : 83.77 (1.76)\n",
      "AUPRC (success) : 98.39 (0.11)\n",
      "AUPRC (error)   : 47.22 (3.14)\n",
      "FPR at 95% TPR  : 26.13 (nan)\n",
      "AURC            : 16.65 (1.10)\n",
      "EAURC           : 15.48 (1.07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "method = 'PVI PTS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "        \n",
    "        pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "        pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "        scores_class = pts_loaded.calibrate(pvi)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04361b4",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167c5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_for_calibration(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name):\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'\n",
    "\n",
    "    def softmax_scaled(scores, temp=1.0):\n",
    "        return np.array([utils.softmax(x / temp) for x in scores])\n",
    "\n",
    "    if conf_method == 'softmax':\n",
    "        scores_class = methods.softmax_prob(model, ds_test)\n",
    "        scores_test = methods.max_softmax_prob(model, ds_test)\n",
    "        return scores_class, scores_test\n",
    "\n",
    "    if conf_method.startswith('softmax_temp_scaling'):\n",
    "        metric = conf_method.split('_')[-1]\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        scores_class = methods.softmax_prob(model, ds_test, opt_temp)\n",
    "        scores_test = methods.max_softmax_prob(model, ds_test, opt_temp)\n",
    "        return scores_class, scores_test\n",
    "\n",
    "    if conf_method in ['pmi', 'psi', 'pvi', 'pvi_best']:\n",
    "        method = conf_method\n",
    "        metric = None\n",
    "        temp = 1.0\n",
    "    elif conf_method.startswith(('pmi_temp_scaling', 'psi_temp_scaling', 'pvi_temp_scaling', 'pvi_best_temp_scaling')):\n",
    "        parts = conf_method.split('_')\n",
    "        method = '_'.join(parts[:2]) if 'best' in parts else parts[0]\n",
    "        metric = parts[-1]\n",
    "        method_dir = {\n",
    "            'pmi': 'pmi/separable_variational_f_js',\n",
    "            'psi': 'psi/gaussian',\n",
    "            'pvi': 'pvi/training_from_scratch',\n",
    "            'pvi_best': 'pvi/training_from_scratch'\n",
    "        }[method]\n",
    "        temp = float(np.load(f'{base_path}/{method_dir}/{method}_opt_temp_{metric}.npy'))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown confidence method: {conf_method}\")\n",
    "\n",
    "    method_paths = {\n",
    "        'pmi': (f'{base_path}/pmi/separable_variational_f_js', 'pmi_output_class_test.npy'),\n",
    "        'psi': (f'{base_path}/psi/gaussian', 'psi_output_class_500_projs_test.npy'),\n",
    "        'pvi': (f'{base_path}/pvi/training_from_scratch', 'pvi_class_test.npy'),\n",
    "        'pvi_best': (f'{base_path}/pvi/training_from_scratch', 'pvi_class_best_test.npy'),\n",
    "    }\n",
    "\n",
    "    method_path, class_file = method_paths[method]\n",
    "    scores_class = np.load(f'{method_path}/{class_file}')\n",
    "    scores_class = softmax_scaled(scores_class, temp)\n",
    "    scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "    return scores_class, scores_test\n",
    "\n",
    "def evaluate_calibration(ds_test, true_y_test, conf_method, n_runs=10):\n",
    "    results = {\n",
    "        \"ece\": [],\n",
    "        \"cc_ece\": [],\n",
    "        \"mce\": [],\n",
    "        \"ace\": [],\n",
    "        \"sce\": [],\n",
    "        \"ada_ece\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_ece\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"cw_ece\": [],\n",
    "        \"cw_sce\": [],\n",
    "        \"cw_ada_ece\": [],\n",
    "        \"cw_ada_sce\": [],\n",
    "        \"cw_ada_ece_rms\": [],\n",
    "        \"cw_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "        \"sharpness\": [],\n",
    "    }\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        scores_class, scores_test = get_scores_for_calibration(\n",
    "            conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name\n",
    "        )\n",
    "\n",
    "#         results[\"ece\"].append(metrics.compute_ece(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"cc_ece\"].append(metrics.compute_cc_ece(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"mce\"].append(metrics.compute_mce(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"ace\"].append(metrics.compute_ace(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"ada_ece\"].append(metrics.compute_adaece(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cc_ada_ece\"].append(metrics.compute_cc_adaece(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_ece\"].append(metrics.compute_cw_ece(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_sce\"].append(metrics.compute_cw_sce(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_ada_ece\"].append(metrics.compute_cw_adaece(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_ada_sce\"].append(metrics.compute_cw_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_ada_ece_rms\"].append(metrics.compute_cw_adaece_rms(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_ada_sce_rms\"].append(metrics.compute_cw_adaece_rms(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "#         results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "#         results[\"sharpness\"].append(metrics.compute_sharpness(scores_class))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c610531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax\n",
      "CC-Ada-SCE-RMS: 3.58 (0.17)\n",
      "Method: pmi\n",
      "CC-Ada-SCE-RMS: 4.85 (0.12)\n",
      "Method: psi\n",
      "CC-Ada-SCE-RMS: 5.16 (0.19)\n",
      "Method: pvi\n",
      "CC-Ada-SCE-RMS: 5.41 (0.10)\n",
      "Method: pvi_best\n",
      "CC-Ada-SCE-RMS: 5.24 (0.01)\n",
      "Method: softmax_temp_scaling_nll\n",
      "CC-Ada-SCE-RMS: 5.41 (0.10)\n",
      "Method: pmi_temp_scaling_nll\n",
      "CC-Ada-SCE-RMS: 5.41 (0.12)\n",
      "Method: psi_temp_scaling_nll\n",
      "CC-Ada-SCE-RMS: 5.41 (0.12)\n",
      "Method: pvi_temp_scaling_nll\n",
      "CC-Ada-SCE-RMS: 5.45 (0.12)\n",
      "Method: pvi_best_temp_scaling_nll\n",
      "CC-Ada-SCE-RMS: 5.25 (0.02)\n"
     ]
    }
   ],
   "source": [
    "methods_list = ['softmax','pmi','psi','pvi','pvi_best',\n",
    "                'softmax_temp_scaling_nll','pmi_temp_scaling_nll','psi_temp_scaling_nll','pvi_temp_scaling_nll','pvi_best_temp_scaling_nll']\n",
    "for method in methods_list:\n",
    "    print(f'Method: {method}')\n",
    "    results = evaluate_calibration(ds_test, true_y_test, conf_method=f'{method}', n_runs=10)\n",
    "#     print(f\"ECE:            {utils.format_ci(results['ece'], scale=100)}\")\n",
    "#     print(f\"CC-ECE:         {utils.format_ci(results['cc_ece'], scale=100)}\")\n",
    "#     print(f\"MCE:            {utils.format_ci(results['mce'], scale=100)}\")\n",
    "#     print(f\"ACE:            {utils.format_ci(results['ace'], scale=100)}\")\n",
    "#     print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "#     print(f\"Ada-ECE:        {utils.format_ci(results['ada_ece'], scale=100)}\")\n",
    "#     print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "#     print(f\"CC-Ada-ECE:     {utils.format_ci(results['cc_ada_ece'], scale=100)}\")\n",
    "#     print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "    print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "#     print(f\"CW-ECE:         {utils.format_ci(results['cw_ece'], scale=100)}\")\n",
    "#     print(f\"CW-SCE:         {utils.format_ci(results['cw_sce'], scale=100)}\")\n",
    "#     print(f\"CW-Ada-ECE:     {utils.format_ci(results['cw_ada_ece'], scale=100)}\")\n",
    "#     print(f\"CW-Ada-SCE:     {utils.format_ci(results['cw_ada_sce'], scale=100)}\")\n",
    "#     print(f\"CW-Ada-ECE-RMS: {utils.format_ci(results['cw_ada_ece_rms'], scale=100)}\")\n",
    "#     print(f\"CW-Ada-SCE-RMS: {utils.format_ci(results['cw_ada_sce_rms'], scale=100)}\")\n",
    "#     print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "#     print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")\n",
    "#     print(f\"Sharpness:      {utils.format_ci(results['sharpness'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'softmax ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "\n",
    "    logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "    opt_temp = np.load(f'{base_path}/softmax_opt_temp_ets_nll.npy')\n",
    "    opt_weights = np.load(f'{base_path}/softmax_opt_weights_ets_nll.npy')\n",
    "\n",
    "    scores_class = apply_ets(logits,opt_temp,opt_weights,num_classes)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c938a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax ETS\n",
      "SCE:            0.35 (0.02)\n",
      "Ada-SCE:        0.29 (0.03)\n",
      "CC-Ada-SCE:     0.62 (0.02)\n",
      "CC-Ada-SCE-RMS: 5.43 (0.12)\n",
      "NLL:            15.78 (0.46)\n",
      "Brier Score:    7.24 (0.20)\n"
     ]
    }
   ],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'PVI ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "    pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "    opt_temp = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_temp_ets_nll.npy')\n",
    "    opt_weights = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_weights_ets_nll.npy')\n",
    "\n",
    "    scores_class = apply_ets(pvi,opt_temp,opt_weights,num_classes)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "561ecc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax ETS\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_1/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 835us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_2/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 823us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_3/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 815us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_4/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 837us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_5/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 827us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_6/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 827us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_7/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 816us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_8/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 832us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_9/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 835us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_10/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 845us/step\n",
      "SCE:            0.95 (0.03)\n",
      "Ada-SCE:        0.45 (0.02)\n",
      "CC-Ada-SCE:     0.03 (0.01)\n",
      "CC-Ada-SCE-RMS: 0.21 (0.18)\n",
      "NLL:            67.29 (6.67)\n",
      "Brier Score:    9.41 (0.26)\n"
     ]
    }
   ],
   "source": [
    "method = 'softmax ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "\n",
    "    logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "\n",
    "    pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "    pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "    scores_class = pts_loaded.calibrate(logits)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a692e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: PVI PTS\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_1/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 843us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_2/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 851us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_3/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 844us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_4/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 864us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_5/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 871us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_6/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 841us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_7/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 874us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_8/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 874us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_9/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 834us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/resnet50_cifar10/run_10/calibration/calibration_model/pts_model.h5\n",
      "313/313 [==============================] - 0s 831us/step\n",
      "SCE:            0.94 (0.03)\n",
      "Ada-SCE:        0.44 (0.02)\n",
      "CC-Ada-SCE:     0.04 (0.03)\n",
      "CC-Ada-SCE-RMS: 0.29 (0.39)\n",
      "NLL:            65.03 (7.87)\n",
      "Brier Score:    9.36 (0.29)\n"
     ]
    }
   ],
   "source": [
    "method = 'PVI PTS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "    pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "\n",
    "    pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "    pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "    scores_class = pts_loaded.calibrate(pvi)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86f779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
