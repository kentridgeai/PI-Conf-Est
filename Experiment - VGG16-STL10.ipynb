{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc082a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:54:29.837059: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-09 10:54:29.837115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-09 10:54:29.838474: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 10:54:29.845780: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.pmi_estimators import train_critic_model, neural_pmi\n",
    "from src.psi_estimators import psi_gaussian_train, psi_gaussian_val_class\n",
    "from src.pvi_estimators import train_pvi_null_model, neural_pvi_class, neural_pvi_ensemble_class\n",
    "import src.utils as utils\n",
    "import src.metrics as metrics\n",
    "import src.methods as methods\n",
    "import src.temp_scaling as temp_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee612f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:54:33.112202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1926] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 74331 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:4e:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'vgg16'\n",
    "dataset_name = 'stl10'\n",
    "\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    'stl10',\n",
    "    split=['train', 'test[:15%]', 'test[15%:]'],\n",
    "    data_dir = '../tensorflow_datasets/',\n",
    "    shuffle_files=False,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "IMG_SIZE = 224\n",
    "num_classes = 10\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    label = tf.one_hot(label, depth=num_classes)\n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train.map(preprocess)\n",
    "ds_val = ds_val.map(preprocess)\n",
    "ds_test = ds_test.map(preprocess)\n",
    "\n",
    "# batch_size = 128\n",
    "# ds_train = ds_train.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "# ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "# ds_test = ds_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "true_y_train = np.argmax([y for x,y in ds_train], axis=1)\n",
    "true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "true_y_test = np.argmax([y for x,y in ds_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1e5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    base_model = VGG16(include_top=False, weights='imagenet', input_tensor=Input(shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(10, activation='linear')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b787a01e",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2bfe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    model = create_model()\n",
    "    \n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1)\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=100, callbacks=[lr_scheduler, early_stop])\n",
    "    \n",
    "    if not os.path.exists(exp_name+'/saved_models'):\n",
    "        print(\"Making directory\", exp_name+'/saved_models')\n",
    "        os.makedirs(exp_name+'/saved_models')\n",
    "\n",
    "    model.save_weights(f'{exp_name}/saved_models/trained_weights.h5')\n",
    "    with open(f'{exp_name}/history.pickle', 'wb') as f:\n",
    "        pickle.dump(history, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b50154",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "test_acc = []\n",
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=loss_fn, metrics=['accuracy'])\n",
    "    train_acc.append(model.evaluate(ds_train.batch(256), verbose=0)[1])\n",
    "    val_acc.append(model.evaluate(ds_val.batch(256), verbose=0)[1])\n",
    "    test_acc.append(model.evaluate(ds_test.batch(256), verbose=0)[1])\n",
    "    print(f'Test accuracy: {test_acc[-1]*100:.2f}')\n",
    "print(f'Average train error: {(100-np.mean(train_acc)*100):.2f}, ({(np.std(train_acc)*100):.2f})')\n",
    "print(f'Average validation error: {(100-np.mean(val_acc)*100):.2f} ({(np.std(val_acc)*100):.2f})')\n",
    "print(f'Average test error: {(100-np.mean(test_acc)*100):.2f} ({(np.std(test_acc)*100):.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c41c5f",
   "metadata": {},
   "source": [
    "### PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734faf7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(1,10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pmi/separable_variational_f_js'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "\n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PMI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "\n",
    "    print(f'Training PMI model...')\n",
    "    ds_activity_trn = ds_train.batch(128).map(lambda x, y: (int_model(x), y)).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    ds_activity_val = ds_val.batch(128).map(lambda x, y: (int_model(x), y)).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    train_critic_model(ds_activity_trn, ds_activity_val, critic='separable', estimator='variational_f_js', epochs=200, save_path=f'{exp_name}/pmi_output_model')\n",
    "\n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PMI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "\n",
    "    pmi_model = tf.keras.models.load_model(f'{exp_name}/pmi_output_model')\n",
    "    n_classes = 10\n",
    "\n",
    "    print(f'Computing PMI for all validation samples and for all classes...')\n",
    "    encoded_x = []\n",
    "    for x, _ in ds_val.batch(128):\n",
    "        encoded_x.append(int_model(x).numpy())\n",
    "    encoded_x = np.concatenate(encoded_x)\n",
    "    num_samples = encoded_x.shape[0]\n",
    "    \n",
    "    pmi_class = []\n",
    "    batch_size = 1024\n",
    "    for k in range(n_classes):\n",
    "        num_samples = encoded_x.shape[0]\n",
    "        y_k = tf.one_hot(tf.fill([num_samples], k), depth=n_classes)\n",
    "        pmi_list = []\n",
    "        for i in tqdm(range(0, len(encoded_x), batch_size), desc=f\"Computing PMI for class {k+1}\"):\n",
    "            x_batch = encoded_x[i:i+batch_size]\n",
    "            y_batch = y_k[i:i+batch_size]\n",
    "            pmi = neural_pmi(x_batch, y_batch, pmi_model, estimator='variational_f_js')\n",
    "            pmi_list += np.array(pmi).tolist()\n",
    "        pmi_class.append(pmi_list)\n",
    "    np.save(f'{exp_name}/pmi_output_class_val.npy', np.array(pmi_class).T)\n",
    "    \n",
    "    print(f'Computing PMI for all test samples and for all classes...')\n",
    "    encoded_x = []\n",
    "    for x, _ in ds_test.batch(128):\n",
    "        encoded_x.append(int_model(x).numpy())\n",
    "    encoded_x = np.concatenate(encoded_x)\n",
    "    num_samples = encoded_x.shape[0]\n",
    "    \n",
    "    pmi_class = []\n",
    "    batch_size = 1024\n",
    "    for k in range(n_classes):\n",
    "        num_samples = encoded_x.shape[0]\n",
    "        y_k = tf.one_hot(tf.fill([num_samples], k), depth=n_classes)\n",
    "        pmi_list = []\n",
    "        for i in tqdm(range(0, len(encoded_x), batch_size), desc=f\"Computing PMI for class {k+1}\"):\n",
    "            x_batch = encoded_x[i:i+batch_size]\n",
    "            y_batch = y_k[i:i+batch_size]\n",
    "            pmi = neural_pmi(x_batch, y_batch, pmi_model, estimator='variational_f_js')\n",
    "            pmi_list += np.array(pmi).tolist()\n",
    "        pmi_class.append(pmi_list)\n",
    "    np.save(f'{exp_name}/pmi_output_class_test.npy', np.array(pmi_class).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc423397",
   "metadata": {},
   "source": [
    "### PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3c9f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/gaussian'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PSI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    x_logits_list = []\n",
    "    y_labels_list = []\n",
    "\n",
    "    for x_batch, y_batch in ds_train.batch(256):\n",
    "        logits = int_model(x_batch)\n",
    "        labels = tf.argmax(y_batch, axis=1)\n",
    "        x_logits_list.append(logits)\n",
    "        y_labels_list.append(labels)\n",
    "\n",
    "    x = tf.concat(x_logits_list, axis=0).numpy()\n",
    "    y = tf.concat(y_labels_list, axis=0).numpy()\n",
    "    \n",
    "    print(f'Training PSI model (gaussian)...')\n",
    "    psi_data = psi_gaussian_train(x, y, n_projs=500)\n",
    "    np.save(f'{exp_name}/gaussian_output_model_500_projs.npy', psi_data)\n",
    "\n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PSI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "\n",
    "    psi_data = np.load(f'{exp_name}/gaussian_output_model_500_projs.npy', allow_pickle=True).item()\n",
    "\n",
    "    print(f'Computing PSI for all validation samples...')\n",
    "    x_logits_list = []\n",
    "\n",
    "    for x_batch, y_batch in ds_val.batch(256):\n",
    "        logits = int_model(x_batch)\n",
    "        x_logits_list.append(logits)\n",
    "    \n",
    "    x = tf.concat(x_logits_list, axis=0).numpy()\n",
    "    psi_class, pmi_arr = psi_gaussian_val_class(x, psi_data)\n",
    "    np.save(f'{exp_name}/psi_output_class_500_projs_val.npy', np.array(psi_class))\n",
    "\n",
    "    print(f'Computing PSI for all test samples...')\n",
    "    x_logits_list = []\n",
    "\n",
    "    for x_batch, y_batch in ds_test.batch(256):\n",
    "        logits = int_model(x_batch)\n",
    "        x_logits_list.append(logits)\n",
    "    \n",
    "    x = tf.concat(x_logits_list, axis=0).numpy()\n",
    "    psi_class, pmi_arr = psi_gaussian_val_class(x, psi_data)\n",
    "    np.save(f'{exp_name}/psi_output_class_500_projs_test.npy', np.array(psi_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fdd37c",
   "metadata": {},
   "source": [
    "### PVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060dd092",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_runs = list(range(10))\n",
    "while any(random_runs[i] == i for i in range(10)):\n",
    "    np.random.shuffle(random_runs)\n",
    "    \n",
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{random_runs[run]+1}/saved_models/trained_weights.h5')\n",
    "    pvi_model.save_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    \n",
    "    untrained_model = create_model()\n",
    "    train_pvi_null_model(ds_train, untrained_model, epochs=10, save_path=f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    null_model = create_model()\n",
    "    null_model.load_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "    opt_temp_pvi = temp_scaling.temp_scaling_nll(pvi_model.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "    opt_temp_null = temp_scaling.temp_scaling_nll(null_model.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "\n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_val.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_test.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a216e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/finetuned'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    pvi_model.compile(optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1)\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "    pvi_model.fit(ds_train.batch(256), validation_data=ds_val.batch(256), epochs=100, callbacks=[lr_scheduler, early_stop])\n",
    "    \n",
    "    pvi_model.save_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    \n",
    "    untrained_model = create_model()\n",
    "    untrained_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_null_model_weights.h5')\n",
    "    untrained_model.save_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    null_model = create_model()\n",
    "    null_model.load_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "    opt_temp_pvi = temp_scaling.temp_scaling_nll(pvi_model.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "    opt_temp_null = temp_scaling.temp_scaling_nll(null_model.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "\n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_val.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_test.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69382dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pvi_runs = [9 if i == 4 else 4 for i in range(10)]\n",
    "    \n",
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{pvi_runs[run]+1}/saved_models/trained_weights.h5')\n",
    "    pvi_model.save_weights(f'{exp_name}/pvi_model_best_weights.h5')\n",
    "    \n",
    "#     untrained_model = create_model()\n",
    "#     train_pvi_null_model(ds_train, untrained_model, epochs=10, save_path=f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'{exp_name}/pvi_model_best_weights.h5')\n",
    "    null_model = create_model()\n",
    "    null_model.load_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "    opt_temp_pvi = temp_scaling.temp_scaling_nll(pvi_model.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "    opt_temp_null = temp_scaling.temp_scaling_nll(null_model.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "\n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_val.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_best_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_test.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_best_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e86eda",
   "metadata": {},
   "source": [
    "### Ensemble PVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228bf89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/ensemble_no_training_training_from_scratch_calibrated'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model_1 = create_model()\n",
    "    pvi_model_1.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    null_model_1 = create_model()\n",
    "    null_model_1.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_null_model_weights.h5')\n",
    "    pvi_model_2 = create_model()\n",
    "    pvi_model_2.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_model_weights.h5')\n",
    "    null_model_2 = create_model()\n",
    "    null_model_2.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_null_model_weights.h5')\n",
    "    \n",
    "    true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "    opt_temp_pvi_1 = temp_scaling.temp_scaling_nll(pvi_model_1.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    opt_temp_pvi_2 = temp_scaling.temp_scaling_nll(pvi_model_2.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "    opt_temp_null = temp_scaling.temp_scaling_nll(null_model_1.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = []\n",
    "    for (x_batch, y_batch) in ds_val.batch(256):\n",
    "        pvi = neural_pvi_ensemble_class([x_batch, x_batch], [pvi_model_1, pvi_model_2], [null_model_1, null_model_2], [opt_temp_pvi_1, opt_temp_pvi_2], [opt_temp_null, opt_temp_null])\n",
    "        pvi_class += np.array(pvi).tolist()\n",
    "    np.save(f'{exp_name}/pvi_class_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = []\n",
    "    for (x_batch, y_batch) in ds_test.batch(256):\n",
    "        pvi = neural_pvi_ensemble_class([x_batch, x_batch], [pvi_model_1, pvi_model_2], [null_model_1, null_model_2], [opt_temp_pvi_1, opt_temp_pvi_2], [opt_temp_null, opt_temp_null])\n",
    "        pvi_class += np.array(pvi).tolist()\n",
    "    np.save(f'{exp_name}/pvi_class_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e0303",
   "metadata": {},
   "source": [
    "### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b781fbb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "#     if not os.path.exists(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'):\n",
    "#         print(\"Making directory\", f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')\n",
    "#         os.makedirs(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')                                  \n",
    "  \n",
    "    \n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = model.predict(ds_val.batch(512), verbose=0)\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)\n",
    "#     np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_nll.npy', opt_temp)\n",
    "\n",
    "#     opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)\n",
    "#     np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90afb856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 09:50:41.598307: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 27s 6s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 09:51:13.250867: I external/local_xla/xla/service/service.cc:168] XLA service 0x556118952d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-09 09:51:13.250899: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2025-04-09 09:51:13.255793: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744192273.375403 1229886 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 2\n",
      "3/3 [==============================] - 1s 271ms/step\n",
      "Run: 3\n",
      "3/3 [==============================] - 1s 268ms/step\n",
      "Run: 4\n",
      "3/3 [==============================] - 1s 274ms/step\n",
      "Run: 5\n",
      "3/3 [==============================] - 1s 267ms/step\n",
      "Run: 6\n",
      "3/3 [==============================] - 1s 276ms/step\n",
      "Run: 7\n",
      "3/3 [==============================] - 1s 276ms/step\n",
      "Run: 8\n",
      "3/3 [==============================] - 1s 277ms/step\n",
      "Run: 9\n",
      "3/3 [==============================] - 1s 275ms/step\n",
      "Run: 10\n",
      "3/3 [==============================] - 1s 275ms/step\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "#     if not os.path.exists(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'):\n",
    "#         print(\"Making directory\", f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')\n",
    "#         os.makedirs(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')                                  \n",
    "  \n",
    "    \n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = model.predict(ds_val.batch(512), verbose=0)\n",
    "    \n",
    "    opt_temp, opt_weights = temp_scaling.ensemble_temp_scaling_nll(scores, true_y_val, num_classes)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_ets_nll.npy', opt_temp)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_weights_ets_nll.npy', opt_weights)\n",
    "\n",
    "#     opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)\n",
    "#     np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3136d42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:41:20.088768: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 27s 6s/step\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:41:52.879519: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f272eba0e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-09 10:41:52.879566: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2025-04-09 10:41:52.885078: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744195313.006583 1372796 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 2s 3ms/step - loss: 0.0462\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0163\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_1/calibration/calibration_model/pts_model.h5\n",
      "Run: 2\n",
      "3/3 [==============================] - 1s 268ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0210\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0063\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_2/calibration/calibration_model/pts_model.h5\n",
      "Run: 3\n",
      "3/3 [==============================] - 1s 269ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0099\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0063\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0059\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0059\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_3/calibration/calibration_model/pts_model.h5\n",
      "Run: 4\n",
      "3/3 [==============================] - 1s 271ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0702\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0261\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_4/calibration/calibration_model/pts_model.h5\n",
      "Run: 5\n",
      "3/3 [==============================] - 1s 270ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0057\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0063\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0059\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_5/calibration/calibration_model/pts_model.h5\n",
      "Run: 6\n",
      "3/3 [==============================] - 1s 272ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0874\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0231\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0255\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0305\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0258\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0062\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_6/calibration/calibration_model/pts_model.h5\n",
      "Run: 7\n",
      "3/3 [==============================] - 1s 273ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 3ms/step - loss: 0.0268\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0344\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0350\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0054\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0046\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0036\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0044\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_7/calibration/calibration_model/pts_model.h5\n",
      "Run: 8\n",
      "3/3 [==============================] - 1s 269ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.1206\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0050\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_8/calibration/calibration_model/pts_model.h5\n",
      "Run: 9\n",
      "3/3 [==============================] - 1s 271ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0221\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0196\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0186\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0233\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_9/calibration/calibration_model/pts_model.h5\n",
      "Run: 10\n",
      "3/3 [==============================] - 1s 274ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 3ms/step - loss: 0.3882\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0063\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0054\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0052\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_10/calibration/calibration_model/pts_model.h5\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    \n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = model.predict(ds_val.batch(512), verbose=0)\n",
    "    \n",
    "    pts = temp_scaling.PTSCalibrator(\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=64,\n",
    "    nlayers=2,\n",
    "    n_nodes=32,\n",
    "    length_logits=10,\n",
    "    top_k_logits=5\n",
    ")\n",
    "\n",
    "    pts.tune(logits=scores, labels=pred_y_val)\n",
    "    pts.save(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3aff7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pmi/separable_variational_f_js'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pmi_output_class_val.npy')\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)                            \n",
    "#     np.save(f'{exp_name}/pmi_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)                                \n",
    "    np.save(f'{exp_name}/pmi_opt_temp_nll.npy', opt_temp)\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)                                \n",
    "#     np.save(f'{exp_name}/pmi_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b022c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/gaussian'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/psi_output_class_500_projs_val.npy')\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)                                 \n",
    "#     np.save(f'{exp_name}/psi_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)                            \n",
    "    np.save(f'{exp_name}/psi_opt_temp_nll.npy', opt_temp)\n",
    "    \n",
    "#     opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)                            \n",
    "#     np.save(f'{exp_name}/psi_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abaf760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/finetuned'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pvi_class_val.npy')\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)                                 \n",
    "    np.save(f'{exp_name}/pvi_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)                                          \n",
    "    np.save(f'{exp_name}/pvi_opt_temp_nll.npy', opt_temp)\n",
    "\n",
    "#     opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)                                          \n",
    "#     np.save(f'{exp_name}/pvi_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8862c4ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "3/3 [==============================] - 1s 279ms/step\n",
      "Run: 2\n",
      "3/3 [==============================] - 1s 271ms/step\n",
      "Run: 3\n",
      "3/3 [==============================] - 1s 271ms/step\n",
      "Run: 4\n",
      "WARNING:tensorflow:5 out of the last 51 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9af82e3520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 51 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9af82e3520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 274ms/step\n",
      "Run: 5\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9af82e31c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9af82e31c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 270ms/step\n",
      "Run: 6\n",
      "3/3 [==============================] - 1s 278ms/step\n",
      "Run: 7\n",
      "3/3 [==============================] - 1s 272ms/step\n",
      "Run: 8\n",
      "3/3 [==============================] - 1s 269ms/step\n",
      "Run: 9\n",
      "3/3 [==============================] - 1s 271ms/step\n",
      "Run: 10\n",
      "3/3 [==============================] - 1s 270ms/step\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pvi_class_val.npy')\n",
    "    \n",
    "    opt_temp, opt_weights = temp_scaling.ensemble_temp_scaling_nll(scores, true_y_val, num_classes)\n",
    "    np.save(f'{exp_name}/pvi_opt_temp_ets_nll.npy', opt_temp)\n",
    "    np.save(f'{exp_name}/pvi_opt_weights_ets_nll.npy', opt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554872af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:54:37.802920: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 27s 6s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:55:05.150416: I external/local_xla/xla/service/service.cc:168] XLA service 0x55f3474445e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-09 10:55:05.150458: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2025-04-09 10:55:05.155417: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744196105.268334 1459893 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "19/19 [==============================] - 2s 2ms/step - loss: 0.0218\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0186\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_1/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 2\n",
      "3/3 [==============================] - 4s 2s/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0221\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0133\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_2/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 3\n",
      "3/3 [==============================] - 1s 270ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0211\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0181\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0158\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0143\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0117\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0107\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0101\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_3/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 4\n",
      "3/3 [==============================] - 1s 275ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0209\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0181\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_4/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 5\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9c50101360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9c50101360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 273ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0190\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0086\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0076\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0076\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0069\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0069\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_5/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 6\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9c50103eb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9c50103eb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 273ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 3ms/step - loss: 0.0216\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0116\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0104\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_6/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 7\n",
      "3/3 [==============================] - 1s 271ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 3ms/step - loss: 0.0209\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0179\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0122\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0093\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0092\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0091\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_7/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 8\n",
      "3/3 [==============================] - 1s 266ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 3ms/step - loss: 0.0209\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0120\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_8/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 9\n",
      "3/3 [==============================] - 1s 270ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 3ms/step - loss: 0.0230\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0199\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0122\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0120\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_9/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n",
      "Run: 10\n",
      "3/3 [==============================] - 1s 269ms/step\n",
      "Epoch 1/30\n",
      "19/19 [==============================] - 1s 2ms/step - loss: 0.0220\n",
      "Epoch 2/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 3/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 4/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 5/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 6/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 7/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 8/30\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.0123\n",
      "Epoch 9/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 10/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 11/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 12/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 13/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 14/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 15/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 16/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 17/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 18/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 19/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 20/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 21/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 22/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 23/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 24/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 25/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 26/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 27/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 28/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 29/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 30/30\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Saved PTS model weights to: ../results/PI_Explainability/vgg16_stl10/run_10/calibration/pvi/training_from_scratch/calibration_model/pts_model.h5\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pvi_class_val.npy')\n",
    "    \n",
    "    opt_temp, opt_weights = temp_scaling.ensemble_temp_scaling_nll(scores, true_y_val, num_classes)\n",
    "    np.save(f'{exp_name}/pvi_opt_temp_ets_nll.npy', opt_temp)\n",
    "    np.save(f'{exp_name}/pvi_opt_weights_ets_nll.npy', opt_weights)\n",
    "    \n",
    "    pts = temp_scaling.PTSCalibrator(\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=64,\n",
    "    nlayers=2,\n",
    "    n_nodes=128,\n",
    "    length_logits=10,\n",
    "    top_k_logits=5\n",
    ")\n",
    "\n",
    "    pts.tune(logits=scores, labels=pred_y_val)\n",
    "    pts.save(path=f'{exp_name}/calibration_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494558ca",
   "metadata": {},
   "source": [
    "### Failure Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f8337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_scores(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name):\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'\n",
    "    metric = conf_method.split('_')[-1] if 'temp_scaling' in conf_method else None\n",
    "    method_key = conf_method.replace(f'_temp_scaling_{metric}', '') if metric else conf_method\n",
    "\n",
    "    if method_key == 'softmax':\n",
    "        if metric:\n",
    "            opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "            return methods.max_softmax_prob(model, ds_test, opt_temp)\n",
    "        else:\n",
    "            return methods.max_softmax_prob(model, ds_test)\n",
    "\n",
    "    elif method_key in ['pmi', 'psi', 'pvi', 'pvi_best']:\n",
    "        if method_key == 'pmi':\n",
    "            exp_path = f'{base_path}/pmi/separable_variational_f_js'\n",
    "            class_file = 'pmi_output_class_test.npy'\n",
    "        elif method_key == 'psi':\n",
    "            exp_path = f'{base_path}/psi/gaussian'\n",
    "            class_file = 'psi_output_class_500_projs_test.npy'\n",
    "        elif method_key == 'pvi':\n",
    "            exp_path = f'{base_path}/pvi/training_from_scratch'\n",
    "            class_file = 'pvi_class_test.npy'\n",
    "        elif method_key == 'pvi_best':\n",
    "            exp_path = f'{base_path}/pvi/training_from_scratch'\n",
    "            class_file = 'pvi_class_best_test.npy'\n",
    "\n",
    "        opt_temp = np.load(f'{exp_path}/{method_key}_opt_temp_{metric}.npy')\n",
    "        scores_class = np.load(f'{exp_path}/{class_file}')\n",
    "        scores_class = np.array([utils.softmax(x / opt_temp) for x in scores_class])\n",
    "        return np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "\n",
    "    elif method_key == 'softmax_margin':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.softmax_margin(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'max_logits':\n",
    "        return methods.max_logits(model, ds_test)\n",
    "\n",
    "    elif method_key == 'logits_margin':\n",
    "        return methods.logits_margin(model, ds_test)\n",
    "\n",
    "    elif method_key == 'negative_entropy':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.negative_entropy(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'negative_gini':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.negative_gini(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'isotonic_regression':\n",
    "        return methods.isotonic_reg(model, ds_val, ds_test, true_y_val)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown confidence method: {conf_method}\")\n",
    "\n",
    "\n",
    "def evaluate_failure_pred(ds_test, true_y_test, conf_method, n_runs=10):\n",
    "    results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        scores_test = get_confidence_scores(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name)\n",
    "\n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods_list = ['softmax_temp_scaling_aurc','pmi_temp_scaling_aurc','psi_temp_scaling_aurc','pvi_temp_scaling_aurc',\n",
    "#                 'softmax_margin_temp_scaling_aurc', 'max_logits', 'logits_margin', 'negative_entropy_temp_scaling_aurc',\n",
    "#                 'negative_gini_temp_scaling_aurc']\n",
    "methods_list = ['softmax_temp_scaling_aurc','pmi_temp_scaling_aurc','psi_temp_scaling_aurc','pvi_temp_scaling_aurc',]\n",
    "for method in methods_list:\n",
    "    print(f'Method: {method}')\n",
    "    results = evaluate_failure_pred(ds_test, true_y_test, conf_method=f'{method}', n_runs=10)\n",
    "    print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "    print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "    print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "    print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "    print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "    print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_scores(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name):\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'\n",
    "    metric = conf_method.split('_')[-1] if 'temp_scaling' in conf_method else None\n",
    "    method_key = conf_method.replace(f'_temp_scaling_{metric}', '') if metric else conf_method\n",
    "\n",
    "    if method_key == 'softmax':\n",
    "        if metric:\n",
    "            opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "            return methods.max_softmax_prob(model, ds_test, opt_temp)\n",
    "        else:\n",
    "            return methods.max_softmax_prob(model, ds_test)\n",
    "\n",
    "    elif method_key in ['pmi', 'psi', 'pvi', 'pvi_best']:\n",
    "        if method_key == 'pmi':\n",
    "            exp_path = f'{base_path}/pmi/separable_variational_f_js'\n",
    "            class_file = 'pmi_output_class_test.npy'\n",
    "        elif method_key == 'psi':\n",
    "            exp_path = f'{base_path}/psi/gaussian'\n",
    "            class_file = 'psi_output_class_500_projs_test.npy'\n",
    "        elif method_key == 'pvi':\n",
    "            exp_path = f'{base_path}/pvi/training_from_scratch'\n",
    "            class_file = 'pvi_class_test.npy'\n",
    "        elif method_key == 'pvi_best':\n",
    "            exp_path = f'{base_path}/pvi/training_from_scratch'\n",
    "            class_file = 'pvi_class_best_test.npy'\n",
    "\n",
    "        opt_temp = np.load(f'{exp_path}/{method_key}_opt_temp_{metric}.npy')\n",
    "        scores_class = np.load(f'{exp_path}/{class_file}')\n",
    "        scores_class = np.array([utils.softmax(x / opt_temp) for x in scores_class])\n",
    "        return np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "\n",
    "    elif method_key == 'softmax_margin':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.softmax_margin(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'max_logits':\n",
    "        return methods.max_logits(model, ds_test)\n",
    "\n",
    "    elif method_key == 'logits_margin':\n",
    "        return methods.logits_margin(model, ds_test)\n",
    "\n",
    "    elif method_key == 'negative_entropy':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.negative_entropy(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'negative_gini':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.negative_gini(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'isotonic_regression':\n",
    "        return methods.isotonic_reg(model, ds_val, ds_test, true_y_val)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown confidence method: {conf_method}\")\n",
    "\n",
    "\n",
    "def evaluate_failure_pred(ds_test, true_y_test, conf_method, n_runs=10):\n",
    "    results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        scores_test = get_confidence_scores(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name)\n",
    "\n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fddb2f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax ETS\n",
      "AUROC           : 92.04 (0.25)\n",
      "AUPRC (success) : 99.15 (0.05)\n",
      "AUPRC (error)   : 50.76 (1.23)\n",
      "FPR at 95% TPR  : 48.24 (1.71)\n",
      "AURC            : 12.05 (0.75)\n",
      "EAURC           : 7.93 (0.48)\n"
     ]
    }
   ],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'softmax ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_ets_nll.npy')\n",
    "        opt_weights = np.load(f'{base_path}/softmax_opt_weights_ets_nll.npy')\n",
    "        \n",
    "        scores_class = apply_ets(logits,opt_temp,opt_weights,num_classes)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73603d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: PVI ETS\n",
      "AUROC           : 92.79 (0.62)\n",
      "AUPRC (success) : 99.23 (0.06)\n",
      "AUPRC (error)   : 55.88 (3.50)\n",
      "FPR at 95% TPR  : 41.97 (3.61)\n",
      "AURC            : 11.30 (0.54)\n",
      "EAURC           : 7.18 (0.53)\n"
     ]
    }
   ],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'PVI ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "        opt_temp = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_temp_ets_nll.npy')\n",
    "        opt_weights = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_weights_ets_nll.npy')\n",
    "        \n",
    "        scores_class = apply_ets(pvi,opt_temp,opt_weights,num_classes)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7695be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax PTS\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_1/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 1ms/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_2/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 960us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_3/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 852us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_4/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 834us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_5/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 858us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_6/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 840us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_7/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 975us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_8/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 859us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_9/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 842us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_10/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 858us/step\n",
      "AUROC           : 64.99 (5.17)\n",
      "AUPRC (success) : 93.69 (1.05)\n",
      "AUPRC (error)   : 25.59 (4.91)\n",
      "FPR at 95% TPR  : 57.70 (nan)\n",
      "AURC            : 63.75 (9.96)\n",
      "EAURC           : 59.62 (10.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "method = 'softmax PTS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "        \n",
    "        pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "        pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "        scores_class = pts_loaded.calibrate(logits)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca37f541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: PVI PTS\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_1/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 1ms/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_2/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 849us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_3/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 828us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_4/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 860us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_5/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 938us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_6/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 981us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_7/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 841us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_8/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 854us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_9/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 835us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_10/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 826us/step\n",
      "AUROC           : 70.13 (6.40)\n",
      "AUPRC (success) : 92.61 (2.20)\n",
      "AUPRC (error)   : 41.37 (3.44)\n",
      "FPR at 95% TPR  : 49.19 (3.42)\n",
      "AURC            : 76.60 (22.72)\n",
      "EAURC           : 72.47 (22.78)\n"
     ]
    }
   ],
   "source": [
    "method = 'PVI PTS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "        \n",
    "        pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "        pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "        scores_class = pts_loaded.calibrate(pvi)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a546e80",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8038ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_for_calibration(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name):\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'\n",
    "\n",
    "    def softmax_scaled(scores, temp=1.0):\n",
    "        return np.array([utils.softmax(x / temp) for x in scores])\n",
    "\n",
    "    if conf_method == 'softmax':\n",
    "        scores_class = methods.softmax_prob(model, ds_test)\n",
    "        scores_test = methods.max_softmax_prob(model, ds_test)\n",
    "        return scores_class, scores_test\n",
    "\n",
    "    if conf_method.startswith('softmax_temp_scaling'):\n",
    "        metric = conf_method.split('_')[-1]\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        scores_class = methods.softmax_prob(model, ds_test, opt_temp)\n",
    "        scores_test = methods.max_softmax_prob(model, ds_test, opt_temp)\n",
    "        return scores_class, scores_test\n",
    "\n",
    "    if conf_method in ['pmi', 'psi', 'pvi', 'pvi_best']:\n",
    "        method = conf_method\n",
    "        metric = None\n",
    "        temp = 1.0\n",
    "    elif conf_method.startswith(('pmi_temp_scaling', 'psi_temp_scaling', 'pvi_temp_scaling', 'pvi_best_temp_scaling')):\n",
    "        parts = conf_method.split('_')\n",
    "        method = '_'.join(parts[:2]) if 'best' in parts else parts[0]\n",
    "        metric = parts[-1]\n",
    "        method_dir = {\n",
    "            'pmi': 'pmi/separable_variational_f_js',\n",
    "            'psi': 'psi/gaussian',\n",
    "            'pvi': 'pvi/training_from_scratch',\n",
    "            'pvi_best': 'pvi/training_from_scratch'\n",
    "        }[method]\n",
    "        temp = float(np.load(f'{base_path}/{method_dir}/{method}_opt_temp_{metric}.npy'))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown confidence method: {conf_method}\")\n",
    "\n",
    "    method_paths = {\n",
    "        'pmi': (f'{base_path}/pmi/separable_variational_f_js', 'pmi_output_class_test.npy'),\n",
    "        'psi': (f'{base_path}/psi/gaussian', 'psi_output_class_500_projs_test.npy'),\n",
    "        'pvi': (f'{base_path}/pvi/training_from_scratch', 'pvi_class_test.npy'),\n",
    "        'pvi_best': (f'{base_path}/pvi/training_from_scratch', 'pvi_class_best_test.npy'),\n",
    "    }\n",
    "\n",
    "    method_path, class_file = method_paths[method]\n",
    "    scores_class = np.load(f'{method_path}/{class_file}')\n",
    "    scores_class = softmax_scaled(scores_class, temp)\n",
    "    scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "    return scores_class, scores_test\n",
    "\n",
    "def evaluate_calibration(ds_test, true_y_test, conf_method, n_runs=10):\n",
    "    results = {\n",
    "        \"ece\": [],\n",
    "        \"cc_ece\": [],\n",
    "        \"mce\": [],\n",
    "        \"ace\": [],\n",
    "        \"sce\": [],\n",
    "        \"ada_ece\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_ece\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"cw_ece\": [],\n",
    "        \"cw_sce\": [],\n",
    "        \"cw_ada_ece\": [],\n",
    "        \"cw_ada_sce\": [],\n",
    "        \"cw_ada_ece_rms\": [],\n",
    "        \"cw_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "        \"sharpness\": [],\n",
    "    }\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        scores_class, scores_test = get_scores_for_calibration(\n",
    "            conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name\n",
    "        )\n",
    "\n",
    "#         results[\"ece\"].append(metrics.compute_ece(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"cc_ece\"].append(metrics.compute_cc_ece(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"mce\"].append(metrics.compute_mce(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"ace\"].append(metrics.compute_ace(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"ada_ece\"].append(metrics.compute_adaece(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cc_ada_ece\"].append(metrics.compute_cc_adaece(scores_test, pred_y_test, true_y_test, 15))\n",
    "#         results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_ece\"].append(metrics.compute_cw_ece(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_sce\"].append(metrics.compute_cw_sce(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_ada_ece\"].append(metrics.compute_cw_adaece(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_ada_sce\"].append(metrics.compute_cw_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_ada_ece_rms\"].append(metrics.compute_cw_adaece_rms(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"cw_ada_sce_rms\"].append(metrics.compute_cw_adaece_rms(scores_class, true_y_test, num_classes, 15))\n",
    "#         results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "#         results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "#         results[\"sharpness\"].append(metrics.compute_sharpness(scores_class))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7577712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 08:55:31.137098: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC-Ada-SCE-RMS: 5.81 (0.34)\n",
      "Method: pmi\n",
      "CC-Ada-SCE-RMS: 7.59 (0.13)\n",
      "Method: psi\n",
      "CC-Ada-SCE-RMS: 9.36 (0.19)\n",
      "Method: pvi\n",
      "CC-Ada-SCE-RMS: 8.12 (0.17)\n",
      "Method: pvi_best\n",
      "CC-Ada-SCE-RMS: 8.01 (0.04)\n",
      "Method: softmax_temp_scaling_nll\n",
      "CC-Ada-SCE-RMS: 8.11 (0.18)\n",
      "Method: pmi_temp_scaling_nll\n",
      "CC-Ada-SCE-RMS: 8.46 (0.12)\n",
      "Method: psi_temp_scaling_nll\n",
      "CC-Ada-SCE-RMS: 8.52 (0.13)\n",
      "Method: pvi_temp_scaling_nll\n",
      "CC-Ada-SCE-RMS: 8.36 (0.14)\n",
      "Method: pvi_best_temp_scaling_nll\n",
      "CC-Ada-SCE-RMS: 8.19 (0.00)\n"
     ]
    }
   ],
   "source": [
    "methods_list = ['softmax','pmi','psi','pvi','pvi_best',\n",
    "                'softmax_temp_scaling_nll','pmi_temp_scaling_nll','psi_temp_scaling_nll','pvi_temp_scaling_nll','pvi_best_temp_scaling_nll']\n",
    "for method in methods_list:\n",
    "    print(f'Method: {method}')\n",
    "    results = evaluate_calibration(ds_test, true_y_test, conf_method=f'{method}', n_runs=10)\n",
    "#     print(f\"ECE:            {utils.format_ci(results['ece'], scale=100)}\")\n",
    "#     print(f\"CC-ECE:         {utils.format_ci(results['cc_ece'], scale=100)}\")\n",
    "#     print(f\"MCE:            {utils.format_ci(results['mce'], scale=100)}\")\n",
    "#     print(f\"ACE:            {utils.format_ci(results['ace'], scale=100)}\")\n",
    "#     print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "#     print(f\"Ada-ECE:        {utils.format_ci(results['ada_ece'], scale=100)}\")\n",
    "#     print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "#     print(f\"CC-Ada-ECE:     {utils.format_ci(results['cc_ada_ece'], scale=100)}\")\n",
    "#     print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "    print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "#     print(f\"CW-ECE:         {utils.format_ci(results['cw_ece'], scale=100)}\")\n",
    "#     print(f\"CW-SCE:         {utils.format_ci(results['cw_sce'], scale=100)}\")\n",
    "#     print(f\"CW-Ada-ECE:     {utils.format_ci(results['cw_ada_ece'], scale=100)}\")\n",
    "#     print(f\"CW-Ada-SCE:     {utils.format_ci(results['cw_ada_sce'], scale=100)}\")\n",
    "#     print(f\"CW-Ada-ECE-RMS: {utils.format_ci(results['cw_ada_ece_rms'], scale=100)}\")\n",
    "#     print(f\"CW-Ada-SCE-RMS: {utils.format_ci(results['cw_ada_sce_rms'], scale=100)}\")\n",
    "#     print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "#     print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")\n",
    "#     print(f\"Sharpness:      {utils.format_ci(results['sharpness'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda46a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax ETS\n",
      "SCE:            0.59 (0.04)\n",
      "Ada-SCE:        0.51 (0.04)\n",
      "CC-Ada-SCE:     1.16 (0.05)\n",
      "CC-Ada-SCE-RMS: 8.14 (0.17)\n",
      "NLL:            27.64 (0.87)\n",
      "Brier Score:    13.07 (0.40)\n"
     ]
    }
   ],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'softmax ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "\n",
    "    logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "    opt_temp = np.load(f'{base_path}/softmax_opt_temp_ets_nll.npy')\n",
    "    opt_weights = np.load(f'{base_path}/softmax_opt_weights_ets_nll.npy')\n",
    "\n",
    "    scores_class = apply_ets(logits,opt_temp,opt_weights,num_classes)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0409b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: PVI ETS\n",
      "SCE:            0.59 (0.04)\n",
      "Ada-SCE:        0.54 (0.05)\n",
      "CC-Ada-SCE:     1.24 (0.04)\n",
      "CC-Ada-SCE-RMS: 8.30 (0.15)\n",
      "NLL:            27.74 (0.79)\n",
      "Brier Score:    13.03 (0.40)\n"
     ]
    }
   ],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'PVI ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "    pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "    opt_temp = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_temp_ets_nll.npy')\n",
    "    opt_weights = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_weights_ets_nll.npy')\n",
    "\n",
    "    scores_class = apply_ets(pvi,opt_temp,opt_weights,num_classes)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0253d0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax ETS\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_1/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 827us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_2/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 842us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_3/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 832us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_4/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 811us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_5/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 846us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_6/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 866us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_7/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 820us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_8/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 837us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_9/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 816us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_10/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 823us/step\n",
      "SCE:            1.77 (0.05)\n",
      "Ada-SCE:        0.84 (0.03)\n",
      "CC-Ada-SCE:     0.07 (0.03)\n",
      "CC-Ada-SCE-RMS: 0.67 (0.49)\n",
      "NLL:            121.67 (8.17)\n",
      "Brier Score:    17.53 (0.52)\n"
     ]
    }
   ],
   "source": [
    "method = 'softmax ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "\n",
    "    logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "\n",
    "    pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "    pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "    scores_class = pts_loaded.calibrate(logits)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd6cc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: PVI PTS\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_1/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 824us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_2/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 834us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_3/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 816us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_4/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 901us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_5/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 819us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_6/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 822us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_7/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 834us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_8/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 896us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_9/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 908us/step\n",
      "Loaded PTS model weights from: ../results/PI_Explainability/vgg16_stl10/run_10/calibration/calibration_model/pts_model.h5\n",
      "213/213 [==============================] - 0s 828us/step\n",
      "SCE:            1.43 (0.27)\n",
      "Ada-SCE:        0.85 (0.03)\n",
      "CC-Ada-SCE:     0.41 (0.25)\n",
      "CC-Ada-SCE-RMS: 3.36 (1.37)\n",
      "NLL:            89.39 (18.31)\n",
      "Brier Score:    16.65 (0.72)\n"
     ]
    }
   ],
   "source": [
    "method = 'PVI PTS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "    pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "\n",
    "    pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "    pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "    scores_class = pts_loaded.calibrate(pvi)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c17559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "run = 0\n",
    "conf_bin_num=15\n",
    "\n",
    "def reliability_params(df):\n",
    "    df['correct'] = (df.pred == df.true).astype(int)\n",
    "    bins = np.linspace(0, 1, conf_bin_num + 1)\n",
    "    df['bin'] = pd.cut(df['conf'], bins=bins, include_lowest=True, labels=False)\n",
    "\n",
    "    bin_acc = df.groupby('bin')['correct'].mean()\n",
    "    bin_counts = df.groupby('bin')['conf'].count()\n",
    "\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    acc = np.zeros(conf_bin_num)\n",
    "    for i in range(conf_bin_num):\n",
    "        acc[i] = bin_acc[i] if i in bin_acc else 0\n",
    "    return acc, bin_centers\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "\n",
    "exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pmi/separable_variational_f_js'\n",
    "opt_temp = np.load(f'{exp_name}/pmi_opt_temp_nll.npy')\n",
    "scores_class = np.load(f'{exp_name}/pmi_output_class_test.npy')\n",
    "scores_class = np.array([utils.softmax(x/opt_temp) for x in scores_class])\n",
    "scores_test = np.array([score[pred_value] for score, pred_value in zip(scores_class, pred_y_test)])\n",
    "\n",
    "df = pd.DataFrame({'conf': scores_test, 'true': true_y_test, 'pred': pred_y_test})\n",
    "acc, bin_centers = reliability_params(df)\n",
    "plt.bar(bin_centers, acc, width=1.0/conf_bin_num, color='red', edgecolor='black', align='center', alpha=0.7, label='PMI')\n",
    "\n",
    "# exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/gaussian'\n",
    "# opt_temp = np.load(f'{exp_name}/psi_opt_temp_nll.npy')\n",
    "# scores_class = np.load(f'{exp_name}/psi_output_class_500_projs_test.npy')\n",
    "# scores_class = np.array([utils.softmax(x/opt_temp) for x in scores_class])\n",
    "# scores_test = np.array([score[pred_value] for score, pred_value in zip(scores_class, pred_y_test)])\n",
    "\n",
    "# df = pd.DataFrame({'conf': scores_test, 'true': true_y_test, 'pred': pred_y_test})\n",
    "# acc, bin_centers = reliability_params(df)\n",
    "# plt.bar(bin_centers, acc, width=1.0/conf_bin_num, edgecolor='black', align='center', alpha=0.7, label='PSI')\n",
    "\n",
    "# exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "# opt_temp = np.load(f'{exp_name}/pvi_opt_temp_nll.npy')\n",
    "# scores_class = np.load(f'{exp_name}/pvi_class_test.npy')\n",
    "# scores_class = np.array([utils.softmax(x/opt_temp) for x in scores_class])\n",
    "# scores_test = np.array([score[pred_value] for score, pred_value in zip(scores_class, pred_y_test)])\n",
    "\n",
    "# df = pd.DataFrame({'conf': scores_test, 'true': true_y_test, 'pred': pred_y_test})\n",
    "# acc, bin_centers = reliability_params(df)\n",
    "# plt.bar(bin_centers, acc, width=1.0/conf_bin_num, color='red', edgecolor='black', align='center', alpha=0.7, label='PVI')\n",
    "\n",
    "opt_temp = np.load(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_nll.npy')\n",
    "scores_class = methods.softmax_prob(model, ds_test, opt_temp)\n",
    "scores_test = methods.max_softmax_prob(model, ds_test, opt_temp)\n",
    "\n",
    "df = pd.DataFrame({'conf': scores_test, 'true': true_y_test, 'pred': pred_y_test})\n",
    "acc, bin_centers = reliability_params(df)\n",
    "\n",
    "plt.bar(bin_centers, acc, width=1.0/conf_bin_num, color='blue', edgecolor='black', align='center', alpha=0.7, label='Softmax')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Reliability Diagram')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ece_contrib_params(df):\n",
    "    \n",
    "    df['correct'] = (df.pred == df.true).astype(int)\n",
    "    bins = np.linspace(0, 1, conf_bin_num + 1)\n",
    "    df['bin'] = pd.cut(df['conf'], bins=bins, include_lowest=True, labels=False)\n",
    "\n",
    "    bin_acc = df.groupby('bin')['correct'].mean()\n",
    "    bin_conf = df.groupby('bin')['conf'].mean()\n",
    "    bin_counts = df.groupby('bin')['conf'].count()\n",
    "    total = len(df)\n",
    "    \n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    bin_ece_contrib = np.zeros(conf_bin_num)\n",
    "    for i in range(conf_bin_num):\n",
    "        if i in bin_acc and i in bin_conf:\n",
    "            bin_ece_contrib[i] = np.abs(bin_acc[i] - bin_conf[i]) * (bin_counts[i] / total)\n",
    "    return bin_ece_contrib, bin_centers\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "\n",
    "exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "opt_temp = np.load(f'{exp_name}/pvi_opt_temp_nll.npy')\n",
    "scores_class = np.load(f'{exp_name}/pvi_class_test.npy')\n",
    "scores_class = np.array([utils.softmax(x/opt_temp) for x in scores_class])\n",
    "scores_test = np.array([score[pred_value] for score, pred_value in zip(scores_class, pred_y_test)])\n",
    "\n",
    "df = pd.DataFrame({'conf': scores_test, 'true': true_y_test, 'pred': pred_y_test})\n",
    "bin_ece_contrib, bin_centers = ece_contrib_params(df)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(bin_centers, bin_ece_contrib, width=1.0/conf_bin_num, color='red', edgecolor='black', align='center', alpha=0.7, label='PVI')\n",
    "\n",
    "opt_temp = np.load(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_nll.npy')\n",
    "scores_class = methods.softmax_prob(model, ds_test, opt_temp)\n",
    "scores_test = methods.max_softmax_prob(model, ds_test, opt_temp)\n",
    "\n",
    "df = pd.DataFrame({'conf': scores_test, 'true': true_y_test, 'pred': pred_y_test})\n",
    "bin_ece_contrib, bin_centers = ece_contrib_params(df)\n",
    "\n",
    "plt.bar(bin_centers, bin_ece_contrib, width=1.0/conf_bin_num, color='blue', edgecolor='black', align='center', alpha=0.7, label='Softmax')\n",
    "\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('ECE Contribution')\n",
    "plt.title('Per-Bin Contribution to ECE')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_counts_params(conf):\n",
    "    bins = np.linspace(0, 1, conf_bin_num + 1)\n",
    "    bin_indices = pd.cut(conf, bins=bins, include_lowest=True, labels=False)\n",
    "\n",
    "    counts = pd.Series(bin_indices).value_counts().sort_index()\n",
    "    full_counts = np.zeros(conf_bin_num)\n",
    "    for i in range(conf_bin_num):\n",
    "        if i in counts:\n",
    "            full_counts[i] = counts[i]\n",
    "\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    return full_counts, bin_centers\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "\n",
    "exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "opt_temp = np.load(f'{exp_name}/pvi_opt_temp_nll.npy')\n",
    "scores_class = np.load(f'{exp_name}/pvi_class_test.npy')\n",
    "scores_class = np.array([utils.softmax(x/opt_temp) for x in scores_class])\n",
    "scores_test = np.array([score[pred_value] for score, pred_value in zip(scores_class, pred_y_test)])\n",
    "\n",
    "full_counts, bin_centers = bin_counts_params(scores_test)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(bin_centers, full_counts, width=1.0/conf_bin_num, color='red', edgecolor='black', align='center', label='PVI')\n",
    "\n",
    "opt_temp = np.load(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_nll.npy')\n",
    "scores_class = methods.softmax_prob(model, ds_test, opt_temp)\n",
    "scores_test = methods.max_softmax_prob(model, ds_test, opt_temp)\n",
    "\n",
    "full_counts, bin_centers = bin_counts_params(scores_test)\n",
    "\n",
    "plt.bar(bin_centers, full_counts, width=1.0/conf_bin_num, color='blue', edgecolor='black', align='center', label='Softmax')\n",
    "\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.title('Number of Samples per Confidence Bin')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3da1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
