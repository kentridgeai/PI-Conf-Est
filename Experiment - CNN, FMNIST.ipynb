{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15bfe164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 11:08:55.098565: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-05 11:08:55.098634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-05 11:08:55.099863: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-05 11:08:55.107218: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.pmi_estimators import train_critic_model, neural_pmi\n",
    "from src.psi_estimators import psi_gaussian_train, psi_gaussian_val_class\n",
    "from src.pvi_estimators import train_pvi_null_model, neural_pvi_class, neural_pvi_ensemble_class\n",
    "import src.utils as utils\n",
    "import src.metrics as metrics\n",
    "import src.methods as methods\n",
    "import src.temp_scaling as temp_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a5cf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory configuration set successfully.\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        memory_limit = 6 * 1024  # 6GB in MB\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)]\n",
    "        )\n",
    "        print(\"GPU memory configuration set successfully.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf466c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 11:09:01.793159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1926] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6144 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn'\n",
    "dataset_name = 'fashion_mnist'\n",
    "\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    'fashion_mnist',\n",
    "    split=['train[:85%]', 'train[85%:]', 'test'],\n",
    "    data_dir = '../tensorflow_datasets/',\n",
    "    shuffle_files=False,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "num_classes = 10\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    label = tf.one_hot(label, depth=num_classes)\n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train.map(preprocess)\n",
    "ds_val = ds_val.map(preprocess)\n",
    "ds_test = ds_test.map(preprocess)\n",
    "\n",
    "# batch_size = 128\n",
    "# ds_train = ds_train.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "# ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "# ds_test = ds_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "true_y_train = np.argmax([y for x,y in ds_train], axis=1)\n",
    "true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "true_y_test = np.argmax([y for x,y in ds_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0414802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43577e3",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be0f74f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 10:26:55.910869: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2025-06-05 10:26:56.137040: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n",
      "2025-06-05 10:26:57.176758: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fa6dd2eb240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-06-05 10:26:57.176842: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2025-06-05 10:26:57.182794: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749119217.316375 1051235 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 8s 9ms/step - loss: 1.0657 - accuracy: 0.6125 - val_loss: 0.5982 - val_accuracy: 0.7820 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.6058 - accuracy: 0.7731 - val_loss: 0.4850 - val_accuracy: 0.8248 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.5135 - accuracy: 0.8083 - val_loss: 0.4267 - val_accuracy: 0.8429 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4574 - accuracy: 0.8311 - val_loss: 0.3775 - val_accuracy: 0.8608 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4203 - accuracy: 0.8461 - val_loss: 0.3493 - val_accuracy: 0.8710 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3936 - accuracy: 0.8549 - val_loss: 0.3451 - val_accuracy: 0.8748 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3740 - accuracy: 0.8625 - val_loss: 0.3159 - val_accuracy: 0.8852 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3576 - accuracy: 0.8678 - val_loss: 0.2959 - val_accuracy: 0.8906 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3451 - accuracy: 0.8721 - val_loss: 0.2880 - val_accuracy: 0.8912 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3321 - accuracy: 0.8782 - val_loss: 0.2787 - val_accuracy: 0.8966 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3203 - accuracy: 0.8812 - val_loss: 0.2714 - val_accuracy: 0.9006 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3119 - accuracy: 0.8846 - val_loss: 0.2665 - val_accuracy: 0.9008 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3022 - accuracy: 0.8889 - val_loss: 0.2566 - val_accuracy: 0.9050 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2928 - accuracy: 0.8913 - val_loss: 0.2539 - val_accuracy: 0.9056 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2849 - accuracy: 0.8950 - val_loss: 0.2440 - val_accuracy: 0.9078 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2796 - accuracy: 0.8978 - val_loss: 0.2386 - val_accuracy: 0.9101 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2725 - accuracy: 0.8994 - val_loss: 0.2389 - val_accuracy: 0.9099 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2650 - accuracy: 0.9017 - val_loss: 0.2340 - val_accuracy: 0.9106 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2575 - accuracy: 0.9052 - val_loss: 0.2239 - val_accuracy: 0.9148 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2528 - accuracy: 0.9066 - val_loss: 0.2266 - val_accuracy: 0.9121 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2485 - accuracy: 0.9083 - val_loss: 0.2199 - val_accuracy: 0.9159 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2459 - accuracy: 0.9082 - val_loss: 0.2146 - val_accuracy: 0.9171 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2405 - accuracy: 0.9107 - val_loss: 0.2106 - val_accuracy: 0.9203 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2355 - accuracy: 0.9140 - val_loss: 0.2213 - val_accuracy: 0.9132 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2325 - accuracy: 0.9139 - val_loss: 0.2080 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2274 - accuracy: 0.9173 - val_loss: 0.2093 - val_accuracy: 0.9189 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2225 - accuracy: 0.9184 - val_loss: 0.2018 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2206 - accuracy: 0.9180 - val_loss: 0.2012 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2165 - accuracy: 0.9204 - val_loss: 0.1967 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2127 - accuracy: 0.9207 - val_loss: 0.1959 - val_accuracy: 0.9259 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2092 - accuracy: 0.9244 - val_loss: 0.1974 - val_accuracy: 0.9259 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2045 - accuracy: 0.9242 - val_loss: 0.1954 - val_accuracy: 0.9272 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2042 - accuracy: 0.9247 - val_loss: 0.1946 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2012 - accuracy: 0.9257 - val_loss: 0.1941 - val_accuracy: 0.9284 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1963 - accuracy: 0.9266 - val_loss: 0.1955 - val_accuracy: 0.9267 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1938 - accuracy: 0.9281 - val_loss: 0.1916 - val_accuracy: 0.9287 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1910 - accuracy: 0.9280 - val_loss: 0.1913 - val_accuracy: 0.9290 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1886 - accuracy: 0.9297 - val_loss: 0.1897 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1874 - accuracy: 0.9307 - val_loss: 0.1906 - val_accuracy: 0.9270 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1829 - accuracy: 0.9322 - val_loss: 0.1904 - val_accuracy: 0.9303 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1804 - accuracy: 0.9329 - val_loss: 0.1899 - val_accuracy: 0.9301 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1797 - accuracy: 0.9338 - val_loss: 0.1868 - val_accuracy: 0.9302 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1756 - accuracy: 0.9351 - val_loss: 0.1873 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1730 - accuracy: 0.9356 - val_loss: 0.1903 - val_accuracy: 0.9297 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1725 - accuracy: 0.9345 - val_loss: 0.1846 - val_accuracy: 0.9326 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1694 - accuracy: 0.9371 - val_loss: 0.1826 - val_accuracy: 0.9330 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1677 - accuracy: 0.9376 - val_loss: 0.1823 - val_accuracy: 0.9316 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1667 - accuracy: 0.9380 - val_loss: 0.1837 - val_accuracy: 0.9318 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1650 - accuracy: 0.9381 - val_loss: 0.1860 - val_accuracy: 0.9332 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1608 - accuracy: 0.9400 - val_loss: 0.1840 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1604 - accuracy: 0.9404 - val_loss: 0.1856 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1585 - accuracy: 0.9402 - val_loss: 0.1831 - val_accuracy: 0.9349 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1569 - accuracy: 0.9405 - val_loss: 0.1844 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1529 - accuracy: 0.9425 - val_loss: 0.1795 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1518 - accuracy: 0.9432 - val_loss: 0.1814 - val_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1508 - accuracy: 0.9431 - val_loss: 0.1859 - val_accuracy: 0.9334 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1458 - accuracy: 0.9448 - val_loss: 0.1847 - val_accuracy: 0.9336 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1491 - accuracy: 0.9435 - val_loss: 0.1830 - val_accuracy: 0.9310 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1426 - accuracy: 0.9453 - val_loss: 0.1858 - val_accuracy: 0.9331 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1434 - accuracy: 0.9452 - val_loss: 0.1922 - val_accuracy: 0.9307 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1416 - accuracy: 0.9468 - val_loss: 0.1811 - val_accuracy: 0.9347 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1394 - accuracy: 0.9470 - val_loss: 0.1842 - val_accuracy: 0.9342 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1395 - accuracy: 0.9476 - val_loss: 0.1824 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1352 - accuracy: 0.9497 - val_loss: 0.1838 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1350 - accuracy: 0.9488 - val_loss: 0.1811 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1307 - accuracy: 0.9502 - val_loss: 0.1883 - val_accuracy: 0.9332 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1333 - accuracy: 0.9498 - val_loss: 0.1870 - val_accuracy: 0.9344 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1288 - accuracy: 0.9508 - val_loss: 0.1967 - val_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1260 - accuracy: 0.9531 - val_loss: 0.1859 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1282 - accuracy: 0.9511 - val_loss: 0.1845 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1249 - accuracy: 0.9521 - val_loss: 0.1803 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1234 - accuracy: 0.9537 - val_loss: 0.1878 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1225 - accuracy: 0.9536 - val_loss: 0.1840 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1190 - accuracy: 0.9552 - val_loss: 0.1884 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1198 - accuracy: 0.9544 - val_loss: 0.1828 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1182 - accuracy: 0.9548 - val_loss: 0.1843 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1154 - accuracy: 0.9568 - val_loss: 0.1847 - val_accuracy: 0.9353 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1158 - accuracy: 0.9554 - val_loss: 0.1888 - val_accuracy: 0.9352 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1129 - accuracy: 0.9569 - val_loss: 0.1920 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1125 - accuracy: 0.9574 - val_loss: 0.1839 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1111 - accuracy: 0.9586 - val_loss: 0.1893 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1103 - accuracy: 0.9580 - val_loss: 0.1870 - val_accuracy: 0.9361 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1090 - accuracy: 0.9580 - val_loss: 0.1914 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1079 - accuracy: 0.9586 - val_loss: 0.1970 - val_accuracy: 0.9328 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1085 - accuracy: 0.9583 - val_loss: 0.1925 - val_accuracy: 0.9352 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1056 - accuracy: 0.9603 - val_loss: 0.1987 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1028 - accuracy: 0.9615 - val_loss: 0.1956 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1022 - accuracy: 0.9613 - val_loss: 0.1944 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1021 - accuracy: 0.9612 - val_loss: 0.2020 - val_accuracy: 0.9360 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1002 - accuracy: 0.9610 - val_loss: 0.1937 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0991 - accuracy: 0.9624 - val_loss: 0.1903 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0960 - accuracy: 0.9640 - val_loss: 0.1959 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0963 - accuracy: 0.9631 - val_loss: 0.1976 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0965 - accuracy: 0.9628 - val_loss: 0.1974 - val_accuracy: 0.9354 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0938 - accuracy: 0.9650 - val_loss: 0.2022 - val_accuracy: 0.9383 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0937 - accuracy: 0.9635 - val_loss: 0.2030 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0947 - accuracy: 0.9633 - val_loss: 0.1884 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0913 - accuracy: 0.9651 - val_loss: 0.1957 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0904 - accuracy: 0.9664 - val_loss: 0.2010 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0914 - accuracy: 0.9647 - val_loss: 0.1940 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0888 - accuracy: 0.9654 - val_loss: 0.1940 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0867 - accuracy: 0.9670 - val_loss: 0.2025 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0847 - accuracy: 0.9674 - val_loss: 0.2053 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0851 - accuracy: 0.9679 - val_loss: 0.2004 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0864 - accuracy: 0.9670 - val_loss: 0.2040 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0833 - accuracy: 0.9671 - val_loss: 0.1924 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0802 - accuracy: 0.9693 - val_loss: 0.2071 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0832 - accuracy: 0.9680 - val_loss: 0.2023 - val_accuracy: 0.9383 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0804 - accuracy: 0.9687 - val_loss: 0.2028 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0794 - accuracy: 0.9693 - val_loss: 0.2103 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0785 - accuracy: 0.9698 - val_loss: 0.2123 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0773 - accuracy: 0.9703 - val_loss: 0.2075 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0779 - accuracy: 0.9700 - val_loss: 0.2121 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0764 - accuracy: 0.9708 - val_loss: 0.2036 - val_accuracy: 0.9401 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0763 - accuracy: 0.9713 - val_loss: 0.2174 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0759 - accuracy: 0.9714 - val_loss: 0.2109 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0741 - accuracy: 0.9724 - val_loss: 0.2078 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0739 - accuracy: 0.9720 - val_loss: 0.2048 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0722 - accuracy: 0.9719 - val_loss: 0.2147 - val_accuracy: 0.9383 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0722 - accuracy: 0.9728 - val_loss: 0.2131 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0723 - accuracy: 0.9719 - val_loss: 0.2223 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0710 - accuracy: 0.9724 - val_loss: 0.2173 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0722 - accuracy: 0.9723 - val_loss: 0.2119 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0684 - accuracy: 0.9741 - val_loss: 0.2136 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0715 - accuracy: 0.9722 - val_loss: 0.2175 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0680 - accuracy: 0.9735 - val_loss: 0.2231 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0648 - accuracy: 0.9752 - val_loss: 0.2281 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0659 - accuracy: 0.9747 - val_loss: 0.2161 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0651 - accuracy: 0.9755 - val_loss: 0.2268 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0648 - accuracy: 0.9757 - val_loss: 0.2213 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0646 - accuracy: 0.9759 - val_loss: 0.2163 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0651 - accuracy: 0.9755 - val_loss: 0.2228 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0632 - accuracy: 0.9757 - val_loss: 0.2286 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "396/399 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9749\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 114.\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0645 - accuracy: 0.9750 - val_loss: 0.2250 - val_accuracy: 0.9337 - lr: 1.0000e-04\n",
      "Epoch 134: early stopping\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 10:31:33.190408: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/dropout_3/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 5s 5ms/step - loss: 1.0726 - accuracy: 0.6055 - val_loss: 0.6334 - val_accuracy: 0.7720 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.6282 - accuracy: 0.7669 - val_loss: 0.5017 - val_accuracy: 0.8191 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.5286 - accuracy: 0.8058 - val_loss: 0.4325 - val_accuracy: 0.8419 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4717 - accuracy: 0.8267 - val_loss: 0.3957 - val_accuracy: 0.8538 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4309 - accuracy: 0.8426 - val_loss: 0.3599 - val_accuracy: 0.8657 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4009 - accuracy: 0.8518 - val_loss: 0.3378 - val_accuracy: 0.8748 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 0.3792 - accuracy: 0.8610 - val_loss: 0.3221 - val_accuracy: 0.8802 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3634 - accuracy: 0.8669 - val_loss: 0.3047 - val_accuracy: 0.8842 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3478 - accuracy: 0.8715 - val_loss: 0.2956 - val_accuracy: 0.8894 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3351 - accuracy: 0.8767 - val_loss: 0.2825 - val_accuracy: 0.8950 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3236 - accuracy: 0.8813 - val_loss: 0.2790 - val_accuracy: 0.8966 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3114 - accuracy: 0.8859 - val_loss: 0.2667 - val_accuracy: 0.9001 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3021 - accuracy: 0.8890 - val_loss: 0.2518 - val_accuracy: 0.9043 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2949 - accuracy: 0.8902 - val_loss: 0.2524 - val_accuracy: 0.9062 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2854 - accuracy: 0.8938 - val_loss: 0.2457 - val_accuracy: 0.9070 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2821 - accuracy: 0.8961 - val_loss: 0.2332 - val_accuracy: 0.9118 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2707 - accuracy: 0.9009 - val_loss: 0.2296 - val_accuracy: 0.9129 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2668 - accuracy: 0.9005 - val_loss: 0.2327 - val_accuracy: 0.9140 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2583 - accuracy: 0.9043 - val_loss: 0.2219 - val_accuracy: 0.9150 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2537 - accuracy: 0.9055 - val_loss: 0.2297 - val_accuracy: 0.9134 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2487 - accuracy: 0.9076 - val_loss: 0.2172 - val_accuracy: 0.9170 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2427 - accuracy: 0.9096 - val_loss: 0.2102 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2385 - accuracy: 0.9122 - val_loss: 0.2105 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2354 - accuracy: 0.9136 - val_loss: 0.2064 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2320 - accuracy: 0.9134 - val_loss: 0.2022 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2251 - accuracy: 0.9173 - val_loss: 0.2048 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2221 - accuracy: 0.9185 - val_loss: 0.2062 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2171 - accuracy: 0.9197 - val_loss: 0.1988 - val_accuracy: 0.9263 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2134 - accuracy: 0.9205 - val_loss: 0.1963 - val_accuracy: 0.9247 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2125 - accuracy: 0.9214 - val_loss: 0.1968 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2080 - accuracy: 0.9230 - val_loss: 0.1939 - val_accuracy: 0.9269 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2044 - accuracy: 0.9243 - val_loss: 0.1927 - val_accuracy: 0.9269 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2024 - accuracy: 0.9259 - val_loss: 0.1920 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2004 - accuracy: 0.9253 - val_loss: 0.1888 - val_accuracy: 0.9284 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1973 - accuracy: 0.9267 - val_loss: 0.1885 - val_accuracy: 0.9297 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1931 - accuracy: 0.9290 - val_loss: 0.1946 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1880 - accuracy: 0.9302 - val_loss: 0.1890 - val_accuracy: 0.9302 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1863 - accuracy: 0.9307 - val_loss: 0.1847 - val_accuracy: 0.9331 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1856 - accuracy: 0.9307 - val_loss: 0.1828 - val_accuracy: 0.9317 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1832 - accuracy: 0.9314 - val_loss: 0.1829 - val_accuracy: 0.9310 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1799 - accuracy: 0.9343 - val_loss: 0.1863 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1783 - accuracy: 0.9330 - val_loss: 0.1843 - val_accuracy: 0.9301 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1755 - accuracy: 0.9345 - val_loss: 0.1814 - val_accuracy: 0.9321 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1730 - accuracy: 0.9364 - val_loss: 0.1813 - val_accuracy: 0.9351 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1714 - accuracy: 0.9351 - val_loss: 0.1804 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1683 - accuracy: 0.9371 - val_loss: 0.1827 - val_accuracy: 0.9329 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1661 - accuracy: 0.9380 - val_loss: 0.1808 - val_accuracy: 0.9331 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1668 - accuracy: 0.9379 - val_loss: 0.1791 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1606 - accuracy: 0.9401 - val_loss: 0.1791 - val_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1616 - accuracy: 0.9400 - val_loss: 0.1811 - val_accuracy: 0.9339 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1568 - accuracy: 0.9418 - val_loss: 0.1813 - val_accuracy: 0.9347 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1547 - accuracy: 0.9424 - val_loss: 0.1807 - val_accuracy: 0.9342 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1537 - accuracy: 0.9417 - val_loss: 0.1809 - val_accuracy: 0.9332 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1526 - accuracy: 0.9423 - val_loss: 0.1768 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1504 - accuracy: 0.9439 - val_loss: 0.1788 - val_accuracy: 0.9354 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1499 - accuracy: 0.9443 - val_loss: 0.1794 - val_accuracy: 0.9353 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1449 - accuracy: 0.9459 - val_loss: 0.1835 - val_accuracy: 0.9337 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1470 - accuracy: 0.9454 - val_loss: 0.1826 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1422 - accuracy: 0.9458 - val_loss: 0.1798 - val_accuracy: 0.9360 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1412 - accuracy: 0.9471 - val_loss: 0.1837 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1389 - accuracy: 0.9484 - val_loss: 0.1860 - val_accuracy: 0.9337 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1365 - accuracy: 0.9490 - val_loss: 0.1870 - val_accuracy: 0.9334 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1379 - accuracy: 0.9467 - val_loss: 0.1783 - val_accuracy: 0.9348 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1338 - accuracy: 0.9499 - val_loss: 0.1880 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1313 - accuracy: 0.9511 - val_loss: 0.1850 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1310 - accuracy: 0.9514 - val_loss: 0.1852 - val_accuracy: 0.9344 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1295 - accuracy: 0.9514 - val_loss: 0.1862 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1299 - accuracy: 0.9507 - val_loss: 0.1835 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1274 - accuracy: 0.9517 - val_loss: 0.1853 - val_accuracy: 0.9316 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1253 - accuracy: 0.9532 - val_loss: 0.1808 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1241 - accuracy: 0.9532 - val_loss: 0.1812 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1209 - accuracy: 0.9541 - val_loss: 0.1874 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1217 - accuracy: 0.9543 - val_loss: 0.1838 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1186 - accuracy: 0.9557 - val_loss: 0.1897 - val_accuracy: 0.9348 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1191 - accuracy: 0.9546 - val_loss: 0.1836 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1170 - accuracy: 0.9552 - val_loss: 0.1851 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1146 - accuracy: 0.9565 - val_loss: 0.1825 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1137 - accuracy: 0.9566 - val_loss: 0.1844 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1133 - accuracy: 0.9564 - val_loss: 0.1828 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1113 - accuracy: 0.9584 - val_loss: 0.1894 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1123 - accuracy: 0.9578 - val_loss: 0.1886 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1066 - accuracy: 0.9598 - val_loss: 0.1870 - val_accuracy: 0.9351 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1084 - accuracy: 0.9599 - val_loss: 0.1814 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1076 - accuracy: 0.9591 - val_loss: 0.1897 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1039 - accuracy: 0.9609 - val_loss: 0.1915 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1037 - accuracy: 0.9602 - val_loss: 0.1880 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1004 - accuracy: 0.9619 - val_loss: 0.1860 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1037 - accuracy: 0.9605 - val_loss: 0.1917 - val_accuracy: 0.9351 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1000 - accuracy: 0.9617 - val_loss: 0.1955 - val_accuracy: 0.9349 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0994 - accuracy: 0.9628 - val_loss: 0.1959 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0969 - accuracy: 0.9628 - val_loss: 0.1955 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0974 - accuracy: 0.9628 - val_loss: 0.1922 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0950 - accuracy: 0.9639 - val_loss: 0.1917 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0951 - accuracy: 0.9639 - val_loss: 0.1986 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0917 - accuracy: 0.9648 - val_loss: 0.2026 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0925 - accuracy: 0.9647 - val_loss: 0.1940 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0929 - accuracy: 0.9655 - val_loss: 0.2006 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0873 - accuracy: 0.9667 - val_loss: 0.2035 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0906 - accuracy: 0.9651 - val_loss: 0.1974 - val_accuracy: 0.9349 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0880 - accuracy: 0.9659 - val_loss: 0.1993 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0900 - accuracy: 0.9660 - val_loss: 0.2021 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0845 - accuracy: 0.9690 - val_loss: 0.2078 - val_accuracy: 0.9352 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0859 - accuracy: 0.9669 - val_loss: 0.2103 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0845 - accuracy: 0.9681 - val_loss: 0.1950 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0818 - accuracy: 0.9691 - val_loss: 0.2048 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0841 - accuracy: 0.9685 - val_loss: 0.2057 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0828 - accuracy: 0.9677 - val_loss: 0.2030 - val_accuracy: 0.9353 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0822 - accuracy: 0.9681 - val_loss: 0.2027 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0824 - accuracy: 0.9694 - val_loss: 0.2057 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0790 - accuracy: 0.9697 - val_loss: 0.2060 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0777 - accuracy: 0.9704 - val_loss: 0.2068 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0772 - accuracy: 0.9707 - val_loss: 0.2109 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0782 - accuracy: 0.9700 - val_loss: 0.2104 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0757 - accuracy: 0.9717 - val_loss: 0.2106 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0765 - accuracy: 0.9707 - val_loss: 0.2046 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0730 - accuracy: 0.9722 - val_loss: 0.2111 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0747 - accuracy: 0.9718 - val_loss: 0.2129 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0721 - accuracy: 0.9728 - val_loss: 0.2133 - val_accuracy: 0.9360 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0718 - accuracy: 0.9725 - val_loss: 0.2196 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "398/399 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9736\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 100.\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0696 - accuracy: 0.9736 - val_loss: 0.2212 - val_accuracy: 0.9352 - lr: 1.0000e-04\n",
      "Epoch 120: early stopping\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 10:35:40.499919: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_2/dropout_6/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 5s 6ms/step - loss: 1.0735 - accuracy: 0.6037 - val_loss: 0.6287 - val_accuracy: 0.7772 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.6152 - accuracy: 0.7722 - val_loss: 0.4898 - val_accuracy: 0.8224 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.5142 - accuracy: 0.8110 - val_loss: 0.4209 - val_accuracy: 0.8473 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4549 - accuracy: 0.8342 - val_loss: 0.3813 - val_accuracy: 0.8589 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4179 - accuracy: 0.8462 - val_loss: 0.3466 - val_accuracy: 0.8708 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3895 - accuracy: 0.8578 - val_loss: 0.3235 - val_accuracy: 0.8792 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3668 - accuracy: 0.8645 - val_loss: 0.3079 - val_accuracy: 0.8874 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3502 - accuracy: 0.8716 - val_loss: 0.2927 - val_accuracy: 0.8942 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3344 - accuracy: 0.8767 - val_loss: 0.2803 - val_accuracy: 0.8978 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3211 - accuracy: 0.8809 - val_loss: 0.2684 - val_accuracy: 0.9008 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3131 - accuracy: 0.8845 - val_loss: 0.2585 - val_accuracy: 0.9042 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3013 - accuracy: 0.8903 - val_loss: 0.2555 - val_accuracy: 0.9067 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2911 - accuracy: 0.8937 - val_loss: 0.2448 - val_accuracy: 0.9103 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2846 - accuracy: 0.8955 - val_loss: 0.2437 - val_accuracy: 0.9099 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2764 - accuracy: 0.8988 - val_loss: 0.2419 - val_accuracy: 0.9101 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2682 - accuracy: 0.9015 - val_loss: 0.2393 - val_accuracy: 0.9107 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2643 - accuracy: 0.9015 - val_loss: 0.2262 - val_accuracy: 0.9151 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2572 - accuracy: 0.9048 - val_loss: 0.2267 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2528 - accuracy: 0.9067 - val_loss: 0.2162 - val_accuracy: 0.9184 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2467 - accuracy: 0.9101 - val_loss: 0.2184 - val_accuracy: 0.9193 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2452 - accuracy: 0.9110 - val_loss: 0.2115 - val_accuracy: 0.9204 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2362 - accuracy: 0.9128 - val_loss: 0.2118 - val_accuracy: 0.9194 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2321 - accuracy: 0.9132 - val_loss: 0.2061 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2300 - accuracy: 0.9149 - val_loss: 0.2064 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2266 - accuracy: 0.9171 - val_loss: 0.2009 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2218 - accuracy: 0.9187 - val_loss: 0.2016 - val_accuracy: 0.9253 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2183 - accuracy: 0.9203 - val_loss: 0.1979 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2126 - accuracy: 0.9214 - val_loss: 0.2024 - val_accuracy: 0.9263 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2114 - accuracy: 0.9217 - val_loss: 0.1965 - val_accuracy: 0.9270 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2089 - accuracy: 0.9228 - val_loss: 0.1945 - val_accuracy: 0.9283 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2052 - accuracy: 0.9233 - val_loss: 0.1944 - val_accuracy: 0.9273 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2015 - accuracy: 0.9264 - val_loss: 0.1942 - val_accuracy: 0.9291 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1999 - accuracy: 0.9254 - val_loss: 0.1954 - val_accuracy: 0.9268 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1992 - accuracy: 0.9257 - val_loss: 0.1891 - val_accuracy: 0.9306 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1938 - accuracy: 0.9285 - val_loss: 0.1893 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1905 - accuracy: 0.9287 - val_loss: 0.1890 - val_accuracy: 0.9316 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1871 - accuracy: 0.9298 - val_loss: 0.1835 - val_accuracy: 0.9327 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1846 - accuracy: 0.9315 - val_loss: 0.1840 - val_accuracy: 0.9324 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.1834 - accuracy: 0.9324 - val_loss: 0.1840 - val_accuracy: 0.9326 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1812 - accuracy: 0.9333 - val_loss: 0.1875 - val_accuracy: 0.9323 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1800 - accuracy: 0.9330 - val_loss: 0.1841 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1753 - accuracy: 0.9332 - val_loss: 0.1840 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1750 - accuracy: 0.9348 - val_loss: 0.1823 - val_accuracy: 0.9323 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1701 - accuracy: 0.9369 - val_loss: 0.1840 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1677 - accuracy: 0.9388 - val_loss: 0.1810 - val_accuracy: 0.9330 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1688 - accuracy: 0.9369 - val_loss: 0.1855 - val_accuracy: 0.9347 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1636 - accuracy: 0.9391 - val_loss: 0.1816 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1637 - accuracy: 0.9387 - val_loss: 0.1865 - val_accuracy: 0.9343 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1606 - accuracy: 0.9407 - val_loss: 0.1834 - val_accuracy: 0.9343 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1581 - accuracy: 0.9414 - val_loss: 0.1839 - val_accuracy: 0.9339 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1555 - accuracy: 0.9416 - val_loss: 0.1817 - val_accuracy: 0.9344 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1573 - accuracy: 0.9416 - val_loss: 0.1812 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1546 - accuracy: 0.9419 - val_loss: 0.1804 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1527 - accuracy: 0.9422 - val_loss: 0.1780 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1485 - accuracy: 0.9441 - val_loss: 0.1786 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1476 - accuracy: 0.9445 - val_loss: 0.1817 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1477 - accuracy: 0.9446 - val_loss: 0.1802 - val_accuracy: 0.9360 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1445 - accuracy: 0.9460 - val_loss: 0.1797 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1417 - accuracy: 0.9472 - val_loss: 0.1869 - val_accuracy: 0.9353 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1396 - accuracy: 0.9474 - val_loss: 0.1767 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1362 - accuracy: 0.9482 - val_loss: 0.1761 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1366 - accuracy: 0.9483 - val_loss: 0.1772 - val_accuracy: 0.9361 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1362 - accuracy: 0.9483 - val_loss: 0.1860 - val_accuracy: 0.9360 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1335 - accuracy: 0.9504 - val_loss: 0.1804 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1348 - accuracy: 0.9497 - val_loss: 0.1759 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1317 - accuracy: 0.9506 - val_loss: 0.1821 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1301 - accuracy: 0.9516 - val_loss: 0.1864 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1285 - accuracy: 0.9508 - val_loss: 0.1860 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1267 - accuracy: 0.9520 - val_loss: 0.1862 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1260 - accuracy: 0.9522 - val_loss: 0.1831 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1238 - accuracy: 0.9533 - val_loss: 0.1866 - val_accuracy: 0.9352 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1220 - accuracy: 0.9541 - val_loss: 0.1792 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1203 - accuracy: 0.9544 - val_loss: 0.1781 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1193 - accuracy: 0.9547 - val_loss: 0.1905 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1196 - accuracy: 0.9538 - val_loss: 0.1821 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1164 - accuracy: 0.9568 - val_loss: 0.1862 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1162 - accuracy: 0.9560 - val_loss: 0.1827 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1152 - accuracy: 0.9565 - val_loss: 0.1835 - val_accuracy: 0.9394 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1140 - accuracy: 0.9566 - val_loss: 0.1890 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1109 - accuracy: 0.9581 - val_loss: 0.1855 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1098 - accuracy: 0.9580 - val_loss: 0.1850 - val_accuracy: 0.9396 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1102 - accuracy: 0.9580 - val_loss: 0.1807 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1073 - accuracy: 0.9592 - val_loss: 0.1834 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1077 - accuracy: 0.9594 - val_loss: 0.1902 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1014 - accuracy: 0.9610 - val_loss: 0.1958 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1044 - accuracy: 0.9607 - val_loss: 0.1878 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1019 - accuracy: 0.9621 - val_loss: 0.1924 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1010 - accuracy: 0.9618 - val_loss: 0.1865 - val_accuracy: 0.9389 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1030 - accuracy: 0.9607 - val_loss: 0.1912 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0998 - accuracy: 0.9619 - val_loss: 0.1913 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0996 - accuracy: 0.9616 - val_loss: 0.1937 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0991 - accuracy: 0.9623 - val_loss: 0.1936 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0949 - accuracy: 0.9637 - val_loss: 0.1922 - val_accuracy: 0.9398 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0963 - accuracy: 0.9630 - val_loss: 0.1883 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0939 - accuracy: 0.9636 - val_loss: 0.1970 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0921 - accuracy: 0.9643 - val_loss: 0.1869 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0929 - accuracy: 0.9651 - val_loss: 0.1889 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0925 - accuracy: 0.9649 - val_loss: 0.1916 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0879 - accuracy: 0.9663 - val_loss: 0.1947 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0881 - accuracy: 0.9666 - val_loss: 0.2098 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0889 - accuracy: 0.9665 - val_loss: 0.1970 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0867 - accuracy: 0.9676 - val_loss: 0.1975 - val_accuracy: 0.9402 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0855 - accuracy: 0.9670 - val_loss: 0.1895 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0839 - accuracy: 0.9676 - val_loss: 0.1908 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0852 - accuracy: 0.9690 - val_loss: 0.1896 - val_accuracy: 0.9402 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0853 - accuracy: 0.9672 - val_loss: 0.1931 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0821 - accuracy: 0.9682 - val_loss: 0.2060 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0818 - accuracy: 0.9680 - val_loss: 0.1925 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0828 - accuracy: 0.9692 - val_loss: 0.2034 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0795 - accuracy: 0.9698 - val_loss: 0.2004 - val_accuracy: 0.9383 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0787 - accuracy: 0.9698 - val_loss: 0.2009 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0803 - accuracy: 0.9696 - val_loss: 0.1981 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0798 - accuracy: 0.9698 - val_loss: 0.2008 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0770 - accuracy: 0.9702 - val_loss: 0.2114 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0778 - accuracy: 0.9705 - val_loss: 0.2000 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0763 - accuracy: 0.9709 - val_loss: 0.2043 - val_accuracy: 0.9398 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0733 - accuracy: 0.9721 - val_loss: 0.2087 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0719 - accuracy: 0.9725 - val_loss: 0.2071 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0729 - accuracy: 0.9721 - val_loss: 0.2080 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0721 - accuracy: 0.9728 - val_loss: 0.2019 - val_accuracy: 0.9398 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0693 - accuracy: 0.9733 - val_loss: 0.2082 - val_accuracy: 0.9397 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "399/399 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9724\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 102.\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0717 - accuracy: 0.9724 - val_loss: 0.2084 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 122: early stopping\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 10:40:08.681334: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_3/dropout_9/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 5s 5ms/step - loss: 1.0577 - accuracy: 0.6130 - val_loss: 0.6031 - val_accuracy: 0.7754 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.6118 - accuracy: 0.7719 - val_loss: 0.4989 - val_accuracy: 0.8148 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.5221 - accuracy: 0.8070 - val_loss: 0.4188 - val_accuracy: 0.8466 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4626 - accuracy: 0.8303 - val_loss: 0.4014 - val_accuracy: 0.8488 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4208 - accuracy: 0.8454 - val_loss: 0.3466 - val_accuracy: 0.8680 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3951 - accuracy: 0.8550 - val_loss: 0.3282 - val_accuracy: 0.8754 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3698 - accuracy: 0.8626 - val_loss: 0.3114 - val_accuracy: 0.8852 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3527 - accuracy: 0.8693 - val_loss: 0.2953 - val_accuracy: 0.8879 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3359 - accuracy: 0.8766 - val_loss: 0.2851 - val_accuracy: 0.8924 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3265 - accuracy: 0.8806 - val_loss: 0.2755 - val_accuracy: 0.8972 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3149 - accuracy: 0.8855 - val_loss: 0.2657 - val_accuracy: 0.9016 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3022 - accuracy: 0.8887 - val_loss: 0.2552 - val_accuracy: 0.9023 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2942 - accuracy: 0.8927 - val_loss: 0.2508 - val_accuracy: 0.9058 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2885 - accuracy: 0.8933 - val_loss: 0.2406 - val_accuracy: 0.9086 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2771 - accuracy: 0.8981 - val_loss: 0.2403 - val_accuracy: 0.9083 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2706 - accuracy: 0.9010 - val_loss: 0.2338 - val_accuracy: 0.9119 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2670 - accuracy: 0.9015 - val_loss: 0.2279 - val_accuracy: 0.9133 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2612 - accuracy: 0.9038 - val_loss: 0.2233 - val_accuracy: 0.9149 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2531 - accuracy: 0.9074 - val_loss: 0.2333 - val_accuracy: 0.9106 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2501 - accuracy: 0.9065 - val_loss: 0.2175 - val_accuracy: 0.9158 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2433 - accuracy: 0.9103 - val_loss: 0.2215 - val_accuracy: 0.9150 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2407 - accuracy: 0.9116 - val_loss: 0.2122 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2365 - accuracy: 0.9125 - val_loss: 0.2095 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2330 - accuracy: 0.9146 - val_loss: 0.2086 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2297 - accuracy: 0.9161 - val_loss: 0.2081 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2248 - accuracy: 0.9174 - val_loss: 0.2024 - val_accuracy: 0.9251 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2221 - accuracy: 0.9186 - val_loss: 0.1998 - val_accuracy: 0.9249 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2158 - accuracy: 0.9204 - val_loss: 0.2053 - val_accuracy: 0.9237 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2139 - accuracy: 0.9215 - val_loss: 0.1999 - val_accuracy: 0.9253 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2110 - accuracy: 0.9215 - val_loss: 0.1976 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2065 - accuracy: 0.9242 - val_loss: 0.1999 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2043 - accuracy: 0.9238 - val_loss: 0.2003 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1999 - accuracy: 0.9261 - val_loss: 0.1950 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1982 - accuracy: 0.9277 - val_loss: 0.1905 - val_accuracy: 0.9277 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1963 - accuracy: 0.9275 - val_loss: 0.1964 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1924 - accuracy: 0.9290 - val_loss: 0.1956 - val_accuracy: 0.9251 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1895 - accuracy: 0.9298 - val_loss: 0.1883 - val_accuracy: 0.9303 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1880 - accuracy: 0.9315 - val_loss: 0.1903 - val_accuracy: 0.9280 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1844 - accuracy: 0.9314 - val_loss: 0.1867 - val_accuracy: 0.9288 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1819 - accuracy: 0.9328 - val_loss: 0.1889 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1787 - accuracy: 0.9328 - val_loss: 0.1885 - val_accuracy: 0.9293 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1781 - accuracy: 0.9338 - val_loss: 0.1922 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "399/399 [==============================] - 3s 6ms/step - loss: 0.1758 - accuracy: 0.9340 - val_loss: 0.1898 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1723 - accuracy: 0.9361 - val_loss: 0.1922 - val_accuracy: 0.9287 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1690 - accuracy: 0.9377 - val_loss: 0.1880 - val_accuracy: 0.9310 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1704 - accuracy: 0.9358 - val_loss: 0.1841 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1676 - accuracy: 0.9384 - val_loss: 0.1836 - val_accuracy: 0.9307 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1652 - accuracy: 0.9379 - val_loss: 0.1826 - val_accuracy: 0.9313 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1612 - accuracy: 0.9399 - val_loss: 0.1804 - val_accuracy: 0.9342 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1609 - accuracy: 0.9403 - val_loss: 0.1806 - val_accuracy: 0.9318 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1575 - accuracy: 0.9421 - val_loss: 0.1798 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1574 - accuracy: 0.9411 - val_loss: 0.1810 - val_accuracy: 0.9322 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1553 - accuracy: 0.9423 - val_loss: 0.1799 - val_accuracy: 0.9328 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1518 - accuracy: 0.9428 - val_loss: 0.1806 - val_accuracy: 0.9323 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1512 - accuracy: 0.9425 - val_loss: 0.1787 - val_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1490 - accuracy: 0.9446 - val_loss: 0.1825 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1455 - accuracy: 0.9459 - val_loss: 0.1802 - val_accuracy: 0.9328 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1470 - accuracy: 0.9449 - val_loss: 0.1835 - val_accuracy: 0.9342 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1450 - accuracy: 0.9452 - val_loss: 0.1788 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1420 - accuracy: 0.9463 - val_loss: 0.1831 - val_accuracy: 0.9318 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1405 - accuracy: 0.9471 - val_loss: 0.1807 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1387 - accuracy: 0.9483 - val_loss: 0.1813 - val_accuracy: 0.9339 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1382 - accuracy: 0.9481 - val_loss: 0.1878 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1360 - accuracy: 0.9480 - val_loss: 0.1807 - val_accuracy: 0.9339 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1333 - accuracy: 0.9496 - val_loss: 0.1832 - val_accuracy: 0.9332 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1326 - accuracy: 0.9502 - val_loss: 0.1789 - val_accuracy: 0.9349 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1313 - accuracy: 0.9506 - val_loss: 0.1859 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1297 - accuracy: 0.9522 - val_loss: 0.1840 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1273 - accuracy: 0.9519 - val_loss: 0.1852 - val_accuracy: 0.9339 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1246 - accuracy: 0.9527 - val_loss: 0.1812 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1248 - accuracy: 0.9527 - val_loss: 0.1793 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1222 - accuracy: 0.9545 - val_loss: 0.1788 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1243 - accuracy: 0.9527 - val_loss: 0.1818 - val_accuracy: 0.9334 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1208 - accuracy: 0.9545 - val_loss: 0.1826 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "399/399 [==============================] - 3s 7ms/step - loss: 0.1200 - accuracy: 0.9542 - val_loss: 0.1866 - val_accuracy: 0.9354 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1180 - accuracy: 0.9558 - val_loss: 0.1847 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1158 - accuracy: 0.9567 - val_loss: 0.1835 - val_accuracy: 0.9354 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1133 - accuracy: 0.9579 - val_loss: 0.1902 - val_accuracy: 0.9351 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1140 - accuracy: 0.9565 - val_loss: 0.1850 - val_accuracy: 0.9343 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1132 - accuracy: 0.9570 - val_loss: 0.1847 - val_accuracy: 0.9349 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1111 - accuracy: 0.9579 - val_loss: 0.1875 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1070 - accuracy: 0.9592 - val_loss: 0.1879 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1053 - accuracy: 0.9602 - val_loss: 0.1894 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1063 - accuracy: 0.9598 - val_loss: 0.1893 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1077 - accuracy: 0.9595 - val_loss: 0.1891 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1043 - accuracy: 0.9607 - val_loss: 0.1918 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1031 - accuracy: 0.9609 - val_loss: 0.1855 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1011 - accuracy: 0.9618 - val_loss: 0.1890 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0992 - accuracy: 0.9615 - val_loss: 0.1865 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0998 - accuracy: 0.9623 - val_loss: 0.1873 - val_accuracy: 0.9348 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0993 - accuracy: 0.9625 - val_loss: 0.1875 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0975 - accuracy: 0.9632 - val_loss: 0.1872 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0946 - accuracy: 0.9644 - val_loss: 0.1941 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0951 - accuracy: 0.9643 - val_loss: 0.1943 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0945 - accuracy: 0.9642 - val_loss: 0.1963 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0954 - accuracy: 0.9631 - val_loss: 0.1965 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0928 - accuracy: 0.9648 - val_loss: 0.1931 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0933 - accuracy: 0.9646 - val_loss: 0.1947 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0901 - accuracy: 0.9662 - val_loss: 0.1947 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0907 - accuracy: 0.9663 - val_loss: 0.1946 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0885 - accuracy: 0.9666 - val_loss: 0.1946 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0865 - accuracy: 0.9676 - val_loss: 0.1981 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0871 - accuracy: 0.9665 - val_loss: 0.1977 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0839 - accuracy: 0.9681 - val_loss: 0.2065 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0863 - accuracy: 0.9674 - val_loss: 0.1939 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0848 - accuracy: 0.9675 - val_loss: 0.1939 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0826 - accuracy: 0.9686 - val_loss: 0.1983 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0811 - accuracy: 0.9688 - val_loss: 0.2011 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0814 - accuracy: 0.9689 - val_loss: 0.2026 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0801 - accuracy: 0.9692 - val_loss: 0.2008 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0806 - accuracy: 0.9698 - val_loss: 0.1984 - val_accuracy: 0.9396 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0787 - accuracy: 0.9709 - val_loss: 0.1974 - val_accuracy: 0.9394 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0785 - accuracy: 0.9702 - val_loss: 0.2009 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0793 - accuracy: 0.9694 - val_loss: 0.1994 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0781 - accuracy: 0.9701 - val_loss: 0.2025 - val_accuracy: 0.9398 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0747 - accuracy: 0.9720 - val_loss: 0.2039 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0742 - accuracy: 0.9709 - val_loss: 0.2042 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0751 - accuracy: 0.9717 - val_loss: 0.2164 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0759 - accuracy: 0.9706 - val_loss: 0.2113 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0721 - accuracy: 0.9732 - val_loss: 0.2154 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0719 - accuracy: 0.9722 - val_loss: 0.2065 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0719 - accuracy: 0.9731 - val_loss: 0.2143 - val_accuracy: 0.9377 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0709 - accuracy: 0.9724 - val_loss: 0.2104 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0710 - accuracy: 0.9732 - val_loss: 0.2020 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0697 - accuracy: 0.9739 - val_loss: 0.2180 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0679 - accuracy: 0.9745 - val_loss: 0.2257 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0665 - accuracy: 0.9747 - val_loss: 0.2114 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0657 - accuracy: 0.9751 - val_loss: 0.2109 - val_accuracy: 0.9406 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0676 - accuracy: 0.9741 - val_loss: 0.2191 - val_accuracy: 0.9402 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0649 - accuracy: 0.9751 - val_loss: 0.2345 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0665 - accuracy: 0.9741 - val_loss: 0.2115 - val_accuracy: 0.9399 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0647 - accuracy: 0.9754 - val_loss: 0.2166 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0641 - accuracy: 0.9756 - val_loss: 0.2203 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0635 - accuracy: 0.9758 - val_loss: 0.2136 - val_accuracy: 0.9389 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0618 - accuracy: 0.9764 - val_loss: 0.2250 - val_accuracy: 0.9396 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0619 - accuracy: 0.9763 - val_loss: 0.2152 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0648 - accuracy: 0.9757 - val_loss: 0.2122 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0614 - accuracy: 0.9773 - val_loss: 0.2189 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0604 - accuracy: 0.9768 - val_loss: 0.2243 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0617 - accuracy: 0.9767 - val_loss: 0.2357 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0590 - accuracy: 0.9780 - val_loss: 0.2205 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0597 - accuracy: 0.9767 - val_loss: 0.2272 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0589 - accuracy: 0.9775 - val_loss: 0.2219 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0578 - accuracy: 0.9783 - val_loss: 0.2233 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0565 - accuracy: 0.9789 - val_loss: 0.2281 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0554 - accuracy: 0.9791 - val_loss: 0.2352 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.0571 - accuracy: 0.9786 - val_loss: 0.2239 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 148/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0576 - accuracy: 0.9782 - val_loss: 0.2303 - val_accuracy: 0.9409 - lr: 1.0000e-04\n",
      "Epoch 149/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0559 - accuracy: 0.9787 - val_loss: 0.2297 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 150/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0574 - accuracy: 0.9780 - val_loss: 0.2215 - val_accuracy: 0.9396 - lr: 1.0000e-04\n",
      "Epoch 151/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0539 - accuracy: 0.9795 - val_loss: 0.2423 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 152/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0543 - accuracy: 0.9797 - val_loss: 0.2401 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 153/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0535 - accuracy: 0.9795 - val_loss: 0.2330 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 154/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0546 - accuracy: 0.9798 - val_loss: 0.2302 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 155/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0536 - accuracy: 0.9797 - val_loss: 0.2354 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 156/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0526 - accuracy: 0.9800 - val_loss: 0.2299 - val_accuracy: 0.9397 - lr: 1.0000e-04\n",
      "Epoch 157/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0522 - accuracy: 0.9803 - val_loss: 0.2310 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 158/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0516 - accuracy: 0.9802 - val_loss: 0.2460 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 159/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0523 - accuracy: 0.9802 - val_loss: 0.2300 - val_accuracy: 0.9397 - lr: 1.0000e-04\n",
      "Epoch 160/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0525 - accuracy: 0.9801 - val_loss: 0.2300 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 161/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0496 - accuracy: 0.9814 - val_loss: 0.2291 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 162/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0499 - accuracy: 0.9815 - val_loss: 0.2328 - val_accuracy: 0.9398 - lr: 1.0000e-04\n",
      "Epoch 163/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0479 - accuracy: 0.9826 - val_loss: 0.2366 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 164/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0472 - accuracy: 0.9823 - val_loss: 0.2411 - val_accuracy: 0.9383 - lr: 1.0000e-04\n",
      "Epoch 165/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0501 - accuracy: 0.9811 - val_loss: 0.2405 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 166/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0494 - accuracy: 0.9813 - val_loss: 0.2450 - val_accuracy: 0.9398 - lr: 1.0000e-04\n",
      "Epoch 167/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0495 - accuracy: 0.9814 - val_loss: 0.2402 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 168/300\n",
      "397/399 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 0.9818\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 148.\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0486 - accuracy: 0.9818 - val_loss: 0.2374 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 168: early stopping\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 10:46:02.415305: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_4/dropout_12/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 5s 5ms/step - loss: 1.0870 - accuracy: 0.5889 - val_loss: 0.6271 - val_accuracy: 0.7624 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.6230 - accuracy: 0.7655 - val_loss: 0.4897 - val_accuracy: 0.8210 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.5199 - accuracy: 0.8064 - val_loss: 0.4231 - val_accuracy: 0.8427 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4585 - accuracy: 0.8294 - val_loss: 0.3858 - val_accuracy: 0.8552 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4175 - accuracy: 0.8454 - val_loss: 0.3558 - val_accuracy: 0.8667 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3899 - accuracy: 0.8549 - val_loss: 0.3352 - val_accuracy: 0.8749 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3692 - accuracy: 0.8624 - val_loss: 0.3200 - val_accuracy: 0.8824 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3530 - accuracy: 0.8700 - val_loss: 0.2959 - val_accuracy: 0.8890 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3389 - accuracy: 0.8744 - val_loss: 0.2939 - val_accuracy: 0.8888 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3255 - accuracy: 0.8805 - val_loss: 0.2820 - val_accuracy: 0.8954 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3153 - accuracy: 0.8825 - val_loss: 0.2659 - val_accuracy: 0.8996 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3067 - accuracy: 0.8869 - val_loss: 0.2638 - val_accuracy: 0.9003 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2978 - accuracy: 0.8894 - val_loss: 0.2552 - val_accuracy: 0.9012 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2896 - accuracy: 0.8938 - val_loss: 0.2485 - val_accuracy: 0.9067 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2805 - accuracy: 0.8960 - val_loss: 0.2408 - val_accuracy: 0.9079 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2742 - accuracy: 0.8989 - val_loss: 0.2370 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2672 - accuracy: 0.9019 - val_loss: 0.2326 - val_accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2617 - accuracy: 0.9035 - val_loss: 0.2250 - val_accuracy: 0.9151 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2536 - accuracy: 0.9064 - val_loss: 0.2250 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2507 - accuracy: 0.9072 - val_loss: 0.2187 - val_accuracy: 0.9161 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2456 - accuracy: 0.9072 - val_loss: 0.2175 - val_accuracy: 0.9179 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2386 - accuracy: 0.9113 - val_loss: 0.2133 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2370 - accuracy: 0.9127 - val_loss: 0.2079 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2340 - accuracy: 0.9134 - val_loss: 0.2082 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2280 - accuracy: 0.9144 - val_loss: 0.2024 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2232 - accuracy: 0.9177 - val_loss: 0.2019 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2201 - accuracy: 0.9182 - val_loss: 0.2017 - val_accuracy: 0.9261 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2142 - accuracy: 0.9198 - val_loss: 0.1981 - val_accuracy: 0.9270 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2098 - accuracy: 0.9215 - val_loss: 0.1936 - val_accuracy: 0.9297 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2084 - accuracy: 0.9224 - val_loss: 0.1947 - val_accuracy: 0.9291 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2040 - accuracy: 0.9232 - val_loss: 0.1915 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1997 - accuracy: 0.9261 - val_loss: 0.1911 - val_accuracy: 0.9306 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1991 - accuracy: 0.9254 - val_loss: 0.1891 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1952 - accuracy: 0.9279 - val_loss: 0.1891 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1926 - accuracy: 0.9271 - val_loss: 0.1895 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1903 - accuracy: 0.9290 - val_loss: 0.1896 - val_accuracy: 0.9314 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1878 - accuracy: 0.9294 - val_loss: 0.1878 - val_accuracy: 0.9310 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1843 - accuracy: 0.9310 - val_loss: 0.1888 - val_accuracy: 0.9309 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1829 - accuracy: 0.9320 - val_loss: 0.1872 - val_accuracy: 0.9317 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1802 - accuracy: 0.9323 - val_loss: 0.1833 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1762 - accuracy: 0.9330 - val_loss: 0.1855 - val_accuracy: 0.9351 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1758 - accuracy: 0.9341 - val_loss: 0.1815 - val_accuracy: 0.9343 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1724 - accuracy: 0.9343 - val_loss: 0.1809 - val_accuracy: 0.9349 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1704 - accuracy: 0.9359 - val_loss: 0.1812 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1686 - accuracy: 0.9365 - val_loss: 0.1817 - val_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1664 - accuracy: 0.9372 - val_loss: 0.1794 - val_accuracy: 0.9334 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1631 - accuracy: 0.9381 - val_loss: 0.1805 - val_accuracy: 0.9361 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1611 - accuracy: 0.9397 - val_loss: 0.1841 - val_accuracy: 0.9351 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1606 - accuracy: 0.9390 - val_loss: 0.1776 - val_accuracy: 0.9361 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1568 - accuracy: 0.9401 - val_loss: 0.1797 - val_accuracy: 0.9349 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1547 - accuracy: 0.9418 - val_loss: 0.1769 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1557 - accuracy: 0.9414 - val_loss: 0.1762 - val_accuracy: 0.9361 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1542 - accuracy: 0.9420 - val_loss: 0.1774 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1480 - accuracy: 0.9436 - val_loss: 0.1790 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1487 - accuracy: 0.9431 - val_loss: 0.1797 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1471 - accuracy: 0.9447 - val_loss: 0.1801 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1447 - accuracy: 0.9451 - val_loss: 0.1759 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1400 - accuracy: 0.9475 - val_loss: 0.1806 - val_accuracy: 0.9368 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1400 - accuracy: 0.9474 - val_loss: 0.1812 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1399 - accuracy: 0.9479 - val_loss: 0.1818 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1374 - accuracy: 0.9480 - val_loss: 0.1782 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1349 - accuracy: 0.9491 - val_loss: 0.1821 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1335 - accuracy: 0.9503 - val_loss: 0.1847 - val_accuracy: 0.9353 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1302 - accuracy: 0.9507 - val_loss: 0.1790 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1339 - accuracy: 0.9501 - val_loss: 0.1805 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1301 - accuracy: 0.9518 - val_loss: 0.1800 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1267 - accuracy: 0.9515 - val_loss: 0.1809 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1301 - accuracy: 0.9500 - val_loss: 0.1877 - val_accuracy: 0.9343 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1260 - accuracy: 0.9521 - val_loss: 0.1803 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1261 - accuracy: 0.9519 - val_loss: 0.1833 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "389/399 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9539\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1200 - accuracy: 0.9538 - val_loss: 0.1818 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 71: early stopping\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 10:48:31.430775: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_5/dropout_15/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 5s 5ms/step - loss: 1.0764 - accuracy: 0.5992 - val_loss: 0.6292 - val_accuracy: 0.7712 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.6179 - accuracy: 0.7711 - val_loss: 0.4926 - val_accuracy: 0.8223 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.5123 - accuracy: 0.8095 - val_loss: 0.4303 - val_accuracy: 0.8459 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4596 - accuracy: 0.8316 - val_loss: 0.3832 - val_accuracy: 0.8580 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4234 - accuracy: 0.8432 - val_loss: 0.3600 - val_accuracy: 0.8678 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4014 - accuracy: 0.8522 - val_loss: 0.3322 - val_accuracy: 0.8736 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "399/399 [==============================] - 2s 6ms/step - loss: 0.3831 - accuracy: 0.8585 - val_loss: 0.3217 - val_accuracy: 0.8787 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3647 - accuracy: 0.8649 - val_loss: 0.3159 - val_accuracy: 0.8816 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3513 - accuracy: 0.8719 - val_loss: 0.3104 - val_accuracy: 0.8830 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3401 - accuracy: 0.8761 - val_loss: 0.2855 - val_accuracy: 0.8923 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3285 - accuracy: 0.8780 - val_loss: 0.2758 - val_accuracy: 0.8970 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3196 - accuracy: 0.8809 - val_loss: 0.2707 - val_accuracy: 0.8991 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3074 - accuracy: 0.8854 - val_loss: 0.2603 - val_accuracy: 0.9020 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3010 - accuracy: 0.8888 - val_loss: 0.2687 - val_accuracy: 0.8970 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2918 - accuracy: 0.8909 - val_loss: 0.2540 - val_accuracy: 0.9048 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2859 - accuracy: 0.8954 - val_loss: 0.2481 - val_accuracy: 0.9084 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2814 - accuracy: 0.8962 - val_loss: 0.2461 - val_accuracy: 0.9088 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2727 - accuracy: 0.8987 - val_loss: 0.2377 - val_accuracy: 0.9097 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2673 - accuracy: 0.9006 - val_loss: 0.2335 - val_accuracy: 0.9120 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2613 - accuracy: 0.9021 - val_loss: 0.2306 - val_accuracy: 0.9147 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2581 - accuracy: 0.9046 - val_loss: 0.2297 - val_accuracy: 0.9143 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2513 - accuracy: 0.9062 - val_loss: 0.2221 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2457 - accuracy: 0.9085 - val_loss: 0.2208 - val_accuracy: 0.9182 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2436 - accuracy: 0.9103 - val_loss: 0.2191 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2389 - accuracy: 0.9101 - val_loss: 0.2247 - val_accuracy: 0.9179 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2347 - accuracy: 0.9129 - val_loss: 0.2208 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2317 - accuracy: 0.9145 - val_loss: 0.2075 - val_accuracy: 0.9243 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2274 - accuracy: 0.9153 - val_loss: 0.2062 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2226 - accuracy: 0.9162 - val_loss: 0.2072 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2207 - accuracy: 0.9181 - val_loss: 0.2044 - val_accuracy: 0.9253 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2180 - accuracy: 0.9188 - val_loss: 0.2085 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2151 - accuracy: 0.9193 - val_loss: 0.2017 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2136 - accuracy: 0.9216 - val_loss: 0.2012 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2062 - accuracy: 0.9232 - val_loss: 0.1946 - val_accuracy: 0.9297 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2045 - accuracy: 0.9240 - val_loss: 0.2006 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2035 - accuracy: 0.9246 - val_loss: 0.1952 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1995 - accuracy: 0.9250 - val_loss: 0.2001 - val_accuracy: 0.9259 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1947 - accuracy: 0.9272 - val_loss: 0.1929 - val_accuracy: 0.9297 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1932 - accuracy: 0.9278 - val_loss: 0.1938 - val_accuracy: 0.9288 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1920 - accuracy: 0.9279 - val_loss: 0.1904 - val_accuracy: 0.9301 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1887 - accuracy: 0.9292 - val_loss: 0.1986 - val_accuracy: 0.9287 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1884 - accuracy: 0.9288 - val_loss: 0.1893 - val_accuracy: 0.9319 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1835 - accuracy: 0.9315 - val_loss: 0.1894 - val_accuracy: 0.9311 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1828 - accuracy: 0.9315 - val_loss: 0.1880 - val_accuracy: 0.9309 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1811 - accuracy: 0.9326 - val_loss: 0.1933 - val_accuracy: 0.9302 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1797 - accuracy: 0.9330 - val_loss: 0.1878 - val_accuracy: 0.9317 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1756 - accuracy: 0.9338 - val_loss: 0.1892 - val_accuracy: 0.9317 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1759 - accuracy: 0.9349 - val_loss: 0.1900 - val_accuracy: 0.9296 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1710 - accuracy: 0.9361 - val_loss: 0.1872 - val_accuracy: 0.9319 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1713 - accuracy: 0.9362 - val_loss: 0.1845 - val_accuracy: 0.9321 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1670 - accuracy: 0.9374 - val_loss: 0.1853 - val_accuracy: 0.9318 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1636 - accuracy: 0.9389 - val_loss: 0.1856 - val_accuracy: 0.9342 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1645 - accuracy: 0.9389 - val_loss: 0.1894 - val_accuracy: 0.9323 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1606 - accuracy: 0.9398 - val_loss: 0.1872 - val_accuracy: 0.9331 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1593 - accuracy: 0.9393 - val_loss: 0.1847 - val_accuracy: 0.9327 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1583 - accuracy: 0.9401 - val_loss: 0.1855 - val_accuracy: 0.9327 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1536 - accuracy: 0.9422 - val_loss: 0.1823 - val_accuracy: 0.9349 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1532 - accuracy: 0.9420 - val_loss: 0.1896 - val_accuracy: 0.9324 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1531 - accuracy: 0.9428 - val_loss: 0.1805 - val_accuracy: 0.9352 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1526 - accuracy: 0.9430 - val_loss: 0.1811 - val_accuracy: 0.9343 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1496 - accuracy: 0.9436 - val_loss: 0.1832 - val_accuracy: 0.9327 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1470 - accuracy: 0.9450 - val_loss: 0.1830 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1423 - accuracy: 0.9469 - val_loss: 0.1882 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1425 - accuracy: 0.9452 - val_loss: 0.1871 - val_accuracy: 0.9334 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1398 - accuracy: 0.9476 - val_loss: 0.1898 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1395 - accuracy: 0.9469 - val_loss: 0.1846 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1375 - accuracy: 0.9474 - val_loss: 0.1851 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1376 - accuracy: 0.9479 - val_loss: 0.1809 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1347 - accuracy: 0.9494 - val_loss: 0.1846 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1340 - accuracy: 0.9497 - val_loss: 0.1824 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1308 - accuracy: 0.9510 - val_loss: 0.1859 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1296 - accuracy: 0.9520 - val_loss: 0.1852 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1277 - accuracy: 0.9518 - val_loss: 0.1865 - val_accuracy: 0.9353 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1260 - accuracy: 0.9520 - val_loss: 0.1847 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1269 - accuracy: 0.9518 - val_loss: 0.1896 - val_accuracy: 0.9351 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1240 - accuracy: 0.9531 - val_loss: 0.1848 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1236 - accuracy: 0.9530 - val_loss: 0.1849 - val_accuracy: 0.9361 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1221 - accuracy: 0.9538 - val_loss: 0.1837 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1217 - accuracy: 0.9539 - val_loss: 0.1866 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1175 - accuracy: 0.9554 - val_loss: 0.1870 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1198 - accuracy: 0.9535 - val_loss: 0.1876 - val_accuracy: 0.9354 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "393/399 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9564\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 62.\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1160 - accuracy: 0.9563 - val_loss: 0.1854 - val_accuracy: 0.9377 - lr: 1.0000e-04\n",
      "Epoch 82: early stopping\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 10:51:22.560858: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_6/dropout_18/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 5s 5ms/step - loss: 1.0582 - accuracy: 0.6093 - val_loss: 0.6116 - val_accuracy: 0.7787 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.6186 - accuracy: 0.7715 - val_loss: 0.5005 - val_accuracy: 0.8187 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.5214 - accuracy: 0.8060 - val_loss: 0.4274 - val_accuracy: 0.8442 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4631 - accuracy: 0.8309 - val_loss: 0.3850 - val_accuracy: 0.8559 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4283 - accuracy: 0.8423 - val_loss: 0.3538 - val_accuracy: 0.8656 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4006 - accuracy: 0.8518 - val_loss: 0.3341 - val_accuracy: 0.8742 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3775 - accuracy: 0.8610 - val_loss: 0.3172 - val_accuracy: 0.8818 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3625 - accuracy: 0.8660 - val_loss: 0.2993 - val_accuracy: 0.8900 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3471 - accuracy: 0.8725 - val_loss: 0.2903 - val_accuracy: 0.8913 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3343 - accuracy: 0.8756 - val_loss: 0.2822 - val_accuracy: 0.8938 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3269 - accuracy: 0.8779 - val_loss: 0.2854 - val_accuracy: 0.8931 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3151 - accuracy: 0.8822 - val_loss: 0.2716 - val_accuracy: 0.8989 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3040 - accuracy: 0.8876 - val_loss: 0.2621 - val_accuracy: 0.9010 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2987 - accuracy: 0.8892 - val_loss: 0.2528 - val_accuracy: 0.9038 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2901 - accuracy: 0.8928 - val_loss: 0.2523 - val_accuracy: 0.9052 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2832 - accuracy: 0.8947 - val_loss: 0.2527 - val_accuracy: 0.9060 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2777 - accuracy: 0.8963 - val_loss: 0.2409 - val_accuracy: 0.9088 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2711 - accuracy: 0.8990 - val_loss: 0.2354 - val_accuracy: 0.9121 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2660 - accuracy: 0.9005 - val_loss: 0.2310 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2592 - accuracy: 0.9043 - val_loss: 0.2279 - val_accuracy: 0.9162 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2568 - accuracy: 0.9046 - val_loss: 0.2328 - val_accuracy: 0.9141 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2505 - accuracy: 0.9065 - val_loss: 0.2181 - val_accuracy: 0.9191 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2473 - accuracy: 0.9088 - val_loss: 0.2175 - val_accuracy: 0.9204 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2384 - accuracy: 0.9113 - val_loss: 0.2191 - val_accuracy: 0.9183 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2332 - accuracy: 0.9125 - val_loss: 0.2111 - val_accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2323 - accuracy: 0.9140 - val_loss: 0.2086 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2266 - accuracy: 0.9157 - val_loss: 0.2061 - val_accuracy: 0.9243 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2244 - accuracy: 0.9173 - val_loss: 0.2102 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2195 - accuracy: 0.9192 - val_loss: 0.1991 - val_accuracy: 0.9249 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2175 - accuracy: 0.9196 - val_loss: 0.1991 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2122 - accuracy: 0.9205 - val_loss: 0.1977 - val_accuracy: 0.9277 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2098 - accuracy: 0.9220 - val_loss: 0.2049 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2059 - accuracy: 0.9235 - val_loss: 0.2009 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2038 - accuracy: 0.9247 - val_loss: 0.1945 - val_accuracy: 0.9273 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2010 - accuracy: 0.9243 - val_loss: 0.1905 - val_accuracy: 0.9306 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2000 - accuracy: 0.9264 - val_loss: 0.1972 - val_accuracy: 0.9284 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1965 - accuracy: 0.9277 - val_loss: 0.1931 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1925 - accuracy: 0.9283 - val_loss: 0.1882 - val_accuracy: 0.9324 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1899 - accuracy: 0.9296 - val_loss: 0.1879 - val_accuracy: 0.9309 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1891 - accuracy: 0.9295 - val_loss: 0.1873 - val_accuracy: 0.9290 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "399/399 [==============================] - 3s 6ms/step - loss: 0.1878 - accuracy: 0.9314 - val_loss: 0.1835 - val_accuracy: 0.9311 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1826 - accuracy: 0.9327 - val_loss: 0.1864 - val_accuracy: 0.9308 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1784 - accuracy: 0.9330 - val_loss: 0.1840 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1771 - accuracy: 0.9342 - val_loss: 0.1852 - val_accuracy: 0.9330 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1756 - accuracy: 0.9343 - val_loss: 0.1854 - val_accuracy: 0.9321 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1709 - accuracy: 0.9355 - val_loss: 0.1821 - val_accuracy: 0.9324 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1722 - accuracy: 0.9349 - val_loss: 0.1811 - val_accuracy: 0.9342 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1702 - accuracy: 0.9367 - val_loss: 0.1811 - val_accuracy: 0.9331 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1692 - accuracy: 0.9361 - val_loss: 0.1806 - val_accuracy: 0.9320 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1644 - accuracy: 0.9382 - val_loss: 0.1820 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1630 - accuracy: 0.9388 - val_loss: 0.1787 - val_accuracy: 0.9344 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1624 - accuracy: 0.9391 - val_loss: 0.1770 - val_accuracy: 0.9348 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1615 - accuracy: 0.9394 - val_loss: 0.1782 - val_accuracy: 0.9348 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1546 - accuracy: 0.9424 - val_loss: 0.1778 - val_accuracy: 0.9353 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1545 - accuracy: 0.9421 - val_loss: 0.1804 - val_accuracy: 0.9347 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1531 - accuracy: 0.9421 - val_loss: 0.1795 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1501 - accuracy: 0.9433 - val_loss: 0.1806 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1488 - accuracy: 0.9431 - val_loss: 0.1846 - val_accuracy: 0.9337 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1484 - accuracy: 0.9437 - val_loss: 0.1759 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1436 - accuracy: 0.9456 - val_loss: 0.1841 - val_accuracy: 0.9347 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1421 - accuracy: 0.9459 - val_loss: 0.1798 - val_accuracy: 0.9361 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1402 - accuracy: 0.9479 - val_loss: 0.1799 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1392 - accuracy: 0.9480 - val_loss: 0.1796 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1373 - accuracy: 0.9495 - val_loss: 0.1815 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1365 - accuracy: 0.9478 - val_loss: 0.1821 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1346 - accuracy: 0.9489 - val_loss: 0.1779 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1314 - accuracy: 0.9502 - val_loss: 0.1822 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1316 - accuracy: 0.9505 - val_loss: 0.1782 - val_accuracy: 0.9360 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1298 - accuracy: 0.9513 - val_loss: 0.1770 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1303 - accuracy: 0.9512 - val_loss: 0.1796 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1252 - accuracy: 0.9527 - val_loss: 0.1748 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1271 - accuracy: 0.9514 - val_loss: 0.1762 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1244 - accuracy: 0.9535 - val_loss: 0.1751 - val_accuracy: 0.9389 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1229 - accuracy: 0.9546 - val_loss: 0.1781 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1187 - accuracy: 0.9545 - val_loss: 0.1811 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1214 - accuracy: 0.9543 - val_loss: 0.1743 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1193 - accuracy: 0.9548 - val_loss: 0.1752 - val_accuracy: 0.9399 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1176 - accuracy: 0.9559 - val_loss: 0.1745 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1171 - accuracy: 0.9554 - val_loss: 0.1780 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1154 - accuracy: 0.9556 - val_loss: 0.1841 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1123 - accuracy: 0.9575 - val_loss: 0.1834 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1128 - accuracy: 0.9572 - val_loss: 0.1823 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1109 - accuracy: 0.9579 - val_loss: 0.1774 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1107 - accuracy: 0.9584 - val_loss: 0.1778 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1094 - accuracy: 0.9579 - val_loss: 0.1821 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1038 - accuracy: 0.9600 - val_loss: 0.1862 - val_accuracy: 0.9360 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1052 - accuracy: 0.9612 - val_loss: 0.1817 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1034 - accuracy: 0.9608 - val_loss: 0.1873 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1024 - accuracy: 0.9609 - val_loss: 0.1837 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1033 - accuracy: 0.9604 - val_loss: 0.1859 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1016 - accuracy: 0.9615 - val_loss: 0.1871 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1007 - accuracy: 0.9613 - val_loss: 0.1835 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0984 - accuracy: 0.9619 - val_loss: 0.1855 - val_accuracy: 0.9403 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0971 - accuracy: 0.9622 - val_loss: 0.1856 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0961 - accuracy: 0.9635 - val_loss: 0.1927 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0964 - accuracy: 0.9624 - val_loss: 0.1955 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0943 - accuracy: 0.9639 - val_loss: 0.1887 - val_accuracy: 0.9404 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0920 - accuracy: 0.9648 - val_loss: 0.1917 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0926 - accuracy: 0.9645 - val_loss: 0.1857 - val_accuracy: 0.9407 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0899 - accuracy: 0.9658 - val_loss: 0.1966 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0937 - accuracy: 0.9636 - val_loss: 0.1867 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0888 - accuracy: 0.9659 - val_loss: 0.1997 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0895 - accuracy: 0.9655 - val_loss: 0.1902 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0870 - accuracy: 0.9665 - val_loss: 0.1976 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0877 - accuracy: 0.9661 - val_loss: 0.1912 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0862 - accuracy: 0.9668 - val_loss: 0.1959 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0848 - accuracy: 0.9678 - val_loss: 0.1923 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0852 - accuracy: 0.9679 - val_loss: 0.1900 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0818 - accuracy: 0.9686 - val_loss: 0.1978 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0829 - accuracy: 0.9686 - val_loss: 0.2017 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0827 - accuracy: 0.9686 - val_loss: 0.1960 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0827 - accuracy: 0.9682 - val_loss: 0.1951 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0781 - accuracy: 0.9709 - val_loss: 0.2008 - val_accuracy: 0.9368 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0818 - accuracy: 0.9694 - val_loss: 0.2006 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0782 - accuracy: 0.9707 - val_loss: 0.1993 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0775 - accuracy: 0.9698 - val_loss: 0.2057 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0767 - accuracy: 0.9706 - val_loss: 0.1986 - val_accuracy: 0.9389 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0778 - accuracy: 0.9705 - val_loss: 0.2030 - val_accuracy: 0.9392 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0746 - accuracy: 0.9718 - val_loss: 0.2059 - val_accuracy: 0.9412 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0735 - accuracy: 0.9717 - val_loss: 0.2040 - val_accuracy: 0.9397 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0747 - accuracy: 0.9708 - val_loss: 0.2034 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0737 - accuracy: 0.9705 - val_loss: 0.2094 - val_accuracy: 0.9383 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0722 - accuracy: 0.9720 - val_loss: 0.2058 - val_accuracy: 0.9416 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0702 - accuracy: 0.9736 - val_loss: 0.2069 - val_accuracy: 0.9410 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0725 - accuracy: 0.9724 - val_loss: 0.2110 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0712 - accuracy: 0.9729 - val_loss: 0.2049 - val_accuracy: 0.9396 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0703 - accuracy: 0.9732 - val_loss: 0.2036 - val_accuracy: 0.9427 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0690 - accuracy: 0.9732 - val_loss: 0.2141 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0683 - accuracy: 0.9744 - val_loss: 0.2137 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0668 - accuracy: 0.9738 - val_loss: 0.2136 - val_accuracy: 0.9409 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0661 - accuracy: 0.9748 - val_loss: 0.2126 - val_accuracy: 0.9398 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0651 - accuracy: 0.9755 - val_loss: 0.2043 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0649 - accuracy: 0.9752 - val_loss: 0.2137 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0689 - accuracy: 0.9733 - val_loss: 0.2072 - val_accuracy: 0.9394 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0625 - accuracy: 0.9763 - val_loss: 0.2095 - val_accuracy: 0.9396 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0624 - accuracy: 0.9757 - val_loss: 0.2163 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0648 - accuracy: 0.9760 - val_loss: 0.2108 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0651 - accuracy: 0.9745 - val_loss: 0.2134 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0613 - accuracy: 0.9772 - val_loss: 0.2181 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0599 - accuracy: 0.9773 - val_loss: 0.2185 - val_accuracy: 0.9403 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0618 - accuracy: 0.9768 - val_loss: 0.2204 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0604 - accuracy: 0.9765 - val_loss: 0.2179 - val_accuracy: 0.9399 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0609 - accuracy: 0.9772 - val_loss: 0.2156 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0611 - accuracy: 0.9768 - val_loss: 0.2137 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0585 - accuracy: 0.9776 - val_loss: 0.2171 - val_accuracy: 0.9403 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0583 - accuracy: 0.9780 - val_loss: 0.2254 - val_accuracy: 0.9409 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "395/399 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9787\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 127.\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0571 - accuracy: 0.9787 - val_loss: 0.2125 - val_accuracy: 0.9401 - lr: 1.0000e-04\n",
      "Epoch 147: early stopping\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 10:56:28.505825: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_7/dropout_21/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 5s 5ms/step - loss: 1.0670 - accuracy: 0.6060 - val_loss: 0.6262 - val_accuracy: 0.7746 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.6268 - accuracy: 0.7670 - val_loss: 0.4987 - val_accuracy: 0.8224 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.5253 - accuracy: 0.8058 - val_loss: 0.4396 - val_accuracy: 0.8383 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4655 - accuracy: 0.8292 - val_loss: 0.3911 - val_accuracy: 0.8560 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4285 - accuracy: 0.8435 - val_loss: 0.3609 - val_accuracy: 0.8668 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3996 - accuracy: 0.8514 - val_loss: 0.3384 - val_accuracy: 0.8740 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3770 - accuracy: 0.8619 - val_loss: 0.3170 - val_accuracy: 0.8821 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3592 - accuracy: 0.8680 - val_loss: 0.3227 - val_accuracy: 0.8754 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3488 - accuracy: 0.8723 - val_loss: 0.2985 - val_accuracy: 0.8923 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3331 - accuracy: 0.8785 - val_loss: 0.2813 - val_accuracy: 0.8961 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3241 - accuracy: 0.8797 - val_loss: 0.2859 - val_accuracy: 0.8919 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3141 - accuracy: 0.8844 - val_loss: 0.2679 - val_accuracy: 0.9003 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3042 - accuracy: 0.8885 - val_loss: 0.2612 - val_accuracy: 0.9020 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2975 - accuracy: 0.8914 - val_loss: 0.2640 - val_accuracy: 0.9014 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2877 - accuracy: 0.8946 - val_loss: 0.2526 - val_accuracy: 0.9052 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2826 - accuracy: 0.8955 - val_loss: 0.2374 - val_accuracy: 0.9109 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2739 - accuracy: 0.8987 - val_loss: 0.2361 - val_accuracy: 0.9109 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2654 - accuracy: 0.9027 - val_loss: 0.2294 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2621 - accuracy: 0.9032 - val_loss: 0.2395 - val_accuracy: 0.9119 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2553 - accuracy: 0.9060 - val_loss: 0.2271 - val_accuracy: 0.9139 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2506 - accuracy: 0.9068 - val_loss: 0.2197 - val_accuracy: 0.9203 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2470 - accuracy: 0.9089 - val_loss: 0.2152 - val_accuracy: 0.9184 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2415 - accuracy: 0.9109 - val_loss: 0.2136 - val_accuracy: 0.9189 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2366 - accuracy: 0.9124 - val_loss: 0.2125 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2326 - accuracy: 0.9157 - val_loss: 0.2079 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2279 - accuracy: 0.9157 - val_loss: 0.2028 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2250 - accuracy: 0.9172 - val_loss: 0.2052 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2197 - accuracy: 0.9187 - val_loss: 0.2000 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2157 - accuracy: 0.9203 - val_loss: 0.2029 - val_accuracy: 0.9243 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2149 - accuracy: 0.9208 - val_loss: 0.1964 - val_accuracy: 0.9247 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2119 - accuracy: 0.9222 - val_loss: 0.1979 - val_accuracy: 0.9259 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2092 - accuracy: 0.9221 - val_loss: 0.1952 - val_accuracy: 0.9263 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2043 - accuracy: 0.9254 - val_loss: 0.1891 - val_accuracy: 0.9287 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2021 - accuracy: 0.9255 - val_loss: 0.1905 - val_accuracy: 0.9283 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1968 - accuracy: 0.9268 - val_loss: 0.1906 - val_accuracy: 0.9283 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1954 - accuracy: 0.9275 - val_loss: 0.1901 - val_accuracy: 0.9287 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1913 - accuracy: 0.9289 - val_loss: 0.1888 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1917 - accuracy: 0.9293 - val_loss: 0.1861 - val_accuracy: 0.9320 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1876 - accuracy: 0.9302 - val_loss: 0.1888 - val_accuracy: 0.9290 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1868 - accuracy: 0.9308 - val_loss: 0.1851 - val_accuracy: 0.9318 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1827 - accuracy: 0.9308 - val_loss: 0.1916 - val_accuracy: 0.9302 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1803 - accuracy: 0.9324 - val_loss: 0.1849 - val_accuracy: 0.9322 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1775 - accuracy: 0.9351 - val_loss: 0.1836 - val_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1748 - accuracy: 0.9346 - val_loss: 0.1803 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1737 - accuracy: 0.9350 - val_loss: 0.1807 - val_accuracy: 0.9344 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1706 - accuracy: 0.9367 - val_loss: 0.1851 - val_accuracy: 0.9317 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1681 - accuracy: 0.9378 - val_loss: 0.1824 - val_accuracy: 0.9323 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1665 - accuracy: 0.9374 - val_loss: 0.1836 - val_accuracy: 0.9330 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1649 - accuracy: 0.9378 - val_loss: 0.1807 - val_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1635 - accuracy: 0.9390 - val_loss: 0.1820 - val_accuracy: 0.9339 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1616 - accuracy: 0.9400 - val_loss: 0.1821 - val_accuracy: 0.9329 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1572 - accuracy: 0.9416 - val_loss: 0.1830 - val_accuracy: 0.9336 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1555 - accuracy: 0.9421 - val_loss: 0.1802 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1540 - accuracy: 0.9416 - val_loss: 0.1799 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1528 - accuracy: 0.9423 - val_loss: 0.1789 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1504 - accuracy: 0.9438 - val_loss: 0.1782 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1494 - accuracy: 0.9434 - val_loss: 0.1799 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1477 - accuracy: 0.9443 - val_loss: 0.1777 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1462 - accuracy: 0.9447 - val_loss: 0.1862 - val_accuracy: 0.9337 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1457 - accuracy: 0.9451 - val_loss: 0.1810 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1420 - accuracy: 0.9473 - val_loss: 0.1789 - val_accuracy: 0.9368 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1396 - accuracy: 0.9480 - val_loss: 0.1777 - val_accuracy: 0.9377 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1372 - accuracy: 0.9486 - val_loss: 0.1862 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1382 - accuracy: 0.9476 - val_loss: 0.1786 - val_accuracy: 0.9344 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1340 - accuracy: 0.9493 - val_loss: 0.1854 - val_accuracy: 0.9353 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1323 - accuracy: 0.9496 - val_loss: 0.1785 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1304 - accuracy: 0.9512 - val_loss: 0.1825 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1299 - accuracy: 0.9508 - val_loss: 0.1781 - val_accuracy: 0.9383 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1270 - accuracy: 0.9522 - val_loss: 0.1795 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1246 - accuracy: 0.9528 - val_loss: 0.1810 - val_accuracy: 0.9361 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1252 - accuracy: 0.9528 - val_loss: 0.1832 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1241 - accuracy: 0.9539 - val_loss: 0.1805 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1206 - accuracy: 0.9542 - val_loss: 0.1822 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1216 - accuracy: 0.9542 - val_loss: 0.1794 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1187 - accuracy: 0.9554 - val_loss: 0.1860 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1217 - accuracy: 0.9540 - val_loss: 0.1793 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1160 - accuracy: 0.9570 - val_loss: 0.1871 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1164 - accuracy: 0.9557 - val_loss: 0.1863 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1132 - accuracy: 0.9572 - val_loss: 0.1854 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1149 - accuracy: 0.9560 - val_loss: 0.1805 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1116 - accuracy: 0.9575 - val_loss: 0.1870 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1141 - accuracy: 0.9573 - val_loss: 0.1867 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1120 - accuracy: 0.9577 - val_loss: 0.1923 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1092 - accuracy: 0.9592 - val_loss: 0.1835 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1068 - accuracy: 0.9594 - val_loss: 0.1848 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1060 - accuracy: 0.9599 - val_loss: 0.1886 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1035 - accuracy: 0.9611 - val_loss: 0.1893 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1057 - accuracy: 0.9601 - val_loss: 0.1938 - val_accuracy: 0.9370 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1030 - accuracy: 0.9611 - val_loss: 0.1859 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1000 - accuracy: 0.9620 - val_loss: 0.1903 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1014 - accuracy: 0.9623 - val_loss: 0.1857 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0991 - accuracy: 0.9631 - val_loss: 0.1894 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0982 - accuracy: 0.9624 - val_loss: 0.1904 - val_accuracy: 0.9387 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0957 - accuracy: 0.9632 - val_loss: 0.1888 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0954 - accuracy: 0.9646 - val_loss: 0.1862 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0928 - accuracy: 0.9645 - val_loss: 0.1941 - val_accuracy: 0.9368 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0939 - accuracy: 0.9640 - val_loss: 0.1930 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0928 - accuracy: 0.9644 - val_loss: 0.1921 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0909 - accuracy: 0.9650 - val_loss: 0.1985 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0912 - accuracy: 0.9655 - val_loss: 0.1987 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0885 - accuracy: 0.9666 - val_loss: 0.1937 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0856 - accuracy: 0.9670 - val_loss: 0.1983 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0896 - accuracy: 0.9654 - val_loss: 0.1934 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0872 - accuracy: 0.9669 - val_loss: 0.1942 - val_accuracy: 0.9381 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0853 - accuracy: 0.9674 - val_loss: 0.1965 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0834 - accuracy: 0.9683 - val_loss: 0.1951 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0841 - accuracy: 0.9684 - val_loss: 0.2023 - val_accuracy: 0.9394 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0796 - accuracy: 0.9691 - val_loss: 0.2050 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0813 - accuracy: 0.9690 - val_loss: 0.2006 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0795 - accuracy: 0.9698 - val_loss: 0.2088 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0798 - accuracy: 0.9697 - val_loss: 0.2071 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0769 - accuracy: 0.9700 - val_loss: 0.2100 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0789 - accuracy: 0.9696 - val_loss: 0.2090 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0787 - accuracy: 0.9694 - val_loss: 0.2031 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0788 - accuracy: 0.9699 - val_loss: 0.2056 - val_accuracy: 0.9399 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0780 - accuracy: 0.9696 - val_loss: 0.2045 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0731 - accuracy: 0.9724 - val_loss: 0.2170 - val_accuracy: 0.9378 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "399/399 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9719\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 98.\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0746 - accuracy: 0.9719 - val_loss: 0.2035 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 118: early stopping\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 11:00:36.558291: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_8/dropout_24/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 5s 5ms/step - loss: 1.0563 - accuracy: 0.6131 - val_loss: 0.6384 - val_accuracy: 0.7613 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.6206 - accuracy: 0.7680 - val_loss: 0.4910 - val_accuracy: 0.8227 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.5197 - accuracy: 0.8080 - val_loss: 0.4222 - val_accuracy: 0.8464 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4589 - accuracy: 0.8301 - val_loss: 0.3791 - val_accuracy: 0.8607 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4220 - accuracy: 0.8452 - val_loss: 0.3522 - val_accuracy: 0.8711 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3956 - accuracy: 0.8546 - val_loss: 0.3299 - val_accuracy: 0.8751 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3758 - accuracy: 0.8631 - val_loss: 0.3146 - val_accuracy: 0.8836 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3575 - accuracy: 0.8674 - val_loss: 0.3011 - val_accuracy: 0.8871 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3420 - accuracy: 0.8735 - val_loss: 0.2927 - val_accuracy: 0.8921 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3300 - accuracy: 0.8780 - val_loss: 0.2786 - val_accuracy: 0.8963 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3203 - accuracy: 0.8831 - val_loss: 0.2691 - val_accuracy: 0.8989 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3089 - accuracy: 0.8875 - val_loss: 0.2695 - val_accuracy: 0.9006 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3020 - accuracy: 0.8879 - val_loss: 0.2652 - val_accuracy: 0.9009 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2935 - accuracy: 0.8929 - val_loss: 0.2548 - val_accuracy: 0.9063 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2851 - accuracy: 0.8941 - val_loss: 0.2420 - val_accuracy: 0.9121 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2770 - accuracy: 0.8983 - val_loss: 0.2378 - val_accuracy: 0.9121 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2710 - accuracy: 0.9008 - val_loss: 0.2328 - val_accuracy: 0.9141 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2652 - accuracy: 0.9023 - val_loss: 0.2299 - val_accuracy: 0.9171 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2601 - accuracy: 0.9038 - val_loss: 0.2227 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2528 - accuracy: 0.9071 - val_loss: 0.2225 - val_accuracy: 0.9183 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2493 - accuracy: 0.9070 - val_loss: 0.2233 - val_accuracy: 0.9191 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2446 - accuracy: 0.9105 - val_loss: 0.2129 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2375 - accuracy: 0.9123 - val_loss: 0.2135 - val_accuracy: 0.9209 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2344 - accuracy: 0.9124 - val_loss: 0.2080 - val_accuracy: 0.9249 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2329 - accuracy: 0.9137 - val_loss: 0.2055 - val_accuracy: 0.9257 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2271 - accuracy: 0.9154 - val_loss: 0.2069 - val_accuracy: 0.9251 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2237 - accuracy: 0.9172 - val_loss: 0.2030 - val_accuracy: 0.9268 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2191 - accuracy: 0.9192 - val_loss: 0.2083 - val_accuracy: 0.9251 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2166 - accuracy: 0.9199 - val_loss: 0.2003 - val_accuracy: 0.9287 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2106 - accuracy: 0.9221 - val_loss: 0.2059 - val_accuracy: 0.9233 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2107 - accuracy: 0.9219 - val_loss: 0.1990 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2061 - accuracy: 0.9243 - val_loss: 0.1975 - val_accuracy: 0.9299 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2004 - accuracy: 0.9266 - val_loss: 0.1994 - val_accuracy: 0.9269 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2005 - accuracy: 0.9261 - val_loss: 0.1956 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1996 - accuracy: 0.9258 - val_loss: 0.1916 - val_accuracy: 0.9316 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1947 - accuracy: 0.9282 - val_loss: 0.1919 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1921 - accuracy: 0.9282 - val_loss: 0.1939 - val_accuracy: 0.9327 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1880 - accuracy: 0.9305 - val_loss: 0.1879 - val_accuracy: 0.9321 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1883 - accuracy: 0.9307 - val_loss: 0.1890 - val_accuracy: 0.9320 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1842 - accuracy: 0.9316 - val_loss: 0.1879 - val_accuracy: 0.9354 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1820 - accuracy: 0.9327 - val_loss: 0.1885 - val_accuracy: 0.9309 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1784 - accuracy: 0.9340 - val_loss: 0.1979 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1782 - accuracy: 0.9338 - val_loss: 0.1850 - val_accuracy: 0.9336 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1735 - accuracy: 0.9345 - val_loss: 0.1927 - val_accuracy: 0.9299 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1735 - accuracy: 0.9351 - val_loss: 0.1864 - val_accuracy: 0.9318 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1683 - accuracy: 0.9375 - val_loss: 0.1860 - val_accuracy: 0.9330 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1667 - accuracy: 0.9375 - val_loss: 0.1872 - val_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1648 - accuracy: 0.9377 - val_loss: 0.1849 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1655 - accuracy: 0.9389 - val_loss: 0.1873 - val_accuracy: 0.9329 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1621 - accuracy: 0.9396 - val_loss: 0.1853 - val_accuracy: 0.9333 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1567 - accuracy: 0.9409 - val_loss: 0.1830 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1576 - accuracy: 0.9397 - val_loss: 0.1862 - val_accuracy: 0.9331 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1558 - accuracy: 0.9422 - val_loss: 0.1847 - val_accuracy: 0.9344 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1545 - accuracy: 0.9422 - val_loss: 0.1870 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1530 - accuracy: 0.9420 - val_loss: 0.1834 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1487 - accuracy: 0.9436 - val_loss: 0.1844 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1475 - accuracy: 0.9440 - val_loss: 0.1888 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1453 - accuracy: 0.9456 - val_loss: 0.1904 - val_accuracy: 0.9322 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1446 - accuracy: 0.9457 - val_loss: 0.1825 - val_accuracy: 0.9336 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "389/399 [============================>.] - ETA: 0s - loss: 0.1431 - accuracy: 0.9469\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1430 - accuracy: 0.9469 - val_loss: 0.1801 - val_accuracy: 0.9352 - lr: 1.0000e-04\n",
      "Epoch 60: early stopping\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 11:02:41.173816: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_9/dropout_27/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 5s 5ms/step - loss: 1.0513 - accuracy: 0.6131 - val_loss: 0.6002 - val_accuracy: 0.7778 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.6009 - accuracy: 0.7765 - val_loss: 0.4737 - val_accuracy: 0.8297 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.5048 - accuracy: 0.8137 - val_loss: 0.4155 - val_accuracy: 0.8482 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4503 - accuracy: 0.8321 - val_loss: 0.3790 - val_accuracy: 0.8582 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.4171 - accuracy: 0.8464 - val_loss: 0.3416 - val_accuracy: 0.8748 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3883 - accuracy: 0.8583 - val_loss: 0.3397 - val_accuracy: 0.8778 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3682 - accuracy: 0.8657 - val_loss: 0.3111 - val_accuracy: 0.8879 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3559 - accuracy: 0.8700 - val_loss: 0.3020 - val_accuracy: 0.8864 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3411 - accuracy: 0.8741 - val_loss: 0.2881 - val_accuracy: 0.8934 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3284 - accuracy: 0.8797 - val_loss: 0.2734 - val_accuracy: 0.8979 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3186 - accuracy: 0.8832 - val_loss: 0.2677 - val_accuracy: 0.9013 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.3067 - accuracy: 0.8878 - val_loss: 0.2615 - val_accuracy: 0.9037 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2980 - accuracy: 0.8894 - val_loss: 0.2526 - val_accuracy: 0.9039 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2889 - accuracy: 0.8937 - val_loss: 0.2479 - val_accuracy: 0.9068 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2809 - accuracy: 0.8965 - val_loss: 0.2429 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2733 - accuracy: 0.8990 - val_loss: 0.2362 - val_accuracy: 0.9114 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2684 - accuracy: 0.9010 - val_loss: 0.2317 - val_accuracy: 0.9146 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2615 - accuracy: 0.9037 - val_loss: 0.2255 - val_accuracy: 0.9160 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2541 - accuracy: 0.9051 - val_loss: 0.2241 - val_accuracy: 0.9167 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2485 - accuracy: 0.9085 - val_loss: 0.2205 - val_accuracy: 0.9178 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2452 - accuracy: 0.9087 - val_loss: 0.2216 - val_accuracy: 0.9172 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2400 - accuracy: 0.9109 - val_loss: 0.2243 - val_accuracy: 0.9178 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2356 - accuracy: 0.9129 - val_loss: 0.2139 - val_accuracy: 0.9197 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2321 - accuracy: 0.9139 - val_loss: 0.2110 - val_accuracy: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2282 - accuracy: 0.9162 - val_loss: 0.2105 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2228 - accuracy: 0.9169 - val_loss: 0.2034 - val_accuracy: 0.9243 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2205 - accuracy: 0.9192 - val_loss: 0.2061 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2164 - accuracy: 0.9206 - val_loss: 0.2080 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2147 - accuracy: 0.9210 - val_loss: 0.2030 - val_accuracy: 0.9253 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2079 - accuracy: 0.9227 - val_loss: 0.2010 - val_accuracy: 0.9251 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2075 - accuracy: 0.9228 - val_loss: 0.1980 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.2015 - accuracy: 0.9253 - val_loss: 0.1932 - val_accuracy: 0.9281 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1999 - accuracy: 0.9263 - val_loss: 0.1930 - val_accuracy: 0.9282 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1956 - accuracy: 0.9277 - val_loss: 0.1931 - val_accuracy: 0.9280 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1960 - accuracy: 0.9272 - val_loss: 0.1928 - val_accuracy: 0.9277 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1930 - accuracy: 0.9285 - val_loss: 0.1924 - val_accuracy: 0.9296 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1889 - accuracy: 0.9299 - val_loss: 0.1927 - val_accuracy: 0.9291 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1852 - accuracy: 0.9308 - val_loss: 0.1972 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1839 - accuracy: 0.9305 - val_loss: 0.1902 - val_accuracy: 0.9304 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1823 - accuracy: 0.9332 - val_loss: 0.1864 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1783 - accuracy: 0.9337 - val_loss: 0.1861 - val_accuracy: 0.9326 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1774 - accuracy: 0.9335 - val_loss: 0.1894 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1752 - accuracy: 0.9363 - val_loss: 0.1886 - val_accuracy: 0.9314 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1710 - accuracy: 0.9364 - val_loss: 0.1850 - val_accuracy: 0.9328 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1708 - accuracy: 0.9360 - val_loss: 0.1825 - val_accuracy: 0.9313 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1703 - accuracy: 0.9370 - val_loss: 0.1831 - val_accuracy: 0.9334 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1671 - accuracy: 0.9375 - val_loss: 0.1836 - val_accuracy: 0.9339 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1620 - accuracy: 0.9390 - val_loss: 0.1854 - val_accuracy: 0.9309 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1617 - accuracy: 0.9386 - val_loss: 0.1793 - val_accuracy: 0.9338 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1610 - accuracy: 0.9397 - val_loss: 0.1845 - val_accuracy: 0.9339 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1579 - accuracy: 0.9408 - val_loss: 0.1900 - val_accuracy: 0.9321 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1559 - accuracy: 0.9411 - val_loss: 0.1801 - val_accuracy: 0.9341 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1540 - accuracy: 0.9424 - val_loss: 0.1827 - val_accuracy: 0.9331 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1514 - accuracy: 0.9424 - val_loss: 0.1802 - val_accuracy: 0.9340 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1489 - accuracy: 0.9446 - val_loss: 0.1822 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1483 - accuracy: 0.9435 - val_loss: 0.1853 - val_accuracy: 0.9330 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1482 - accuracy: 0.9445 - val_loss: 0.1824 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1442 - accuracy: 0.9456 - val_loss: 0.1795 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1438 - accuracy: 0.9469 - val_loss: 0.1826 - val_accuracy: 0.9354 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1422 - accuracy: 0.9467 - val_loss: 0.1804 - val_accuracy: 0.9352 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1406 - accuracy: 0.9464 - val_loss: 0.1825 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1394 - accuracy: 0.9476 - val_loss: 0.1833 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1376 - accuracy: 0.9485 - val_loss: 0.1849 - val_accuracy: 0.9348 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1335 - accuracy: 0.9497 - val_loss: 0.1815 - val_accuracy: 0.9352 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1342 - accuracy: 0.9489 - val_loss: 0.1834 - val_accuracy: 0.9350 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1315 - accuracy: 0.9500 - val_loss: 0.1799 - val_accuracy: 0.9368 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1276 - accuracy: 0.9526 - val_loss: 0.1821 - val_accuracy: 0.9348 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1297 - accuracy: 0.9507 - val_loss: 0.1838 - val_accuracy: 0.9347 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1271 - accuracy: 0.9511 - val_loss: 0.1795 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1266 - accuracy: 0.9526 - val_loss: 0.1847 - val_accuracy: 0.9343 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1248 - accuracy: 0.9533 - val_loss: 0.1811 - val_accuracy: 0.9358 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1226 - accuracy: 0.9533 - val_loss: 0.1806 - val_accuracy: 0.9360 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1210 - accuracy: 0.9541 - val_loss: 0.1821 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1168 - accuracy: 0.9552 - val_loss: 0.1863 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1167 - accuracy: 0.9559 - val_loss: 0.1807 - val_accuracy: 0.9383 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1153 - accuracy: 0.9558 - val_loss: 0.1878 - val_accuracy: 0.9361 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1147 - accuracy: 0.9573 - val_loss: 0.1834 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1127 - accuracy: 0.9576 - val_loss: 0.1852 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1142 - accuracy: 0.9567 - val_loss: 0.1871 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1094 - accuracy: 0.9579 - val_loss: 0.1868 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1089 - accuracy: 0.9578 - val_loss: 0.1966 - val_accuracy: 0.9356 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1081 - accuracy: 0.9580 - val_loss: 0.1815 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1062 - accuracy: 0.9599 - val_loss: 0.1922 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1056 - accuracy: 0.9611 - val_loss: 0.1887 - val_accuracy: 0.9354 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1040 - accuracy: 0.9602 - val_loss: 0.1873 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1040 - accuracy: 0.9604 - val_loss: 0.1929 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1028 - accuracy: 0.9612 - val_loss: 0.1907 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1016 - accuracy: 0.9609 - val_loss: 0.1898 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0994 - accuracy: 0.9630 - val_loss: 0.1923 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.1000 - accuracy: 0.9613 - val_loss: 0.1906 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0987 - accuracy: 0.9622 - val_loss: 0.1897 - val_accuracy: 0.9369 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0947 - accuracy: 0.9644 - val_loss: 0.1919 - val_accuracy: 0.9367 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0981 - accuracy: 0.9630 - val_loss: 0.1934 - val_accuracy: 0.9354 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0940 - accuracy: 0.9636 - val_loss: 0.1898 - val_accuracy: 0.9371 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0936 - accuracy: 0.9647 - val_loss: 0.1927 - val_accuracy: 0.9372 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0945 - accuracy: 0.9636 - val_loss: 0.1910 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0897 - accuracy: 0.9659 - val_loss: 0.1945 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0900 - accuracy: 0.9662 - val_loss: 0.1992 - val_accuracy: 0.9359 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0900 - accuracy: 0.9658 - val_loss: 0.1944 - val_accuracy: 0.9394 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0884 - accuracy: 0.9659 - val_loss: 0.2032 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0915 - accuracy: 0.9648 - val_loss: 0.1998 - val_accuracy: 0.9366 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0877 - accuracy: 0.9667 - val_loss: 0.1967 - val_accuracy: 0.9379 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0852 - accuracy: 0.9675 - val_loss: 0.1991 - val_accuracy: 0.9363 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0877 - accuracy: 0.9668 - val_loss: 0.2000 - val_accuracy: 0.9353 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0829 - accuracy: 0.9685 - val_loss: 0.1939 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0832 - accuracy: 0.9681 - val_loss: 0.1997 - val_accuracy: 0.9382 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0827 - accuracy: 0.9682 - val_loss: 0.1964 - val_accuracy: 0.9377 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0827 - accuracy: 0.9689 - val_loss: 0.1986 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0796 - accuracy: 0.9695 - val_loss: 0.1991 - val_accuracy: 0.9388 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0802 - accuracy: 0.9691 - val_loss: 0.2014 - val_accuracy: 0.9368 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0785 - accuracy: 0.9701 - val_loss: 0.2065 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0786 - accuracy: 0.9699 - val_loss: 0.2054 - val_accuracy: 0.9374 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0786 - accuracy: 0.9709 - val_loss: 0.1973 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0773 - accuracy: 0.9699 - val_loss: 0.1945 - val_accuracy: 0.9393 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0769 - accuracy: 0.9704 - val_loss: 0.1971 - val_accuracy: 0.9386 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0754 - accuracy: 0.9714 - val_loss: 0.2052 - val_accuracy: 0.9389 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0747 - accuracy: 0.9716 - val_loss: 0.2154 - val_accuracy: 0.9376 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0737 - accuracy: 0.9722 - val_loss: 0.2055 - val_accuracy: 0.9384 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "393/399 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9717\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 99.\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.0738 - accuracy: 0.9716 - val_loss: 0.2044 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 119: early stopping\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "    \n",
    "    model = create_model()\n",
    "    \n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=20, verbose=1)\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, verbose=1)\n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=300, callbacks=[lr_scheduler, early_stop])\n",
    "    \n",
    "    if not os.path.exists(exp_name+'/saved_models'):\n",
    "        print(\"Making directory\", exp_name+'/saved_models')\n",
    "        os.makedirs(exp_name+'/saved_models')\n",
    "\n",
    "    model.save_weights(f'{exp_name}/saved_models/trained_weights.h5')\n",
    "    with open(f'{exp_name}/history.pickle', 'wb') as f:\n",
    "        pickle.dump(history, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb6b3d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0255 - accuracy: 0.9927\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2036 - accuracy: 0.9401\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2487 - accuracy: 0.9309\n",
      "Run: 2\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0352 - accuracy: 0.9888\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.1993 - accuracy: 0.9393\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.2329 - accuracy: 0.9333\n",
      "Run: 3\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0339 - accuracy: 0.9892\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 0.9402\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.2363 - accuracy: 0.9315\n",
      "Run: 4\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0110 - accuracy: 0.9976\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.9409\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.9309\n",
      "Run: 5\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.1038 - accuracy: 0.9625\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.1769 - accuracy: 0.9372\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.2115 - accuracy: 0.9281\n",
      "Run: 6\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0950 - accuracy: 0.9657\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.1830 - accuracy: 0.9378\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.2235 - accuracy: 0.9235\n",
      "Run: 7\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9951\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2036 - accuracy: 0.9427\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.2525 - accuracy: 0.9328\n",
      "Run: 8\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0376 - accuracy: 0.9876\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.1921 - accuracy: 0.9400\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.2297 - accuracy: 0.9335\n",
      "Run: 9\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.1322 - accuracy: 0.9522\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.1879 - accuracy: 0.9354\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9237\n",
      "Run: 10\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0362 - accuracy: 0.9884\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.1944 - accuracy: 0.9394\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.2376 - accuracy: 0.9327\n",
      "Average train error: 1.80 (1.49)\n",
      "Average validation error: 6.07 (0.19)\n",
      "Average test error: 6.99 (0.36)\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "test_acc = []\n",
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=loss_fn, metrics=['accuracy'])\n",
    "    train_acc.append(model.evaluate(ds_train, verbose=1)[1])\n",
    "    val_acc.append(model.evaluate(ds_val, verbose=1)[1])\n",
    "    test_acc.append(model.evaluate(ds_test, verbose=1)[1])\n",
    "print(f'Average train error: {(100-np.mean(train_acc)*100):.2f} ({(np.std(train_acc)*100):.2f})')\n",
    "print(f'Average validation error: {(100-np.mean(val_acc)*100):.2f} ({(np.std(val_acc)*100):.2f})')\n",
    "print(f'Average test error: {(100-np.mean(test_acc)*100):.2f} ({(np.std(test_acc)*100):.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059232e",
   "metadata": {},
   "source": [
    "### PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48d4dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "Training PMI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]2025-06-05 11:09:14.524985: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fc7f6886f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-06-05 11:09:14.525038: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2025-06-05 11:09:14.530623: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-06-05 11:09:14.572753: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90100\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749121754.662716 1996598 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_1/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_1/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   3%|▎         | 6/200 [00:57<12:33,  3.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_1/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_1/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   8%|▊         | 16/200 [01:07<13:01,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all validation samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 9/9 [00:00<00:00, 148.76it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 9/9 [00:00<00:00, 679.28it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 9/9 [00:00<00:00, 663.46it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 9/9 [00:00<00:00, 668.91it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 9/9 [00:00<00:00, 666.31it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 9/9 [00:00<00:00, 670.41it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 9/9 [00:00<00:00, 668.97it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 9/9 [00:00<00:00, 674.24it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 9/9 [00:00<00:00, 668.43it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 9/9 [00:00<00:00, 590.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all test samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 10/10 [00:00<00:00, 276.32it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 10/10 [00:00<00:00, 664.62it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 10/10 [00:00<00:00, 698.26it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 10/10 [00:00<00:00, 688.02it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 10/10 [00:00<00:00, 700.79it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 10/10 [00:00<00:00, 703.19it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 10/10 [00:00<00:00, 699.87it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 10/10 [00:00<00:00, 701.87it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 10/10 [00:00<00:00, 695.22it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 10/10 [00:00<00:00, 712.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 2\n",
      "Training PMI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_2/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_2/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   2%|▎         | 5/200 [00:55<17:24,  5.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_2/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_2/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   3%|▎         | 6/200 [00:57<12:54,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_2/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_2/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   8%|▊         | 16/200 [01:07<12:57,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all validation samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 9/9 [00:00<00:00, 153.87it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 9/9 [00:00<00:00, 556.11it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 9/9 [00:00<00:00, 664.43it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 9/9 [00:00<00:00, 666.22it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 9/9 [00:00<00:00, 676.94it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 9/9 [00:00<00:00, 671.95it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 9/9 [00:00<00:00, 674.70it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 9/9 [00:00<00:00, 675.01it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 9/9 [00:00<00:00, 675.60it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 9/9 [00:00<00:00, 672.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all test samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 10/10 [00:00<00:00, 276.85it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 10/10 [00:00<00:00, 676.93it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 10/10 [00:00<00:00, 691.08it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 10/10 [00:00<00:00, 695.71it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 10/10 [00:00<00:00, 687.21it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 10/10 [00:00<00:00, 706.39it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 10/10 [00:00<00:00, 708.86it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 10/10 [00:00<00:00, 696.31it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 10/10 [00:00<00:00, 703.97it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 10/10 [00:00<00:00, 712.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 3\n",
      "Training PMI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_3/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_3/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   4%|▍         | 9/200 [00:59<05:52,  1.85s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_3/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_3/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:  10%|▉         | 19/200 [01:10<11:12,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all validation samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 9/9 [00:00<00:00, 155.57it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 9/9 [00:00<00:00, 659.72it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 9/9 [00:00<00:00, 670.99it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 9/9 [00:00<00:00, 679.63it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 9/9 [00:00<00:00, 668.30it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 9/9 [00:00<00:00, 677.23it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 9/9 [00:00<00:00, 678.47it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 9/9 [00:00<00:00, 689.87it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 9/9 [00:00<00:00, 684.46it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 9/9 [00:00<00:00, 681.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all test samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 10/10 [00:00<00:00, 269.97it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 10/10 [00:00<00:00, 662.14it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 10/10 [00:00<00:00, 670.51it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 10/10 [00:00<00:00, 679.96it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 10/10 [00:00<00:00, 683.61it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 10/10 [00:00<00:00, 678.85it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 10/10 [00:00<00:00, 683.27it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 10/10 [00:00<00:00, 684.25it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 10/10 [00:00<00:00, 678.80it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 10/10 [00:00<00:00, 682.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 4\n",
      "Training PMI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_4/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_4/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   0%|          | 1/200 [00:52<2:54:41, 52.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_4/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_4/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   2%|▎         | 5/200 [00:56<17:47,  5.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_4/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_4/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   3%|▎         | 6/200 [00:58<13:12,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_4/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_4/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   8%|▊         | 16/200 [01:09<13:14,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all validation samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 9/9 [00:00<00:00, 152.41it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 9/9 [00:00<00:00, 644.36it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 9/9 [00:00<00:00, 666.29it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 9/9 [00:00<00:00, 676.57it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 9/9 [00:00<00:00, 676.54it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 9/9 [00:00<00:00, 675.39it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 9/9 [00:00<00:00, 678.67it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 9/9 [00:00<00:00, 677.40it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 9/9 [00:00<00:00, 666.08it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 9/9 [00:00<00:00, 656.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all test samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 10/10 [00:00<00:00, 271.13it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 10/10 [00:00<00:00, 654.34it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 10/10 [00:00<00:00, 660.70it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 10/10 [00:00<00:00, 659.84it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 10/10 [00:00<00:00, 666.50it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 10/10 [00:00<00:00, 664.87it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 10/10 [00:00<00:00, 648.38it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 10/10 [00:00<00:00, 674.03it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 10/10 [00:00<00:00, 673.29it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 10/10 [00:00<00:00, 670.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 5\n",
      "Training PMI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_5/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_5/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   0%|          | 1/200 [00:52<2:52:29, 52.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_5/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_5/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   1%|          | 2/200 [00:53<1:13:19, 22.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_5/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_5/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   2%|▏         | 4/200 [00:55<26:19,  8.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_5/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_5/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   7%|▋         | 14/200 [01:06<14:46,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all validation samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 9/9 [00:00<00:00, 144.89it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 9/9 [00:00<00:00, 651.08it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 9/9 [00:00<00:00, 661.89it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 9/9 [00:00<00:00, 680.40it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 9/9 [00:00<00:00, 671.36it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 9/9 [00:00<00:00, 675.63it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 9/9 [00:00<00:00, 679.33it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 9/9 [00:00<00:00, 676.57it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 9/9 [00:00<00:00, 678.43it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 9/9 [00:00<00:00, 680.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all test samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 10/10 [00:00<00:00, 270.15it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 10/10 [00:00<00:00, 658.74it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 10/10 [00:00<00:00, 681.00it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 10/10 [00:00<00:00, 670.68it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 10/10 [00:00<00:00, 682.29it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 10/10 [00:00<00:00, 679.65it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 10/10 [00:00<00:00, 683.14it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 10/10 [00:00<00:00, 684.85it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 10/10 [00:00<00:00, 687.47it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 10/10 [00:00<00:00, 685.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 6\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/pmi/separable_variational_f_js\n",
      "Training PMI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   0%|          | 1/200 [00:52<2:53:18, 52.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   1%|          | 2/200 [00:53<1:13:39, 22.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   2%|▏         | 3/200 [00:54<41:53, 12.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   6%|▋         | 13/200 [01:05<15:45,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all validation samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 9/9 [00:00<00:00, 158.58it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 9/9 [00:00<00:00, 662.07it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 9/9 [00:00<00:00, 683.93it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 9/9 [00:00<00:00, 676.42it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 9/9 [00:00<00:00, 695.02it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 9/9 [00:00<00:00, 691.62it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 9/9 [00:00<00:00, 693.26it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 9/9 [00:00<00:00, 692.23it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 9/9 [00:00<00:00, 699.10it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 9/9 [00:00<00:00, 701.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all test samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 10/10 [00:00<00:00, 271.51it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 10/10 [00:00<00:00, 670.50it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 10/10 [00:00<00:00, 695.90it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 10/10 [00:00<00:00, 696.15it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 10/10 [00:00<00:00, 689.17it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 10/10 [00:00<00:00, 695.99it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 10/10 [00:00<00:00, 703.42it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 10/10 [00:00<00:00, 700.02it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 10/10 [00:00<00:00, 696.95it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 10/10 [00:00<00:00, 699.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 7\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_7/calibration/pmi/separable_variational_f_js\n",
      "Training PMI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_7/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_7/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   3%|▎         | 6/200 [00:57<12:36,  3.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_7/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_7/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   4%|▎         | 7/200 [00:58<09:55,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_7/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_7/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   8%|▊         | 17/200 [01:09<12:29,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all validation samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 9/9 [00:00<00:00, 149.13it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 9/9 [00:00<00:00, 651.84it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 9/9 [00:00<00:00, 679.26it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 9/9 [00:00<00:00, 673.81it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 9/9 [00:00<00:00, 673.58it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 9/9 [00:00<00:00, 671.75it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 9/9 [00:00<00:00, 674.04it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 9/9 [00:00<00:00, 675.36it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 9/9 [00:00<00:00, 672.70it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 9/9 [00:00<00:00, 676.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all test samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 10/10 [00:00<00:00, 273.88it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 10/10 [00:00<00:00, 656.47it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 10/10 [00:00<00:00, 675.10it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 10/10 [00:00<00:00, 677.28it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 10/10 [00:00<00:00, 676.95it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 10/10 [00:00<00:00, 683.84it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 10/10 [00:00<00:00, 674.18it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 10/10 [00:00<00:00, 680.53it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 10/10 [00:00<00:00, 673.85it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 10/10 [00:00<00:00, 681.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 8\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js\n",
      "Training PMI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   2%|▎         | 5/200 [00:55<17:28,  5.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   4%|▎         | 7/200 [00:58<09:40,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   4%|▍         | 8/200 [00:59<07:56,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   4%|▍         | 9/200 [01:00<06:48,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:  10%|▉         | 19/200 [01:12<11:28,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all validation samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 9/9 [00:00<00:00, 146.85it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 9/9 [00:00<00:00, 660.57it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 9/9 [00:00<00:00, 681.94it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 9/9 [00:00<00:00, 681.75it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 9/9 [00:00<00:00, 683.41it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 9/9 [00:00<00:00, 688.85it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 9/9 [00:00<00:00, 688.21it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 9/9 [00:00<00:00, 688.47it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 9/9 [00:00<00:00, 684.80it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 9/9 [00:00<00:00, 691.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all test samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 10/10 [00:00<00:00, 249.12it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 10/10 [00:00<00:00, 663.14it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 10/10 [00:00<00:00, 679.76it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 10/10 [00:00<00:00, 675.07it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 10/10 [00:00<00:00, 686.24it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 10/10 [00:00<00:00, 686.85it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 10/10 [00:00<00:00, 689.15it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 10/10 [00:00<00:00, 685.37it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 10/10 [00:00<00:00, 690.30it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 10/10 [00:00<00:00, 692.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 9\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_9/calibration/pmi/separable_variational_f_js\n",
      "Training PMI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_9/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_9/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   0%|          | 1/200 [00:51<2:52:20, 51.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_9/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_9/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   1%|          | 2/200 [00:53<1:13:17, 22.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_9/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_9/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   6%|▌         | 12/200 [01:04<16:44,  5.34s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all validation samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 9/9 [00:00<00:00, 148.40it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 9/9 [00:00<00:00, 626.31it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 9/9 [00:00<00:00, 648.44it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 9/9 [00:00<00:00, 647.62it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 9/9 [00:00<00:00, 652.57it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 9/9 [00:00<00:00, 663.52it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 9/9 [00:00<00:00, 624.17it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 9/9 [00:00<00:00, 651.31it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 9/9 [00:00<00:00, 650.57it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 9/9 [00:00<00:00, 649.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all test samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 10/10 [00:00<00:00, 262.17it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 10/10 [00:00<00:00, 642.03it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 10/10 [00:00<00:00, 654.44it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 10/10 [00:00<00:00, 654.88it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 10/10 [00:00<00:00, 667.40it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 10/10 [00:00<00:00, 668.67it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 10/10 [00:00<00:00, 650.91it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 10/10 [00:00<00:00, 650.94it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 10/10 [00:00<00:00, 659.97it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 10/10 [00:00<00:00, 630.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 10\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_10/calibration/pmi/separable_variational_f_js\n",
      "Training PMI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_10/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_10/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   0%|          | 1/200 [00:52<2:52:39, 52.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_10/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../results/PI_Explainability/cnn_fashion_mnist/run_10/calibration/pmi/separable_variational_f_js/pmi_output_model/assets\n",
      "Epochs:   6%|▌         | 11/200 [01:03<18:05,  5.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all validation samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 9/9 [00:00<00:00, 153.14it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 9/9 [00:00<00:00, 662.60it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 9/9 [00:00<00:00, 698.43it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 9/9 [00:00<00:00, 679.51it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 9/9 [00:00<00:00, 696.18it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 9/9 [00:00<00:00, 693.12it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 9/9 [00:00<00:00, 685.95it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 9/9 [00:00<00:00, 687.29it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 9/9 [00:00<00:00, 694.29it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 9/9 [00:00<00:00, 686.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PMI for all test samples and for all classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PMI for class 1: 100%|██████████| 10/10 [00:00<00:00, 272.26it/s]\n",
      "Computing PMI for class 2: 100%|██████████| 10/10 [00:00<00:00, 660.37it/s]\n",
      "Computing PMI for class 3: 100%|██████████| 10/10 [00:00<00:00, 694.88it/s]\n",
      "Computing PMI for class 4: 100%|██████████| 10/10 [00:00<00:00, 684.06it/s]\n",
      "Computing PMI for class 5: 100%|██████████| 10/10 [00:00<00:00, 697.42it/s]\n",
      "Computing PMI for class 6: 100%|██████████| 10/10 [00:00<00:00, 692.52it/s]\n",
      "Computing PMI for class 7: 100%|██████████| 10/10 [00:00<00:00, 695.61it/s]\n",
      "Computing PMI for class 8: 100%|██████████| 10/10 [00:00<00:00, 693.19it/s]\n",
      "Computing PMI for class 9: 100%|██████████| 10/10 [00:00<00:00, 689.34it/s]\n",
      "Computing PMI for class 10: 100%|██████████| 10/10 [00:00<00:00, 696.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.pmi_estimators import train_critic_model, neural_pmi\n",
    "from tqdm import tqdm\n",
    "\n",
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pmi/separable_variational_f_js'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "\n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PMI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "\n",
    "    print(f'Training PMI model...')\n",
    "    ds_activity_trn = ds_train.batch(128).map(lambda x, y: (int_model(x), y)).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    ds_activity_val = ds_val.batch(128).map(lambda x, y: (int_model(x), y)).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    train_critic_model(ds_activity_trn, ds_activity_val, critic='separable', estimator='variational_f_js', epochs=200, save_path=f'{exp_name}/pmi_output_model')\n",
    "\n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PMI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "\n",
    "    pmi_model = tf.keras.models.load_model(f'{exp_name}/pmi_output_model')\n",
    "    n_classes = 10\n",
    "\n",
    "    print(f'Computing PMI for all validation samples and for all classes...')\n",
    "    encoded_x = []\n",
    "    for x, _ in ds_val.batch(128):\n",
    "        encoded_x.append(int_model(x).numpy())\n",
    "    encoded_x = np.concatenate(encoded_x)\n",
    "    num_samples = encoded_x.shape[0]\n",
    "    \n",
    "    pmi_class = []\n",
    "    batch_size = 1024\n",
    "    for k in range(n_classes):\n",
    "        num_samples = encoded_x.shape[0]\n",
    "        y_k = tf.one_hot(tf.fill([num_samples], k), depth=n_classes)\n",
    "        pmi_list = []\n",
    "        for i in tqdm(range(0, len(encoded_x), batch_size), desc=f\"Computing PMI for class {k+1}\"):\n",
    "            x_batch = encoded_x[i:i+batch_size]\n",
    "            y_batch = y_k[i:i+batch_size]\n",
    "            pmi = neural_pmi(x_batch, y_batch, pmi_model, estimator='variational_f_js')\n",
    "            pmi_list += np.array(pmi).tolist()\n",
    "        pmi_class.append(pmi_list)\n",
    "    np.save(f'{exp_name}/pmi_output_class_val.npy', np.array(pmi_class).T)\n",
    "    \n",
    "    print(f'Computing PMI for all test samples and for all classes...')\n",
    "    encoded_x = []\n",
    "    for x, _ in ds_test.batch(128):\n",
    "        encoded_x.append(int_model(x).numpy())\n",
    "    encoded_x = np.concatenate(encoded_x)\n",
    "    num_samples = encoded_x.shape[0]\n",
    "    \n",
    "    pmi_class = []\n",
    "    batch_size = 1024\n",
    "    for k in range(n_classes):\n",
    "        num_samples = encoded_x.shape[0]\n",
    "        y_k = tf.one_hot(tf.fill([num_samples], k), depth=n_classes)\n",
    "        pmi_list = []\n",
    "        for i in tqdm(range(0, len(encoded_x), batch_size), desc=f\"Computing PMI for class {k+1}\"):\n",
    "            x_batch = encoded_x[i:i+batch_size]\n",
    "            y_batch = y_k[i:i+batch_size]\n",
    "            pmi = neural_pmi(x_batch, y_batch, pmi_model, estimator='variational_f_js')\n",
    "            pmi_list += np.array(pmi).tolist()\n",
    "        pmi_class.append(pmi_list)\n",
    "    np.save(f'{exp_name}/pmi_output_class_test.npy', np.array(pmi_class).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63b6a1",
   "metadata": {},
   "source": [
    "### PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8626c2ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "Training PSI model (gaussian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 257.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 217.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 235.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 2\n",
      "Training PSI model (gaussian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 271.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 245.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 234.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 3\n",
      "Training PSI model (gaussian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 253.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 245.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 235.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 4\n",
      "Training PSI model (gaussian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 273.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 246.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 229.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 5\n",
      "Training PSI model (gaussian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 274.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 255.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 238.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 6\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/psi/gaussian\n",
      "Training PSI model (gaussian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 276.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 254.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 236.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 7\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_7/calibration/psi/gaussian\n",
      "Training PSI model (gaussian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 274.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 254.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 233.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 8\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/psi/gaussian\n",
      "Training PSI model (gaussian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 273.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 254.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 236.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 9\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_9/calibration/psi/gaussian\n",
      "Training PSI model (gaussian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 274.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 255.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 235.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 10\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_10/calibration/psi/gaussian\n",
      "Training PSI model (gaussian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 275.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:01, 252.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PSI for all test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projections: 500it [00:02, 235.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/gaussian'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PSI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    x_logits_list = []\n",
    "    y_labels_list = []\n",
    "\n",
    "    for x_batch, y_batch in ds_train.batch(256):\n",
    "        logits = int_model(x_batch)\n",
    "        labels = tf.argmax(y_batch, axis=1)\n",
    "        x_logits_list.append(logits)\n",
    "        y_labels_list.append(labels)\n",
    "\n",
    "    x = tf.concat(x_logits_list, axis=0).numpy()\n",
    "    y = tf.concat(y_labels_list, axis=0).numpy()\n",
    "    \n",
    "    print(f'Training PSI model (gaussian)...')\n",
    "    psi_data = psi_gaussian_train(x, y, n_projs=500)\n",
    "    np.save(f'{exp_name}/gaussian_output_model_500_projs.npy', psi_data)\n",
    "\n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PSI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "\n",
    "    psi_data = np.load(f'{exp_name}/gaussian_output_model_500_projs.npy', allow_pickle=True).item()\n",
    "\n",
    "    print(f'Computing PSI for all validation samples...')\n",
    "    x_logits_list = []\n",
    "\n",
    "    for x_batch, y_batch in ds_val.batch(256):\n",
    "        logits = int_model(x_batch)\n",
    "        x_logits_list.append(logits)\n",
    "    \n",
    "    x = tf.concat(x_logits_list, axis=0).numpy()\n",
    "    psi_class, pmi_arr = psi_gaussian_val_class(x, psi_data)\n",
    "    np.save(f'{exp_name}/psi_output_class_500_projs_val.npy', np.array(psi_class))\n",
    "\n",
    "    print(f'Computing PSI for all test samples...')\n",
    "    x_logits_list = []\n",
    "\n",
    "    for x_batch, y_batch in ds_test.batch(256):\n",
    "        logits = int_model(x_batch)\n",
    "        x_logits_list.append(logits)\n",
    "    \n",
    "    x = tf.concat(x_logits_list, axis=0).numpy()\n",
    "    psi_class, pmi_arr = psi_gaussian_val_class(x, psi_data)\n",
    "    np.save(f'{exp_name}/psi_output_class_500_projs_test.npy', np.array(psi_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605f01a",
   "metadata": {},
   "source": [
    "### PVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7feae117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 18:45:18.029283: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_45/dropout_75/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 4s 4ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 2ms/step\n",
      "79/79 [==============================] - 0s 2ms/step\n",
      "Run: 2\n",
      "Epoch 1/10\n",
      "399/399 [==============================] - 4s 4ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 3\n",
      "Epoch 1/10\n",
      "399/399 [==============================] - 4s 4ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 4\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 18:46:31.209015: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_57/dropout_111/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 4s 4ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 5\n",
      "Epoch 1/10\n",
      "399/399 [==============================] - 4s 4ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 6\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_6/calibration/pvi/training_from_scratch\n",
      "Epoch 1/10\n",
      "399/399 [==============================] - 4s 4ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 7\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_7/calibration/pvi/training_from_scratch\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 18:47:43.437386: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_69/dropout_147/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 4s 4ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 8\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_8/calibration/pvi/training_from_scratch\n",
      "Epoch 1/10\n",
      "399/399 [==============================] - 4s 4ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 9\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_9/calibration/pvi/training_from_scratch\n",
      "Epoch 1/10\n",
      "399/399 [==============================] - 4s 4ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 2ms/step\n",
      "Run: 10\n",
      "Making directory ../results/PI_Explainability/cnn_fashion_mnist/run_10/calibration/pvi/training_from_scratch\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 18:48:56.057107: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_81/dropout_183/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 4s 4ms/step - loss: 2.3027 - accuracy: 0.0991\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0987\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0980\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.0981\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 2ms/step\n",
      "79/79 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "random_runs = list(range(10))\n",
    "while any(random_runs[i] == i for i in range(10)):\n",
    "    np.random.shuffle(random_runs)\n",
    "    \n",
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{random_runs[run]+1}/saved_models/trained_weights.h5')\n",
    "    pvi_model.save_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    \n",
    "    untrained_model = create_model()\n",
    "    train_pvi_null_model(ds_train, untrained_model, epochs=10, save_path=f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    null_model = create_model()\n",
    "    null_model.load_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "    opt_temp_pvi = temp_scaling.temp_scaling_nll(pvi_model.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "    opt_temp_null = temp_scaling.temp_scaling_nll(null_model.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "\n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_val.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_test.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5802d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/finetuned'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    pvi_model.compile(optimizer=AdamW(learning_rate=1e-4, weight_decay=1e-4), loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1)\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "    pvi_model.fit(ds_train.batch(256), validation_data=ds_val.batch(256), epochs=100, callbacks=[lr_scheduler, early_stop])\n",
    "    \n",
    "    pvi_model.save_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    \n",
    "    untrained_model = create_model()\n",
    "    untrained_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_null_model_weights.h5')\n",
    "    untrained_model.save_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'{exp_name}/pvi_model_weights.h5')\n",
    "    null_model = create_model()\n",
    "    null_model.load_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "    opt_temp_pvi = temp_scaling.temp_scaling_nll(pvi_model.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "    opt_temp_null = temp_scaling.temp_scaling_nll(null_model.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "\n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_val.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_test.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e58582e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 2ms/step\n",
      "Run: 2\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 2ms/step\n",
      "Run: 3\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 4\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 5\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 2ms/step\n",
      "Run: 6\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 7\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 8\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 9\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "Run: 10\n",
      "Computing PVI for all validation samples and for all classes...\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "71/71 [==============================] - 0s 1ms/step\n",
      "Computing PVI for all test samples and for all classes...\n",
      "79/79 [==============================] - 0s 1ms/step\n",
      "79/79 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "pvi_runs = [1 if i == 7 else 7 for i in range(10)]\n",
    "    \n",
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{pvi_runs[run]+1}/saved_models/trained_weights.h5')\n",
    "    pvi_model.save_weights(f'{exp_name}/pvi_model_best_weights.h5')\n",
    "    \n",
    "#     untrained_model = create_model()\n",
    "#     train_pvi_null_model(ds_train, untrained_model, epochs=10, save_path=f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model = create_model()\n",
    "    pvi_model.load_weights(f'{exp_name}/pvi_model_best_weights.h5')\n",
    "    null_model = create_model()\n",
    "    null_model.load_weights(f'{exp_name}/pvi_null_model_weights.h5')\n",
    "    \n",
    "    true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "    opt_temp_pvi = temp_scaling.temp_scaling_nll(pvi_model.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "    ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "    opt_temp_null = temp_scaling.temp_scaling_nll(null_model.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "\n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_val.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_best_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = neural_pvi_class(ds_test.batch(128), pvi_model, null_model, opt_temp_pvi, opt_temp_null)\n",
    "    np.save(f'{exp_name}/pvi_class_best_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd012e39",
   "metadata": {},
   "source": [
    "### Ensemble PVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1d6bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/ensemble_no_training_training_from_scratch'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "        \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PVI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    pvi_model_1 = create_model()\n",
    "    pvi_model_1.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    null_model_1 = create_model()\n",
    "    null_model_1.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_null_model_weights.h5')\n",
    "    pvi_model_2 = create_model()\n",
    "    pvi_model_2.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_model_weights.h5')\n",
    "    null_model_2 = create_model()\n",
    "    null_model_2.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch/pvi_null_model_weights.h5')\n",
    "    \n",
    "#     true_y_val = np.argmax([y for x,y in ds_val], axis=1)\n",
    "#     opt_temp_pvi_1 = utils.temp_scaling_nll(pvi_model_1.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "#     opt_temp_pvi_2 = utils.temp_scaling_nll(pvi_model_2.predict(ds_val.batch(128), verbose=0), true_y_val)\n",
    "#     ds_null = ds_val.map(lambda x, y: (tf.zeros_like(x), y))\n",
    "#     opt_temp_null = utils.temp_scaling_nll(null_model_1.predict(ds_null.batch(128), verbose=0), true_y_val)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PVI for all training and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    print(f'Computing PVI for all validation samples and for all classes...')\n",
    "    pvi_class = []\n",
    "    for (x_batch, y_batch) in ds_val.batch(256):\n",
    "        pvi = neural_pvi_ensemble_class([x_batch, x_batch], [pvi_model_1, pvi_model_2], [null_model_1, null_model_2])\n",
    "        pvi_class += np.array(pvi).tolist()\n",
    "    np.save(f'{exp_name}/pvi_class_val.npy', np.array(pvi_class))\n",
    "\n",
    "    print(f'Computing PVI for all test samples and for all classes...')\n",
    "    pvi_class = []\n",
    "    for (x_batch, y_batch) in ds_test.batch(256):\n",
    "        pvi = neural_pvi_ensemble_class([x_batch, x_batch], [pvi_model_1, pvi_model_2], [null_model_1, null_model_2])\n",
    "        pvi_class += np.array(pvi).tolist()\n",
    "    np.save(f'{exp_name}/pvi_class_test.npy', np.array(pvi_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a7dc6",
   "metadata": {},
   "source": [
    "### Temp Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a195b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "18/18 [==============================] - 1s 12ms/step\n",
      "Run: 2\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 3\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 4\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 5\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 6\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 7\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 8\n",
      "18/18 [==============================] - 0s 7ms/step\n",
      "Run: 9\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 10\n",
      "18/18 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    if not os.path.exists(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'):\n",
    "        print(\"Making directory\", f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')\n",
    "        os.makedirs(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')                                  \n",
    "  \n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = model.predict(ds_val.batch(512), verbose=0)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_nll.npy', opt_temp)\n",
    "\n",
    "    opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81bcfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "#     if not os.path.exists(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'):\n",
    "#         print(\"Making directory\", f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')\n",
    "#         os.makedirs(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration')                                  \n",
    "  \n",
    "    \n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = model.predict(ds_val.batch(512), verbose=0)\n",
    "    \n",
    "    opt_temp, opt_weights = temp_scaling.ensemble_temp_scaling_nll(scores, true_y_val, num_classes)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_ets_nll.npy', opt_temp)\n",
    "    np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_weights_ets_nll.npy', opt_weights)\n",
    "\n",
    "#     opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)\n",
    "#     np.save(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/softmax_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a39b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "    \n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = model.predict(ds_val.batch(512), verbose=0)\n",
    "    \n",
    "    pts = temp_scaling.PTSCalibrator(\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=64,\n",
    "    nlayers=2,\n",
    "    n_nodes=32,\n",
    "    length_logits=10,\n",
    "    top_k_logits=5\n",
    ")\n",
    "\n",
    "    pts.tune(logits=scores, labels=pred_y_val)\n",
    "    pts.save(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "830f252e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 2\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 3\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 4\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 5\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "Run: 6\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 7\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 8\n",
      "18/18 [==============================] - 0s 7ms/step\n",
      "Run: 9\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "Run: 10\n",
      "18/18 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pmi/separable_variational_f_js'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pmi_output_class_val.npy')\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)                            \n",
    "    np.save(f'{exp_name}/pmi_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)                                \n",
    "    np.save(f'{exp_name}/pmi_opt_temp_nll.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)                                \n",
    "    np.save(f'{exp_name}/pmi_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b513fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 2\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 3\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 4\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 5\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 6\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "Run: 7\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 8\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 9\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 10\n",
      "18/18 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/gaussian'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/psi_output_class_500_projs_val.npy')\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)                                 \n",
    "    np.save(f'{exp_name}/psi_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)                            \n",
    "    np.save(f'{exp_name}/psi_opt_temp_nll.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)                            \n",
    "    np.save(f'{exp_name}/psi_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "083364f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 2\n",
      "18/18 [==============================] - 0s 7ms/step\n",
      "Run: 3\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "Run: 4\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 5\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 6\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 7\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "Run: 8\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 9\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 10\n",
      "18/18 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pvi_class_val.npy')\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)                                 \n",
    "    np.save(f'{exp_name}/pvi_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)                                          \n",
    "    np.save(f'{exp_name}/pvi_opt_temp_nll.npy', opt_temp)\n",
    "\n",
    "    opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)                                          \n",
    "    np.save(f'{exp_name}/pvi_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196407d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 2\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "Run: 3\n",
      "18/18 [==============================] - 0s 7ms/step\n",
      "Run: 4\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "Run: 5\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 6\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 7\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 8\n",
      "18/18 [==============================] - 0s 6ms/step\n",
      "Run: 9\n",
      "18/18 [==============================] - 0s 5ms/step\n",
      "Run: 10\n",
      "18/18 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pvi_class_best_val.npy')\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_aurc(scores, pred_y_val, true_y_val)                                 \n",
    "    np.save(f'{exp_name}/pvi_best_opt_temp_aurc.npy', opt_temp)\n",
    "    \n",
    "    opt_temp = temp_scaling.temp_scaling_nll(scores, true_y_val)                                          \n",
    "    np.save(f'{exp_name}/pvi_best_opt_temp_nll.npy', opt_temp)\n",
    "\n",
    "    opt_temp = temp_scaling.temp_scaling_ece(scores, pred_y_val, true_y_val, 15)                                          \n",
    "    np.save(f'{exp_name}/pvi_best_opt_temp_ece.npy', opt_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pvi_class_val.npy')\n",
    "    \n",
    "    opt_temp, opt_weights = temp_scaling.ensemble_temp_scaling_nll(scores, true_y_val, num_classes)\n",
    "    np.save(f'{exp_name}/pvi_opt_temp_ets_nll.npy', opt_temp)\n",
    "    np.save(f'{exp_name}/pvi_opt_weights_ets_nll.npy', opt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf4bee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(10):\n",
    "    print(f'Run: {run+1}')\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/pvi/training_from_scratch'\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_val = np.argmax(model.predict(ds_val.batch(512), verbose=1), axis=1)\n",
    "    scores = np.load(f'{exp_name}/pvi_class_val.npy')\n",
    "    \n",
    "    opt_temp, opt_weights = temp_scaling.ensemble_temp_scaling_nll(scores, true_y_val, num_classes)\n",
    "    np.save(f'{exp_name}/pvi_opt_temp_ets_nll.npy', opt_temp)\n",
    "    np.save(f'{exp_name}/pvi_opt_weights_ets_nll.npy', opt_weights)\n",
    "    \n",
    "    pts = temp_scaling.PTSCalibrator(\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=64,\n",
    "    nlayers=2,\n",
    "    n_nodes=128,\n",
    "    length_logits=10,\n",
    "    top_k_logits=5\n",
    ")\n",
    "\n",
    "    pts.tune(logits=scores, labels=pred_y_val)\n",
    "    pts.save(path=f'{exp_name}/calibration_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591243a",
   "metadata": {},
   "source": [
    "### Failure Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c3a8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_scores(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name):\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'\n",
    "    metric = conf_method.split('_')[-1] if 'temp_scaling' in conf_method else None\n",
    "    method_key = conf_method.replace(f'_temp_scaling_{metric}', '') if metric else conf_method\n",
    "\n",
    "    if method_key == 'softmax':\n",
    "        if metric:\n",
    "            opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "            return methods.max_softmax_prob(model, ds_test, opt_temp)\n",
    "        else:\n",
    "            return methods.max_softmax_prob(model, ds_test)\n",
    "\n",
    "    elif method_key in ['pmi', 'psi', 'pvi', 'pvi_best']:\n",
    "        if method_key == 'pmi':\n",
    "            exp_path = f'{base_path}/pmi/separable_variational_f_js'\n",
    "            class_file = 'pmi_output_class_test.npy'\n",
    "        elif method_key == 'psi':\n",
    "            exp_path = f'{base_path}/psi/gaussian'\n",
    "            class_file = 'psi_output_class_500_projs_test.npy'\n",
    "        elif method_key == 'pvi':\n",
    "            exp_path = f'{base_path}/pvi/training_from_scratch'\n",
    "            class_file = 'pvi_class_test.npy'\n",
    "        elif method_key == 'pvi_best':\n",
    "            exp_path = f'{base_path}/pvi/training_from_scratch'\n",
    "            class_file = 'pvi_class_best_test.npy'\n",
    "\n",
    "        opt_temp = np.load(f'{exp_path}/{method_key}_opt_temp_{metric}.npy')\n",
    "        scores_class = np.load(f'{exp_path}/{class_file}')\n",
    "        scores_class = np.array([utils.softmax(x / opt_temp) for x in scores_class])\n",
    "        return np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "\n",
    "    elif method_key == 'softmax_margin':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.softmax_margin(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'max_logits':\n",
    "        return methods.max_logits(model, ds_test)\n",
    "\n",
    "    elif method_key == 'logits_margin':\n",
    "        return methods.logits_margin(model, ds_test)\n",
    "\n",
    "    elif method_key == 'negative_entropy':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.negative_entropy(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'negative_gini':\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        return methods.negative_gini(model, ds_test, opt_temp)\n",
    "\n",
    "    elif method_key == 'isotonic_regression':\n",
    "        return methods.isotonic_reg(model, ds_val, ds_test, true_y_val)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown confidence method: {conf_method}\")\n",
    "\n",
    "\n",
    "def evaluate_failure_pred(ds_test, true_y_test, conf_method, n_runs=10):\n",
    "    results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": [],\n",
    "        \"naurc\": []\n",
    "    }\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        scores_test = get_confidence_scores(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name)\n",
    "\n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"naurc\"].append(metrics.compute_naurc(scores_test, pred_y_test, true_y_test))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2a644da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax_temp_scaling_aurc\n",
      "AUROC           : 92.06 (0.45)\n",
      "AUPRC (success) : 99.30 (0.10)\n",
      "AUPRC (error)   : 43.61 (1.12)\n",
      "FPR at 95% TPR  : 47.47 (1.50)\n",
      "AURC            : 9.14 (1.06)\n",
      "EAURC           : 6.64 (0.97)\n",
      "NAURC           : 98.34 (13.60)\n",
      "Method: pmi_temp_scaling_aurc\n",
      "AUROC           : 58.65 (0.71)\n",
      "AUPRC (success) : 94.16 (0.15)\n",
      "AUPRC (error)   : 15.71 (1.26)\n",
      "FPR at 95% TPR  : N/A\n",
      "AURC            : 58.46 (1.53)\n",
      "EAURC           : 55.95 (1.34)\n",
      "NAURC           : 830.99 (13.78)\n",
      "Method: psi_temp_scaling_aurc\n",
      "AUROC           : 78.46 (0.40)\n",
      "AUPRC (success) : 97.04 (0.14)\n",
      "AUPRC (error)   : 29.17 (0.99)\n",
      "FPR at 95% TPR  : N/A\n",
      "AURC            : 30.50 (1.45)\n",
      "EAURC           : 27.99 (1.26)\n",
      "NAURC           : 415.27 (7.81)\n",
      "Method: pvi_temp_scaling_aurc\n",
      "AUROC           : 91.02 (1.81)\n",
      "AUPRC (success) : 99.18 (0.33)\n",
      "AUPRC (error)   : 46.06 (3.89)\n",
      "FPR at 95% TPR  : 46.99 (3.99)\n",
      "AURC            : 10.26 (3.06)\n",
      "EAURC           : 7.76 (3.07)\n",
      "NAURC           : 115.52 (46.30)\n",
      "Method: softmax_margin_temp_scaling_aurc\n",
      "AUROC           : 92.09 (0.37)\n",
      "AUPRC (success) : 99.31 (0.09)\n",
      "AUPRC (error)   : 42.43 (1.33)\n",
      "FPR at 95% TPR  : 47.92 (1.59)\n",
      "AURC            : 9.04 (0.93)\n",
      "EAURC           : 6.53 (0.83)\n",
      "NAURC           : 96.78 (11.32)\n",
      "Method: max_logits\n",
      "AUROC           : 86.76 (0.50)\n",
      "AUPRC (success) : 98.85 (0.08)\n",
      "AUPRC (error)   : 32.38 (1.01)\n",
      "FPR at 95% TPR  : 63.26 (1.73)\n",
      "AURC            : 13.42 (0.92)\n",
      "EAURC           : 10.91 (0.76)\n",
      "NAURC           : 161.72 (7.39)\n",
      "Method: logits_margin\n",
      "AUROC           : 92.29 (0.17)\n",
      "AUPRC (success) : 99.37 (0.03)\n",
      "AUPRC (error)   : 42.12 (1.52)\n",
      "FPR at 95% TPR  : 48.21 (1.67)\n",
      "AURC            : 8.42 (0.48)\n",
      "EAURC           : 5.91 (0.29)\n",
      "NAURC           : 87.65 (2.00)\n",
      "Method: negative_entropy_temp_scaling_aurc\n",
      "AUROC           : 92.11 (0.55)\n",
      "AUPRC (success) : 99.36 (0.04)\n",
      "AUPRC (error)   : 43.07 (2.63)\n",
      "FPR at 95% TPR  : 49.68 (3.86)\n",
      "AURC            : 8.55 (0.51)\n",
      "EAURC           : 6.04 (0.41)\n",
      "NAURC           : 89.78 (6.25)\n",
      "Method: negative_gini_temp_scaling_aurc\n",
      "AUROC           : 91.98 (0.45)\n",
      "AUPRC (success) : 99.29 (0.10)\n",
      "AUPRC (error)   : 43.54 (1.56)\n",
      "FPR at 95% TPR  : 48.59 (2.72)\n",
      "AURC            : 9.20 (1.03)\n",
      "EAURC           : 6.69 (0.94)\n",
      "NAURC           : 99.15 (13.30)\n"
     ]
    }
   ],
   "source": [
    "methods_list = ['softmax_temp_scaling_aurc','pmi_temp_scaling_aurc','psi_temp_scaling_aurc','pvi_temp_scaling_aurc',\n",
    "                'softmax_margin_temp_scaling_aurc', 'max_logits', 'logits_margin', 'negative_entropy_temp_scaling_aurc',\n",
    "                'negative_gini_temp_scaling_aurc']\n",
    "for method in methods_list:\n",
    "    print(f'Method: {method}')\n",
    "    results = evaluate_failure_pred(ds_test, true_y_test, conf_method=f'{method}', n_runs=10)\n",
    "    print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "    print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "    print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "    print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "    print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "    print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")\n",
    "    print(f\"NAURC           : {utils.format_ci(results['naurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209deb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'softmax ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_ets_nll.npy')\n",
    "        opt_weights = np.load(f'{base_path}/softmax_opt_weights_ets_nll.npy')\n",
    "        \n",
    "        scores_class = apply_ets(logits,opt_temp,opt_weights,num_classes)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'PVI ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "        opt_temp = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_temp_ets_nll.npy')\n",
    "        opt_weights = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_weights_ets_nll.npy')\n",
    "        \n",
    "        scores_class = apply_ets(pvi,opt_temp,opt_weights,num_classes)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'softmax PTS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_ets_nll.npy')\n",
    "        opt_weights = np.load(f'{base_path}/softmax_opt_weights_ets_nll.npy')\n",
    "        \n",
    "        pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "        pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "        scores_class = pts_loaded.calibrate(logits)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ce79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'PVI PTS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"auroc\": [],\n",
    "        \"fpr_at_95tpr\": [],\n",
    "        \"auprc_success\": [],\n",
    "        \"auprc_error\": [],\n",
    "        \"aurc\": [],\n",
    "        \"eaurc\": []\n",
    "    }\n",
    "for run in range(10):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        \n",
    "        base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "        pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "        \n",
    "        pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "        pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "        scores_class = pts_loaded.calibrate(pvi)\n",
    "        scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "        \n",
    "        results[\"auroc\"].append(metrics.compute_auroc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_success\"].append(metrics.compute_auprc_success(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"auprc_error\"].append(metrics.compute_auprc_error(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"fpr_at_95tpr\"].append(metrics.compute_fpr_at_95tpr(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"aurc\"].append(metrics.compute_aurc(scores_test, pred_y_test, true_y_test))\n",
    "        results[\"eaurc\"].append(metrics.compute_eaurc(scores_test, pred_y_test, true_y_test))\n",
    "        \n",
    "print(f\"AUROC           : {utils.format_ci(results['auroc'], scale=100)}\")\n",
    "print(f\"AUPRC (success) : {utils.format_ci(results['auprc_success'], scale=100)}\")\n",
    "print(f\"AUPRC (error)   : {utils.format_ci(results['auprc_error'], scale=100)}\")\n",
    "print(f\"FPR at 95% TPR  : {utils.format_ci(results['fpr_at_95tpr'], scale=100)}\")\n",
    "print(f\"AURC            : {utils.format_ci(results['aurc'], scale=1000)}\")\n",
    "print(f\"EAURC           : {utils.format_ci(results['eaurc'], scale=1000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742c26f",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cbcae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_for_calibration(conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name):\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration'\n",
    "\n",
    "    def softmax_scaled(scores, temp=1.0):\n",
    "        return np.array([utils.softmax(x / temp) for x in scores])\n",
    "\n",
    "    if conf_method == 'softmax':\n",
    "        scores_class = methods.softmax_prob(model, ds_test)\n",
    "        scores_test = methods.max_softmax_prob(model, ds_test)\n",
    "        return scores_class, scores_test\n",
    "\n",
    "    if conf_method.startswith('softmax_temp_scaling'):\n",
    "        metric = conf_method.split('_')[-1]\n",
    "        opt_temp = np.load(f'{base_path}/softmax_opt_temp_{metric}.npy')\n",
    "        scores_class = methods.softmax_prob(model, ds_test, opt_temp)\n",
    "        scores_test = methods.max_softmax_prob(model, ds_test, opt_temp)\n",
    "        return scores_class, scores_test\n",
    "\n",
    "    if conf_method in ['pmi', 'psi', 'pvi', 'pvi_best']:\n",
    "        method = conf_method\n",
    "        metric = None\n",
    "        temp = 1.0\n",
    "    elif conf_method.startswith(('pmi_temp_scaling', 'psi_temp_scaling', 'pvi_temp_scaling', 'pvi_best_temp_scaling')):\n",
    "        parts = conf_method.split('_')\n",
    "        method = '_'.join(parts[:2]) if 'best' in parts else parts[0]\n",
    "        metric = parts[-1]\n",
    "        method_dir = {\n",
    "            'pmi': 'pmi/separable_variational_f_js',\n",
    "            'psi': 'psi/gaussian',\n",
    "            'pvi': 'pvi/training_from_scratch',\n",
    "            'pvi_best': 'pvi/training_from_scratch'\n",
    "        }[method]\n",
    "        temp = float(np.load(f'{base_path}/{method_dir}/{method}_opt_temp_{metric}.npy'))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown confidence method: {conf_method}\")\n",
    "\n",
    "    method_paths = {\n",
    "        'pmi': (f'{base_path}/pmi/separable_variational_f_js', 'pmi_output_class_test.npy'),\n",
    "        'psi': (f'{base_path}/psi/gaussian', 'psi_output_class_500_projs_test.npy'),\n",
    "        'pvi': (f'{base_path}/pvi/training_from_scratch', 'pvi_class_test.npy'),\n",
    "        'pvi_best': (f'{base_path}/pvi/training_from_scratch', 'pvi_class_best_test.npy'),\n",
    "    }\n",
    "\n",
    "    method_path, class_file = method_paths[method]\n",
    "    scores_class = np.load(f'{method_path}/{class_file}')\n",
    "    scores_class = softmax_scaled(scores_class, temp)\n",
    "    scores_test = np.array([score[pred] for score, pred in zip(scores_class, pred_y_test)])\n",
    "    return scores_class, scores_test\n",
    "\n",
    "def evaluate_calibration(ds_test, true_y_test, conf_method, n_runs=10):\n",
    "    results = {\n",
    "        \"ece\": [],\n",
    "        \"cc_ece\": [],\n",
    "        \"mce\": [],\n",
    "        \"ace\": [],\n",
    "        \"sce\": [],\n",
    "        \"ada_ece\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_ece\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"cw_ece\": [],\n",
    "        \"cw_sce\": [],\n",
    "        \"cw_ada_ece\": [],\n",
    "        \"cw_ada_sce\": [],\n",
    "        \"cw_ada_ece_rms\": [],\n",
    "        \"cw_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "        \"sharpness\": [],\n",
    "    }\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        tf.keras.utils.set_random_seed(run + 10)\n",
    "        model = create_model()\n",
    "        model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "        pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "        scores_class, scores_test = get_scores_for_calibration(\n",
    "            conf_method, model, ds_test, pred_y_test, run, model_name, dataset_name\n",
    "        )\n",
    "\n",
    "        results[\"ece\"].append(metrics.compute_ece(scores_test, pred_y_test, true_y_test, 15))\n",
    "        results[\"cc_ece\"].append(metrics.compute_cc_ece(scores_test, pred_y_test, true_y_test, 15))\n",
    "        results[\"mce\"].append(metrics.compute_mce(scores_test, pred_y_test, true_y_test, 15))\n",
    "        results[\"ace\"].append(metrics.compute_ace(scores_test, pred_y_test, true_y_test, 15))\n",
    "        results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"ada_ece\"].append(metrics.compute_adaece(scores_test, pred_y_test, true_y_test, 15))\n",
    "        results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"cc_ada_ece\"].append(metrics.compute_cc_adaece(scores_test, pred_y_test, true_y_test, 15))\n",
    "        results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"cw_ece\"].append(metrics.compute_cw_ece(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"cw_sce\"].append(metrics.compute_cw_sce(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"cw_ada_ece\"].append(metrics.compute_cw_adaece(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"cw_ada_sce\"].append(metrics.compute_cw_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"cw_ada_ece_rms\"].append(metrics.compute_cw_adaece_rms(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"cw_ada_sce_rms\"].append(metrics.compute_cw_adaece_rms(scores_class, true_y_test, num_classes, 15))\n",
    "        results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "        results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        results[\"sharpness\"].append(metrics.compute_sharpness(scores_class))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15696143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: softmax\n",
      "ECE:            3.06 (0.56)\n",
      "CC-ECE:         3.43 (0.48)\n",
      "MCE:            1.54 (0.42)\n",
      "ACE:            11.16 (2.42)\n",
      "SCE:            0.69 (0.10)\n",
      "Ada-ECE:        3.03 (0.57)\n",
      "Ada-SCE:        0.52 (0.06)\n",
      "CC-Ada-ECE:     3.19 (0.47)\n",
      "CC-Ada-SCE:     0.70 (0.09)\n",
      "CC-Ada-SCE-RMS: 5.24 (0.32)\n",
      "CW-ECE:         0.69 (0.10)\n",
      "CW-SCE:         0.69 (0.10)\n",
      "CW-Ada-ECE:     0.42 (0.04)\n",
      "CW-Ada-SCE:     0.42 (0.04)\n",
      "CW-Ada-ECE-RMS: 1.01 (0.09)\n",
      "CW-Ada-SCE-RMS: 1.01 (0.09)\n",
      "NLL:            23.63 (1.37)\n",
      "Brier Score:    10.73 (0.21)\n",
      "Sharpness:      10.21 (2.13)\n",
      "Method: pmi\n",
      "ECE:            3.05 (0.55)\n",
      "CC-ECE:         3.67 (0.45)\n",
      "MCE:            1.49 (0.42)\n",
      "ACE:            13.29 (2.95)\n",
      "SCE:            0.73 (0.09)\n",
      "Ada-ECE:        2.65 (0.51)\n",
      "Ada-SCE:        0.56 (0.05)\n",
      "CC-Ada-ECE:     3.31 (0.40)\n",
      "CC-Ada-SCE:     0.71 (0.10)\n",
      "CC-Ada-SCE-RMS: 5.48 (0.31)\n",
      "CW-ECE:         0.73 (0.09)\n",
      "CW-SCE:         0.73 (0.09)\n",
      "CW-Ada-ECE:     0.49 (0.06)\n",
      "CW-Ada-SCE:     0.49 (0.06)\n",
      "CW-Ada-ECE-RMS: 1.16 (0.14)\n",
      "CW-Ada-SCE-RMS: 1.16 (0.14)\n",
      "NLL:            24.48 (1.27)\n",
      "Brier Score:    10.94 (0.19)\n",
      "Sharpness:      11.45 (2.23)\n",
      "Method: psi\n",
      "ECE:            22.85 (0.80)\n",
      "CC-ECE:         22.89 (0.79)\n",
      "MCE:            3.97 (0.28)\n",
      "ACE:            30.21 (1.48)\n",
      "SCE:            4.21 (0.19)\n",
      "Ada-ECE:        22.85 (0.80)\n",
      "Ada-SCE:        4.11 (0.17)\n",
      "CC-Ada-ECE:     22.85 (0.80)\n",
      "CC-Ada-SCE:     3.06 (0.06)\n",
      "CC-Ada-SCE-RMS: 11.07 (0.16)\n",
      "CW-ECE:         4.21 (0.19)\n",
      "CW-SCE:         4.21 (0.19)\n",
      "CW-Ada-ECE:     3.79 (0.16)\n",
      "CW-Ada-SCE:     3.79 (0.16)\n",
      "CW-Ada-ECE-RMS: 6.95 (0.26)\n",
      "CW-Ada-SCE-RMS: 6.95 (0.26)\n",
      "NLL:            45.04 (0.84)\n",
      "Brier Score:    21.43 (0.41)\n",
      "Sharpness:      81.57 (2.63)\n",
      "Method: pvi\n",
      "ECE:            1.49 (0.30)\n",
      "CC-ECE:         2.35 (0.23)\n",
      "MCE:            0.18 (0.03)\n",
      "ACE:            10.29 (2.65)\n",
      "SCE:            0.34 (0.02)\n",
      "Ada-ECE:        1.49 (0.31)\n",
      "Ada-SCE:        0.29 (0.02)\n",
      "CC-Ada-ECE:     2.23 (0.29)\n",
      "CC-Ada-SCE:     0.93 (0.03)\n",
      "CC-Ada-SCE-RMS: 5.92 (0.13)\n",
      "CW-ECE:         0.34 (0.02)\n",
      "CW-SCE:         0.34 (0.02)\n",
      "CW-Ada-ECE:     0.24 (0.03)\n",
      "CW-Ada-SCE:     0.24 (0.03)\n",
      "CW-Ada-ECE-RMS: 0.64 (0.10)\n",
      "CW-Ada-SCE-RMS: 0.64 (0.10)\n",
      "NLL:            19.72 (0.54)\n",
      "Brier Score:    10.22 (0.34)\n",
      "Sharpness:      17.40 (0.45)\n",
      "Method: pvi_best\n",
      "ECE:            1.17 (0.19)\n",
      "CC-ECE:         2.05 (0.10)\n",
      "MCE:            0.16 (0.01)\n",
      "ACE:            8.65 (1.81)\n",
      "SCE:            0.33 (0.00)\n",
      "Ada-ECE:        1.17 (0.21)\n",
      "Ada-SCE:        0.26 (0.01)\n",
      "CC-Ada-ECE:     1.99 (0.18)\n",
      "CC-Ada-SCE:     0.91 (0.00)\n",
      "CC-Ada-SCE-RMS: 5.81 (0.01)\n",
      "CW-ECE:         0.33 (0.00)\n",
      "CW-SCE:         0.33 (0.00)\n",
      "CW-Ada-ECE:     0.19 (0.01)\n",
      "CW-Ada-SCE:     0.19 (0.01)\n",
      "CW-Ada-ECE-RMS: 0.48 (0.04)\n",
      "CW-Ada-SCE-RMS: 0.48 (0.04)\n",
      "NLL:            19.15 (0.00)\n",
      "Brier Score:    9.91 (0.00)\n",
      "Sharpness:      17.10 (0.06)\n",
      "Method: softmax_temp_scaling_nll\n",
      "ECE:            0.72 (0.10)\n",
      "CC-ECE:         1.78 (0.10)\n",
      "MCE:            0.18 (0.02)\n",
      "ACE:            5.12 (1.47)\n",
      "SCE:            0.35 (0.02)\n",
      "Ada-ECE:        0.65 (0.13)\n",
      "Ada-SCE:        0.29 (0.02)\n",
      "CC-Ada-ECE:     1.72 (0.12)\n",
      "CC-Ada-SCE:     0.93 (0.03)\n",
      "CC-Ada-SCE-RMS: 5.92 (0.13)\n",
      "CW-ECE:         0.35 (0.02)\n",
      "CW-SCE:         0.35 (0.02)\n",
      "CW-Ada-ECE:     0.24 (0.03)\n",
      "CW-Ada-SCE:     0.24 (0.03)\n",
      "CW-Ada-ECE-RMS: 0.64 (0.10)\n",
      "CW-Ada-SCE-RMS: 0.64 (0.10)\n",
      "NLL:            19.72 (0.54)\n",
      "Brier Score:    10.23 (0.34)\n",
      "Sharpness:      17.40 (0.45)\n",
      "Method: pmi_temp_scaling_nll\n",
      "ECE:            1.27 (0.10)\n",
      "CC-ECE:         2.69 (0.15)\n",
      "MCE:            0.30 (0.06)\n",
      "ACE:            8.17 (1.14)\n",
      "SCE:            0.58 (0.05)\n",
      "Ada-ECE:        1.10 (0.12)\n",
      "Ada-SCE:        0.46 (0.03)\n",
      "CC-Ada-ECE:     2.56 (0.16)\n",
      "CC-Ada-SCE:     0.91 (0.04)\n",
      "CC-Ada-SCE-RMS: 6.11 (0.12)\n",
      "CW-ECE:         0.58 (0.05)\n",
      "CW-SCE:         0.58 (0.05)\n",
      "CW-Ada-ECE:     0.41 (0.05)\n",
      "CW-Ada-SCE:     0.41 (0.05)\n",
      "CW-Ada-ECE-RMS: 0.95 (0.12)\n",
      "CW-Ada-SCE-RMS: 0.95 (0.12)\n",
      "NLL:            21.93 (0.33)\n",
      "Brier Score:    10.56 (0.29)\n",
      "Sharpness:      19.42 (0.50)\n",
      "Method: psi_temp_scaling_nll\n",
      "ECE:            2.85 (0.28)\n",
      "CC-ECE:         4.04 (0.34)\n",
      "MCE:            0.32 (0.03)\n",
      "ACE:            18.40 (0.96)\n",
      "SCE:            0.57 (0.05)\n",
      "Ada-ECE:        3.08 (0.29)\n",
      "Ada-SCE:        0.50 (0.04)\n",
      "CC-Ada-ECE:     3.84 (0.33)\n",
      "CC-Ada-SCE:     1.14 (0.06)\n",
      "CC-Ada-SCE-RMS: 6.29 (0.18)\n",
      "CW-ECE:         0.57 (0.05)\n",
      "CW-SCE:         0.57 (0.05)\n",
      "CW-Ada-ECE:     0.48 (0.04)\n",
      "CW-Ada-SCE:     0.48 (0.04)\n",
      "CW-Ada-ECE-RMS: 1.25 (0.10)\n",
      "CW-Ada-SCE-RMS: 1.25 (0.10)\n",
      "NLL:            24.54 (1.12)\n",
      "Brier Score:    12.10 (0.56)\n",
      "Sharpness:      22.10 (1.04)\n",
      "Method: pvi_temp_scaling_nll\n",
      "ECE:            1.48 (0.30)\n",
      "CC-ECE:         2.35 (0.23)\n",
      "MCE:            0.18 (0.03)\n",
      "ACE:            10.28 (2.65)\n",
      "SCE:            0.34 (0.02)\n",
      "Ada-ECE:        1.49 (0.31)\n",
      "Ada-SCE:        0.29 (0.02)\n",
      "CC-Ada-ECE:     2.23 (0.29)\n",
      "CC-Ada-SCE:     0.93 (0.03)\n",
      "CC-Ada-SCE-RMS: 5.93 (0.13)\n",
      "CW-ECE:         0.34 (0.02)\n",
      "CW-SCE:         0.34 (0.02)\n",
      "CW-Ada-ECE:     0.24 (0.03)\n",
      "CW-Ada-SCE:     0.24 (0.03)\n",
      "CW-Ada-ECE-RMS: 0.64 (0.10)\n",
      "CW-Ada-SCE-RMS: 0.64 (0.10)\n",
      "NLL:            19.71 (0.54)\n",
      "Brier Score:    10.22 (0.34)\n",
      "Sharpness:      17.46 (0.42)\n",
      "Method: pvi_best_temp_scaling_nll\n",
      "ECE:            1.17 (0.19)\n",
      "CC-ECE:         2.05 (0.10)\n",
      "MCE:            0.16 (0.01)\n",
      "ACE:            8.65 (1.81)\n",
      "SCE:            0.33 (0.00)\n",
      "Ada-ECE:        1.17 (0.21)\n",
      "Ada-SCE:        0.26 (0.01)\n",
      "CC-Ada-ECE:     1.99 (0.18)\n",
      "CC-Ada-SCE:     0.91 (0.00)\n",
      "CC-Ada-SCE-RMS: 5.81 (0.01)\n",
      "CW-ECE:         0.33 (0.00)\n",
      "CW-SCE:         0.33 (0.00)\n",
      "CW-Ada-ECE:     0.19 (0.01)\n",
      "CW-Ada-SCE:     0.19 (0.01)\n",
      "CW-Ada-ECE-RMS: 0.48 (0.04)\n",
      "CW-Ada-SCE-RMS: 0.48 (0.04)\n",
      "NLL:            19.15 (0.00)\n",
      "Brier Score:    9.91 (0.00)\n",
      "Sharpness:      17.10 (0.06)\n"
     ]
    }
   ],
   "source": [
    "methods_list = ['softmax','pmi','psi','pvi','pvi_best',\n",
    "                'softmax_temp_scaling_nll','pmi_temp_scaling_nll','psi_temp_scaling_nll','pvi_temp_scaling_nll','pvi_best_temp_scaling_nll']\n",
    "for method in methods_list:\n",
    "    print(f'Method: {method}')\n",
    "    results = evaluate_calibration(ds_test, true_y_test, conf_method=f'{method}', n_runs=10)\n",
    "    print(f\"ECE:            {utils.format_ci(results['ece'], scale=100)}\")\n",
    "    print(f\"CC-ECE:         {utils.format_ci(results['cc_ece'], scale=100)}\")\n",
    "    print(f\"MCE:            {utils.format_ci(results['mce'], scale=100)}\")\n",
    "    print(f\"ACE:            {utils.format_ci(results['ace'], scale=100)}\")\n",
    "    print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "    print(f\"Ada-ECE:        {utils.format_ci(results['ada_ece'], scale=100)}\")\n",
    "    print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "    print(f\"CC-Ada-ECE:     {utils.format_ci(results['cc_ada_ece'], scale=100)}\")\n",
    "    print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "    print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "    print(f\"CW-ECE:         {utils.format_ci(results['cw_ece'], scale=100)}\")\n",
    "    print(f\"CW-SCE:         {utils.format_ci(results['cw_sce'], scale=100)}\")\n",
    "    print(f\"CW-Ada-ECE:     {utils.format_ci(results['cw_ada_ece'], scale=100)}\")\n",
    "    print(f\"CW-Ada-SCE:     {utils.format_ci(results['cw_ada_sce'], scale=100)}\")\n",
    "    print(f\"CW-Ada-ECE-RMS: {utils.format_ci(results['cw_ada_ece_rms'], scale=100)}\")\n",
    "    print(f\"CW-Ada-SCE-RMS: {utils.format_ci(results['cw_ada_sce_rms'], scale=100)}\")\n",
    "    print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "    print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")\n",
    "    print(f\"Sharpness:      {utils.format_ci(results['sharpness'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'softmax ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "\n",
    "    logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "    opt_temp = np.load(f'{base_path}/softmax_opt_temp_ets_nll.npy')\n",
    "    opt_weights = np.load(f'{base_path}/softmax_opt_weights_ets_nll.npy')\n",
    "\n",
    "    scores_class = apply_ets(logits,opt_temp,opt_weights,num_classes)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51815a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ets(logits, opt_temp, opt_weights, n_class):\n",
    "    p1 = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    scaled_logits = logits / opt_temp\n",
    "    p0 = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)\n",
    "    p2 = np.ones_like(p0) / n_class\n",
    "    w = opt_weights / np.sum(opt_weights)  # just in case\n",
    "    calibrated_probs = w[0] * p0 + w[1] * p1 + w[2] * p2\n",
    "    return calibrated_probs\n",
    "\n",
    "\n",
    "method = 'PVI ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "    pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "    opt_temp = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_temp_ets_nll.npy')\n",
    "    opt_weights = np.load(f'{base_path}/pvi/training_from_scratch/pvi_opt_weights_ets_nll.npy')\n",
    "\n",
    "    scores_class = apply_ets(pvi,opt_temp,opt_weights,num_classes)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'softmax ETS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "\n",
    "    logits = model.predict(ds_test.batch(512), verbose=0)\n",
    "\n",
    "    pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "    pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "    scores_class = pts_loaded.calibrate(logits)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc7001",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'PVI PTS'\n",
    "print(f'Method: {method}')\n",
    "results = {\n",
    "        \"sce\": [],\n",
    "        \"ada_sce\": [],\n",
    "        \"cc_ada_sce\": [],\n",
    "        \"cc_ada_sce_rms\": [],\n",
    "        \"nll\": [],\n",
    "        \"bs\": [],\n",
    "    }\n",
    "for run in range(10):\n",
    "    tf.keras.utils.set_random_seed(run + 10)\n",
    "    model = create_model()\n",
    "    model.load_weights(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/saved_models/trained_weights.h5')\n",
    "\n",
    "    pred_y_test = np.argmax(model.predict(ds_test.batch(256), verbose=0), axis=1)\n",
    "    base_path = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/'\n",
    "    pvi =  np.load(f'{base_path}/pvi/training_from_scratch/pvi_class_test.npy')\n",
    "\n",
    "    pts_loaded = temp_scaling.PTSCalibrator(\n",
    "        epochs=0,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=64,\n",
    "        nlayers=2,\n",
    "        n_nodes=32,\n",
    "        length_logits=10,\n",
    "        top_k_logits=5\n",
    "    )\n",
    "    pts_loaded.load(path=f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/calibration_model/')\n",
    "    scores_class = pts_loaded.calibrate(pvi)\n",
    "\n",
    "    results[\"sce\"].append(metrics.compute_sce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"ada_sce\"].append(metrics.compute_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce\"].append(metrics.compute_cc_adasce(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"cc_ada_sce_rms\"].append(metrics.compute_cc_adasce_rms(scores_class, true_y_test, num_classes, 15))\n",
    "    results[\"nll\"].append(metrics.compute_nll(scores_class, true_y_test, num_classes))\n",
    "    results[\"bs\"].append(metrics.compute_brier_score(scores_class, true_y_test, num_classes))\n",
    "        \n",
    "print(f\"SCE:            {utils.format_ci(results['sce'], scale=100)}\")\n",
    "print(f\"Ada-SCE:        {utils.format_ci(results['ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE:     {utils.format_ci(results['cc_ada_sce'], scale=100)}\")\n",
    "print(f\"CC-Ada-SCE-RMS: {utils.format_ci(results['cc_ada_sce_rms'], scale=100)}\")\n",
    "print(f\"NLL:            {utils.format_ci(results['nll'], scale=100)}\")\n",
    "print(f\"Brier Score:    {utils.format_ci(results['bs'], scale=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5ef34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
